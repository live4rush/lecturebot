Awesome. Uh, thanks aj. Alright, let's get started. Uh, so for today's agenda, we'll go through CNNs and nns, uh, roughly two hours each. Uh, so after NN theoretical material, we'll go through a coding workbook. And, uh, after wrapping that up, we'll go through r and n theoretical material and a coding workbook. Um, so that's, uh, uh, enough for today. Uh, so we'll, we'll try to stay track on track, uh, on for time today. Um, make sure that, you know, we can finish this in four hour time window. Uh, I think we can, uh, it's, it's a bit of material, but I think we can do that in four hours. Uh, feel free to ask questions like you did yesterday. Uh, use the q and a feature and, uh, I'll try to answer them, um, in, in short time windows. Cool. So without further ado, diving deep into CNN's, um, for that, actually, uh, I want you to noodle on these questions a little bit. Uh, just get your mind thinking like what's happening here. Uh, like, you know, if you look at these questions, how Google photos tax different in individuals in the photo, uh, you might have experienced that, uh, uh, while using Google Photos or Amazon photos, uh, or, you know, how does autonomous driving work? Um, uh, or, you know, if you use Snapchat, you know, how, how is a mask overlaid on, on a particular part of the face? Um, and, uh, so think, think along those lines, how that is happening. Uh, what are the, uh, core methods that are happening? Well, you don't have to have, like, you know, deep answers, but think, think along those lines. Alright, so, um, as, as I said earlier, the first thing is we'll try to develop why, uh, CNNs are used. What are the, some of the salient features of CNN, why they're applicable for images, um, some principles applicable to CNN and applications. Let me set up my iPad real quick. Uh, sec everyone follow, become, host. Okay. Looks good. Alright, so, brief agenda. Um, we'll look at the principles of CNNI look at different layers that make up, uh, you know, convolution, neur networks, uh, different types of CNN models out there, um, that are, uh, industry standard. Uh, what are some advantages, limitations and applications of CNN. Um, and then we'll wrap it up with a coding walkthrough. So first we'll think of why not FNN for images. Why, uh, why is, uh, why, why do we have to invent a new architecture for, for, uh, images? So I want to contrast that a little bit. Uh, like what happens, you use an FNN on images. So think of, um, a one megapixel image, right? So a one megapixel image has, um, um, you have thousand by thousand pixels. Um, that's order of 10.6, um, features to begin with. That's your input. And with FNN, um, as we've seen yesterday, we have fully connected layers. That's the salient feature of FNN. So, so you have your layer one and layer two, which are fully connected. Um, and that is, let's say your hidden layer is also, um, the same size as your input layer 10, 4 6. So now the, let me change the color, the weight parameters, the weight matrix that you need to learn is of order of ten four, six times 10, 4 6 or ten four twelve. That's a huge explosion of parameters, uh, if you try to use an FNN uh, uh, for a image kind of problem. So we want to try to make that more efficient by better parameter sharing, by kind of exploiting the principles of images, like how images work. So images have certain inherent principles we'll look at, uh, that make them, um, suitable for developing some, uh, principles that will exploit through the CNN architecture. So first thing is the principle of, uh, let's see, next slide. Yeah, the first thing is the principle of locality. So what it means is fairly simple. So when you look at an image a and take a part of the image, uh, let's say in this case, if you're taking a part of the flower here, um, the pixels in that vicinity have more correlation to each other. So, so a nearby pixel is likely to be very related to a part of the flower rather than, you know, a part of the sky. So, uh, that's one. Um, so that's one, uh, principle of the images, uh, that is very useful and we'll see how to exploit that through CNNs. Uh, so what it means is nearby pixels are more, more important to tell, tell you about what the pixel is about than, uh, farther of pixels. There is more information conveyed within the nearby pixels, um, about that pixel. So that's one principle. It's called principle of locality. And, um, the learning patterns and features within these localized regions, um, are inherently, uh, unique rather than processing the entire image. Uh, so you can take small parts of the image and learn about those parts, um, and capture the learning patterns there and features there rather than, you know, taking the whole image. The second thing is, uh, translation in variance. Let's say you're trying to find a cat in image. So it doesn't matter if the cat is, uh, you know, here or in the next image of the cat is here or in the next image, and the cat is here. So you're trying to find the cat, um, whether you know how it's oriented or where it is in the particular image, uh, your goal of the model is trying to detect that cat. So if you, if your model is, um, not translation in variant, then you need to train the model with every possible combination of cat, uh, and various parts of the image or various orientations, uh, to be able to learn. Uh, and you, it won't even guarantee you in the future. Let's say if the cat is like upside down in this part of the image and you didn't learn from an image of that sort, then, um, the model will break. It won't generalize. So the goal is to develop a network that would be translation very irrespective of, you know, how a particular object is oriented in an image. Um, we would want the model to learn that pattern. So that's called translation in variance. So these are the two key principles, locality and translation variance that we want to exploit, uh, for, uh, making the network architectures more efficient. Uh, so as we've seen with FNN, uh, we, we, we are in the order of magnitude of 10, 12 with just one MAGA pixel cell image and one hidden layer. So let's see if we can bring that down to a smaller number with a better architecture. Um, just to caveat before we go into this math, um, from a interview and practice standpoint, these principles, locality and translation in variants are, uh, important for you to know from a theoretical perspective. But this map is more for your understanding. Um, I can pretty much guarantee you this math won't be you asked in interviews or you would use this in practice, but we'll still go through this math for you to understand what you know, how to apply this, um, how, how to more intuitively learn from math, like, uh, this translation variance and locality principles translate to a, a better architecture. So for that, let me use a whiteboard. So in a regular FNN, uh, let's start with that. So you have your input vector in one dimension, and you probably have a hidden, uh, layer, H one. Um, so let's do a 10, um, uh, 10 feature vector. Same thing with hidden layer. Let's do a 10 node. So as we know that there are dense connections, right? And there is a matrix you are trying to learn here for weights and for biases. So you can represent this in a regular, um, fashion as you know, um, HIJ equal to, sorry, HI equal to UI plus WIJ that's the weight for that particular connection into XI, uh, right, so that's your, um, equation, how you create the, uh, the hidden layer activations. And then you, you wrap this up, uh, in an activation function. So let's not complicate that part. So essentially it's, it's your wait times, your input plus bias, um, and there is a summation here, uh, across all the ice. Um, so, so you are summing, um, across all the, uh, input features, um, and then adding a bias. So that's how you come to this particular, um, activation, uh, following, uh, so far. Cool. So let's try to contrast that with, let's say, let's make this a 2D. So now instead of your input as one dimension, now I have a two dimension. So your X is, um, across INJ, um, dimensions. And then let's say we have a weight vector and a bias, and then your hidden layer is also a two dimension. Now, given we we're dealing with images, now we have input in two dimensions and a hidden layer and two dimensions. So you have then, right, so now this is a two dimensional, this is two dimensional, and this tensor becomes a fourth degree tensor, four dimensional tensor. Um, so sorry, uh, this is not the right representation. So, uh, so this is two, And then the tensor that you're trying to learn will be a fourth order or, uh, tensor. Um, so just trying to contrast to the previous one. So you have one dimensional inputs, one dimensional hidden layer, and the weights you're trying to learn are two dimensional, right? So just trying to, um, uh, extend that to a two dimensional input and two dimensional hidden layer. So now you're trying to learn a four D, fourth, fourth order, uh, uh, fourth dimensional tensor and the bias term that you're trying to learn. Instead of one dimensional here, you would have to learn a two dimensional, um, um, bio stem as well. So four dimensional anyways, okay, so let's try to put that in mathematical form. So you can write this as, uh, lemme select the right color. H IJ can be represented as your bias term, UIJ plus summation over K, summation over LW weight, uh, vector, IJKL or weight tenor times your input. So, um, I'm just putting the, what we have seen in the previous, uh, slide in a mathematical form, how that would look like. Um, so we can use some, um, mathematical tricks to, to represent it in a easier fashion. So let's do that. So I'm just converting the KL here into a i plus A and I plus B format. So, so essentially I want to represent K equal to I plus A and L equal to J plus B. So, so that makes the mathematical conversion easier. So going back, you can write this equation as UIJ plus SMA A summation B rate IJI plus A K plus B times XI plus a K plus B. So essentially we have this input that is a one, my CapEx image, and 0.6 hidden layer is 10.6, and we are trying to learn a weight that is 10 point 12. Uh, our goal is to reduce to a smaller number, and this is of the order of, again, ten four six, right? So our goal is to see if we can reduce these to a smaller number, uh, by using the principles of locality and translation in variants. Um, I'm seeing from questions, um, why weight is becoming four dimensional. So as you've seen with one dimensional vectors, right? So your weight mattresses is, um, uh, two dimensional, um, uh, when you, uh, use one dimensional input. So, so you can see that, you know, per, um, so let's, let me try to put this in, in a way. So, so here, your input, let's say is, is, um, X one, X two, X three, so on, right? And you're trying to get to a hidden layer with H one, H two, H three, so on. So in order to get there for a fully connect network, how many weights do you need? You need this many times, this many weights. We looked at it yesterday, like, how many parameters? So that's this matrix. So you need, um, what are the dimension here? Let's say it's 10, so 10 times 10 weights to be able to get to this middle layer metrics. So if you think of that, that in two dimensional terms, uh, that becomes a four dimensional. Um, so, so if this explodes to a, a two dimensional and this is a two dimensional, then this will be a fourth degree cancer. Um, I hope that answers, uh, your question, Sarah. All right. Uh, define j and d. I didn't understand that. Um, okay, uh, let's keep moving. Um, hopefully things will get clearer as we move forward. So again, the goal is to, uh, reduce these weights and biases to a smaller dimension, uh, by using, exploiting the principles of locality and translation variance. So let's first apply translation variance. So with translation variance, what we are saying is irrespective of where, um, the, uh, input image is, um, like, you know, where in the input image is, it doesn't matter, like the, like, even if it travels across INJ. Um, so, uh, we shouldn't see any change in the hidden representation. Like, let's say if the CAT is in, uh, like I said, if the cat is in, uh, this location or this location. So even if it travels in the dimensions of INJ, the hidden representation of the CAT itself should not change. So by that, we can essentially take this equation and reduce it to let use a different color. You can reduce the IJ terms completely and remove the IJ terms completely. So that would become something like h IJ equal to your bias. You can completely remove the IJ terms because it is translation variant. And w you can you remove the IJ terms because again, we, because of invoking the translation variance functionality and XAJ plus B, so, so we are left with, so we went from, you know, uh, a two dimensional, um, bias, uh, a fourth degree tensor weight to, uh, uh, one dimensional, uh, bias and a second degree, uh, weight by invoking the translation variance. Because what we are saying is by translation variance, irrespective of IJ positions of the x, my hidden layer should not change. And that would not change if your biases and weights do not change if you travel across INJ dimensions. Um, so that's what we did here. So this is translational in variance. So now let's see what our, um, order became. So, so we are still with ten four six here, uh, ten four six, uh, given this is traversing across two dimensions only, A and B now, so it's also order of 10, 4 6. Um, yeah, so that's, uh, that's the, uh, dimensions. Uh, we are at, at in, in the order of ten six at the moment. So now let's invoke the principle of locality. So what we are saying here is we are still traveling across A and B, the entire, um, part of the image. So, uh, but my weights are translation variant, um, by principle of locality. What I'm say, what we are saying is, um, we don't need to traverse the entire image. We only traverse parts of the image at any point of time. So, so we only, instead of traversing from, um, for a, uh, let's see, since we have one me cell image, we have, um, zero oh oh. Alright, so, uh, sorry. Yeah. Uh, oh oh 0, 0 0 0, actually this is zero thousand. Okay. So anyways, so those are the coordinates of your, uh, one magix image. Uh, we're still taking one shot at the entire image, uh, while, you know, uh, while computing this hidden layer. Instead, the principle locality says, I only have to look at a small portion of the image at any given point of time to be able to glean insights from that image, uh, to be able to learn parts of that image. So this small portion technically, uh, can be a 10 by 10 area, uh, which is a decent size. As we look through, like, you know, how convolution filters work, uh, how this is explored, uh, we only have to look at a small part of the image at any given point of time instead of the entire image. So that brings this number ab to a, a small number instead of, you know, in the order of thousand, it will become, um, 10. Now. So, so instead of, uh, you know, traversing the entire image, uh, we're, we're just looking at a small portions of the image at any given point of time. So that brings this to an order of 10 power two. So we went from somewhere from 10 point 12 to 10.2 for just one mega pixel image in one layer, uh, by invoking the two principles of translation in variance. And, uh, locality, uh, we'll see how that is, uh, that works in action in terms of, you know, how convolution layers work, um, how the kernels and convolution filters work, how parameter sharing works. I think there was a question that was asked yesterday, we'll address that, what parameter sharing means. Uh, but, um, essentially that's, uh, that's the math, uh, in terms of trying to understand how, if you start with an FNN kind of network for a image problem and then re reduce it based on the principles of translation variance and locality to a, uh, CNN kind of, uh, network, uh, what would be the advantage be? Uh, as I said, again, this is more for, you know, trying to understand the math behind it. Um, this is, um, I can pretty much guarantee this is not an interview question or you won't use this in practice. Uh, but happy to take any questions. Uh, we can also reserve some time at the end for questions, uh, if you want to go deeper into math. Alright, so, uh, we went through this. Um, so essentially I'll, I'll get this one. Um, so yeah, so let's get back to, you know, some of the theoretical aspects of C nnn. Um, so convolution layers, uh, CNN again, you know, it stands for convolutional network. Um, and it's, the structure of it is designed and we'll see what are the salient features that make them useful for images, uh, why they're efficient for images, we see, we saw it from a math perspective. Now we'll see from the layers perspective, like, you know, why, how these layers are applying these kind of principles. Uh, so the key layers in CNN are convolutional layers. Uh, we have pooling layers, and there are, uh, flattening layers and fully connected layers. Uh, we'll, we'll talk about this flatten layers too. So, um, and we'll talk about each of these, uh, in detail in the next session. Um, um, in the convolution layers, uh, there are usually, uh, uh, uh, kernels, or you can call them convolution filters. Uh, these words are inter, inter, uh, uh, can be used interchangeably, uh, kernels or conation filters. Um, so these conation filters extract the local features, um, like we discussed earlier, by exploiting the principle of locality, instead of taking the entire image at once, they just look at parts of the image and, um, by sliding over the input data. So, so if you look at, take this as an image. So convolution filters, think of them as small windows, um, and they slide over parts of the image, um, at once tried at a time to extract features, um, and which, and those features will be used, uh, in further layers of the network. Um, and the small window is exploiting the principle of locality, uh, and the principle of translation in variance. So we'll look at each of these layers, uh, in detail, but, uh, those are the key salient layers of a convolution network. Until now, we have seen why FNN doesn't work for, um, you know, it's not efficient. It, it might work, it's not efficient for a, uh, uh, image kind of problem. Um, how we can exploit, uh, from a math perspective, um, the principles of locality and translation variance. Uh, we also looked at what those principles are. Uh, now we are trying to look at like from a convolutional, uh, uh, layers perspective, why these layers are structured in the way they are, uh, so that, uh, we are taking advantage of those, uh, principles. Uh, I think this is, uh, I think this is the animation slide, so let's use, okay, I can use this slide to explain the feature sharing actually. Um, so, so here, this is actually not a CNN. This is a f and n. Um, so, uh, here, uh, we just, uh, took an image and, uh, this is, uh, I think from the same problem. Yesterday. We took 28 by 28 pixels from, um, a handwritten digit, and we flattened that into one dimensional vector as input layer, and we used an f and n essentially as two layers. And then finally, um, uh, created a output layer with 10 classes, right? So as we can see, it's densely connected. Um, here, each hidden unit is essentially learning from all the, uh, input features. So there is no, like, you know, every hidden unit is connected to every input feature. So there is no kind of, you know, parameter sharing. It's not like, you know, only these hidden units are learning from this part of the, uh, input layer. And these hidden units are learning from this part of the input layer. Um, so that's the part we'll exploit with convolutions. So just to draw that out. So if, so, for example, if there is some information in this part of the layer that's being learned kind of by every unit, there is not, not much of, you know, sharing efficiency here. Um, instead in CNNs, we'll try to learn that once and try to reuse, uh, across different, uh, layers. We'll see how that happens. Cool. So, um, again, I think this picture is trying to indicate like, you know, how the principles of locality, um, is used. And also in CNNs, there is an aspect of hierarchical hierarchical learning. So if you think of, um, a C NNN network, as, you know, multiple layers, input layer one, layer two, layer three. Um, so in the initial layers, the intuition is to learn the edges and textures, uh, and the further layers as we go through. Um, the network learns the object parts and further down, uh, the entire objects themselves. Um, so instead of, um, so they do this by using a concept called kernels or local receptive fields, um, because you're only looking at small parts, and then you're looking at bigger parts later on. Um, so the way I could show this is in layer one, your filters are looking at smaller edges. And in layer two, the filters are looking at all of the information extracted by smaller filters in a previous layer. And in layer three, the filters are looking at all of the information extracted by filters in the layer, previous layer, the layer two. So it's kind of building on top of, um, uh, one, one another. And, uh, essentially, uh, the receptive field gets bigger and bigger. So the, the area that this filter looks at is called receptive field. And that receptive field is, uh, gets bigger and bigger as we pass through the layers. Uh, that's how the hierarchical learning is achieved. Uh, and then finally, once all that learning happens, all this information is flattened and connected to a fully connected layer where reasoning happens. So, so I've learned all these features, uh, now I want to make some reasons, like whether it's a dog or cat or, you know, or, um, some other, uh, answer. Uh, so, so that's, um, uh, that's how, uh, CNNs work, uh, by starting from, uh, smaller receptive fields to, uh, moving towards a bigger receptor field. So quickly touch base on the concept of, um, parameter sharing. Um, so here this filter is denoted by a list of weights. Uh, we'll look at it, what filter means? Oh, so a filter is, let's say a three by three filter is denoted by, you know, nine weights. And these weights are learned as part of the training process. But what we are doing is we are using the same filter and moving across the image, uh, through a process called convolution. So that's how the parameters are being shared. The same weights are used for the entire image. You are not learning one weight for this part or one weight for this part, but you're using the exact same weights. That's how the parameters are shared, and, uh, that's why the CNN um, networks are efficient. And then, uh, let's see. So, so for CNNs, we looked at principle of locality, looked at translation in variance, and then we looked at the architecture that, uh, is influenced due to this, which is you have convolution layers, Putting layers and fully connected layers. So with convolution layers, we looked at, you know, how there are filters or kernels that traverses across the image and this process called convolutions. And these filters have weights that are learned during the training process and shared, um, these filters are shared for the entire image. So, so that's the parameter sharing part. Um, so that's one aspect of one or one layer of CNNs. And the pooling layer, um, are used to down sample. Uh, so once you learn a lot of information from different filters, um, again, to reduce the information and reduce the noise, um, CNNs use a concept called pooling layers. So here in these layers, the information is downs sampled, meaning these are also like filters, but instead of filtering them, what they do is when the convolute, they just take in that receptor field, they take a average or max of these, uh, of all the values in that area, and convert the nine, uh, three by three nine, um, uh, values into one value essentially boils down to one value. So that's what happens in the pooling layer that try to down sample the number of dimensions. Um, and we'll look at, you know, uh, actual, like, you know, how the pooling layers work. Um, and, um, so, so with the process of convolutions and pooling, um, and then, um, using a fully connected layer for reasoning, uh, C Ns learn, uh, features in a hierarchical way. Okay? Um, this is just a mathematical representation of, uh, convolutions in, uh, 1D and, uh, 2D uh, essentially. Um, so, uh, FNG are two mattresses, and if you are convoluting, um, uh, across these mattresses, this, this is the mathematical representation of it. Cool. Let's talk about filters. Um, what they're, uh, why they're important. So, uh, in the early stages of CNN, um, before AlexNet, maybe in the, uh, stage of linnet or even earlier, um, scientists created these kernels by hand. Uh, like, you know, they know like, Hey, by applying this filter on this image, I can, um, extract edges, or I can extract, uh, identities, I can, or I can sharpen the image. So, so, so these are some filters created that way. So, uh, these are all hand created filters. So, um, so this is one type of filter where, you know, your, uh, the filter is, you know, just one in the middle and everything else is zero. So, as you can think of this intuitively, when you apply this filter on an image, right? So, so this is a 10 by 10 image, and you're applying this filter of three by three with only one in the center, right? So when you apply this filter, this is giving, you know, highest way to the center of this filter area at any given point of time. And then, uh, making zero everything else. And then you keep on moving, keep on moving, and do the same thing. So it's giving way to only the center part of the filter at any given point of time. That's what this one means. Um, so, so that's, that's why it's called an identity filter. It's try to extract the, the core information or salient information from the image. Again, these are hand created filters in practice. In, in CNN's today, these filters are learned. We don't hand create these filters. Uh, these are learned, all these values are learned as part of the training process. Um, so for example, if you look at this edge detection, so here we are trying to give, uh, more way to these edges, right? So, and not so much way to, to these kind of edges. Um, similarly, like, you know, in this, this is a different kind of edge direction where you're giving more way to, uh, you know, these edges, uh, and, uh, not so much to these edges. So, uh, so again, these are all hand created filters in the earlier days of CNN where scientists use these filters, applied them to extract, uh, you know, uh, certain, uh, aspects of the image. Like, for example, if you apply this filter, this would be the output. This would be the output for this, this kind of filter, so on and so forth. For sharpening, you know, when they used these kind of fitters, um, or blurring, uh, or gush and blur, they use these kinders. So, uh, again, just to give you an intuition of, you know, what filters mean and, um, uh, why, uh, how, you know, you can also hand create filter filters to do certain kind of things. Uh, let me take quick pause there. What is the major difference between f and share? The, so, as we've seen, right, so, so far we've seen that f and S are fully connected, and, uh, CNNs don't operate that way. CNNs have different, uh, types of layers called convolutional layers and, uh, pooling layers that are unique to them, um, that make them take advantage of, of the principles of an image, um, uh, to, to, to make it more efficient. Um, so we looked at like, you know, some of the hand created kernels earlier. So, so, uh, just talk, let's talk a little bit more about, you know, uh, what are these, why do we need them? Um, so, um, again, uh, kernels are small mattresses essentially. Uh, typically, you know, um, most architectures use a three by three, five by five or seven by seven, uh, in my experience. Um, and these kernels slide across the input image, uh, to, and during the sliding operation, they're trying to focus at any given point of time on a small, uh, part of the image, just like your eyes, right? Your, your, when you look at things here, you focus on certain parts of the image, and you try to learn, and then, you know, you, then you take the bigger picture, uh, and assimilate that, uh, just like that CNN is trying to, you know, using this filtered mechanism. It's trying to look at small parts of the image and, and then, you know, learn from, from that image and move on. Um, and during this filter process, it's essentially, uh, we'll look at the math. So, uh, is a, uh, is, is a dot product and a sum. So, so, so that's an image, and this is a filter essentially for every Excel you're doing a, so let's say X is the, um, input image, so XIJ times and filter is the filter value. So you have FIJ, uh, so you submit, um, by doing a dot product, straight dot product. And, um, um, and we'll look at this formula in the next slide. So, um, so again, uh, just want to give you a bit more intuition on what filters mean. Um, uh, as, as we move, move through this, uh, understanding of how kernels work. Um, so we don't have to use one filter. We can use, you know, multiple filters in a convolution layer. Typically, a convolution layer is one con layer equal to a number of filters, and each filter can be unique. Like, one filter can be detecting edges, one filter can be, uh, detecting texture. One filter can be detecting, you know, uh, a different aspect of the image. All of these are essentially learned as part of the training process. So a convolution layer is, think of it as a set of filters, and these filters are applied to the images and new feature maps are formed. So once you apply these filters, you get some new features, and those features become the input to the next layer. So, bringing this all together. So going back, I think this answer, I think someone asked a question while I was talking about, you know, hidden layers and stuff. So, um, yeah, so convolution layers, pooling layers, fully connected layers, all of these are hidden layers, right? Um, uh, if you talk in the sense of f and n, uh, only the input layer and all layer or, or, or, uh, external layers. So, uh, so instead of one generic vanilla hidden layer, like in f and n where it's a fully connected layer here, we have different kinds of hidden layers, uh, con pooling, um, uh, and, uh, fully connected as well. So that's, uh, uh, that's the, uh, that's how you, you need to think about that. Um, so, um, we talked about a lot of these in the previous slide. So, so input layer, again, receives the raw input, um, you know, such as an image or sequence. Uh, and it's usually represented as a grid, right? Uh, so it's, it's a grid data. So that's, that's what CNNs are efficient at. So that's input layer and conation layer are, these are core building blocks of CNNs. Uh, this consists of multiple filters, like n filters, and each filter by doing an operation called convolution. Uh, and by looking at a small region of the input, uh, they perform element, element wise, multiplication, and summations to produce a feature map. So, feature map, um, so convolution layers, you know, by using multiple filters, and through a process called convolutions, they produce feature maps. And the depth of, you know, the output feature maps correspond to the number of filters used. So if you have 10 filters, um, so, uh, and each filter produces a one feature map. So now you have a, a 10 dimensional depth, uh, uh, sorry, 10, 10 depth of 10 for, you know, uh, uh, in terms of feature maps, uh, as an output from the convolution layer, uh, activation functions, we know about them. There is nothing new here. So in CNNs, typically, uh, relu is a very common activation function. Um, sigmoid and tan hs are also used, but relu is a very common activation function used in, uh, con, uh, CNNs pooling layers. Uh, like I said, pooling layers, uh, given convolution layers create lot of feature maps and increase the dimensionality. Pooling layers are used to reduce the spatial dimensions by downsampling. Uh, the most common pooling operation is Maxs. Uh, so you just take in a given receptor field, what is the max of this entire, um, uh, area. Uh, but average is also, uh, used in certain scenarios. Uh, the, the essence of pooling the intuition behind pooling is capturing the most salient features, uh, and reducing the com computational complexity. And then fully connected layers, uh, like these are the same kind of layers we've seen in f and m. Um, uh, these are also called dense layers. Uh, these are used, uh, at the end of CNN architecture to perform high level reasoning and classification. Um, these layers capture complex relationships between features, uh, learned by, you know, preceding convolutional and pooling layers, and output layer produces a final predictions. Um, the structure of the output layer, um, it depends on, you know, what task, what is the task at hand. Like, for example, in image classification, the output layer may consist of, um, soft max, like, you know, soft max activation across different classes. Uh, like you can a regression task, like let's say you're trying to output the bonding box of the cat, uh, then it's a regression task, so it'll output, uh, um, uh, a single neuron providing some sort of continuous prediction, like this is the, uh, pixel, uh, location for that bonding box. So bring this all together. So this is how A CNN network looks, right? So you have your input image, and in the, this is your convolution layer. Um, and in the convolution layer, as you can see, there are multiple feature maps, uh, sorry, multiple kernels. Uh, and each kernel is producing, uh, multiple feature maps. Um, so, and all these feature maps are go through a pooling layer where it is down sample to a smaller size. Um, you can do another iteration of convolution and pooling depending on, you know, the learning complexity. So there is another convolution layer that's creating more feature maps, and then downsampling using pooling layer. And finally, uh, we put all of this into a flattened layer in, in one dimension, uh, which would be used towards a fully connected layer, uh, where the final classification is happening, uh, of, of, you know, uh, of, for the particular task. In this case, it's a classification task for across three classes. Um, so that's how, um, and all of these are hidden layers, like someone asked, like, all of these are hidden layers. Um, if you think of this in the aspect of, you know, uh, input hidden and output layers. Alright, uh, let's see some questions. Can there be more than one such trial in the full network? What is trial? Uh, I, I didn't understand what trial means. Uh, so maybe you can help out there, type it in the chat window. Uh, okay, let's keep moving. Yeah, yeah. So, yeah. Yes, yes. Um, there would be, uh, more than, you know, um, uh, one combination or two combinations of these, uh, combination cooling. We'll see that more of that in how some of the more popular architectures like, um, inception net or google net, uh, use these layers. Uh, yes, there is, there are multiple, actually, there are seven times these combinations or more, uh, in most popular networks. Would you lose spatial relationship in the flattened layer? Um, so we already learned all the spatial relationships in the previous layers. Um, so now is the time to actually use that information to make decisions. So, so, yes, then no kind of, so, so those patient relationships, like, you know, if you take one feature map, so the way this flattening happens is it takes the entire row and makes it as, as, you know, one vector and then stacks on top of each other like this. Uh, sorry, my colors are not that great here. So, so it takes a, um, feature map and essentially takes this whole thing and transposes it like this, and this one transposes here, this one trans here. So there is some, uh, spatial relationships still maintained because, you know, uh, the, these features are still, you know, closer to each other, but we've already taken advantage of the learning part of those spatial features. Like, you know, these were close to each other, we already learned and gleaned the information and insights, and now it's time to use that information to make decisions, uh, if that makes sense. Uh, good questions. Uh, and I haven't sent, so, uh, let, feel free to type more if those haven't been answered. Uh, I'm trying to find my mouse. Oops, where is it? Okay, anyways, um, okay, cool. Yeah. Alright. Um, so, um, you might have, you might hear a term called channel, uh, in convolution networks. So, uh, channel refers to just one of the dimensions, um, uh, of the image. So in image, you have your typical heightened width dimension, right? And there is also a channel dimension. Uh, if in a simplistic terms in RGB, you can think of RGB S3 channels. So, uh, if you take a gray scale image, um, there is just one channel, uh, with, you know, you've seen the values, right? Zero to 2 55. So now when you make that A RGB image color image, so you have three channels. So you have the same metrics for, uh, you know, the green values and the red values and, uh, blue values as well. Um, so, so, but, uh, that's a very simplistic definition of channel. Um, so as you see how CNNs work, um, CNN's take input image, apply the kernels and create feature maps, right? The feature maps are also called channels. So, so if you apply 10 filters, you are creating 10 channels, essentially. Uh, so that's, um, that's the, that's how channel terminology is used in convolution, neur networks. Um, yeah. Now let's dive deep into the convolution operation. Um, so as you've seen, uh, earlier, uh, convolution operation, um, essentially involves applying a set of learnable filters, uh, to the input data, sliding them over, uh, the input, uh, sequentially and performing element wise, multiplication and suming to extract local features. Uh, so this is the core operation in the convolution layer, and a convolution layer is essentially a set of filters, and each filter is slided over the image. Uh, so here is a math, so, so if this is an input image, and this is the filter. So essentially when you slide this, uh, filter over the input image, uh, we are doing a element wise, multiplication and summation. So essentially, um, uh, in this particular case, we are doing seven into one plus two times zero, plus three times minus one plus four times one plus zero times five, plus three times minus one one plus times minus one. So let's see if we can get the answer of six here. Seven, um, minus three, four plus 4, 8, 8 minus 3, 5, 5 plus 3, 8, 8 minus two six. Yeah. So, so that's, um, that's essentially how, you know, a filter is applied to a part of, um, input image, and you come up with a particular, um, uh, feature for that part of the image. And the, the part of convolution is you do this sequentially multiple times, right? So in the next convolution, this filter slides to this portion of the image, uh, if the stride, stride factor is one. So you are, you are striding by one, one step at a time. Now this value is created, and in the next convolution, uh, it slides by one more, and, uh, this value is created and so on and forth. So this what this is created, this is the feature map. So this is one feature map created from one filter. Um, and as I said, a convolution layer is a combination of multiple filters. So there are multiple feature maps like this created, uh, based on, you know, um, different filters. So let's say you have four filters. There are four feature maps created. So here is a map, like, you know, uh, what is the dimension of the feature map created, uh, based on the size of the filter and the size of the input. So if you take, uh, your input, uh, size as you know, W one and H one width and height, also in this case, it's uh, five, five by five. So w one equal five, H one equal five. And if you filter size, in this case, the filter size is three. Uh, that's your f and, uh, padding, uh, we'll talk about padding and what that means and why we need to do that. Here. We are not doing any padding. We are doing a zero padding. So p is zero. So if you apply, and stride size is step, size is one. Um, so, uh, because we are only moving one at a time, um, so if you apply all that, um, you can come to the, these dimensions of three by three. Um, uh, and, um, so that's, that's essentially, you know, uh, how you can come to like, what is the output of my feature map, what are the output dimensions of the feature map through a formula? Uh, uh, again, you don't need to use these formulas and practical, this is just more for your understanding how the convolution filter creates these element wise, multiplication and summations to come up with feature maps. And what the dimensions of these feature maps are. A good question on the filter size. So, uh, as we look through like, you know, network architectures, uh, the filter sizes ease a, uh, hyper parameter essentially, so different, um, depending on, uh, so if you think of it as an intuition, uh, and your image has like very fine elements, like small elements, fine elements, you might want to think of smaller filter sizes. Like, you know, the smallest I've seen is a two by two filter. Um, uh, you can think of, you know, three by three, two by two small filters. But if you have like, you know, large features in image, um, and you know, uh, less finer details, you can go with, uh, bigger feature filter sizes as well. Typically, in CNN's, we see filter sizes of various sizes used, uh, in more popular architectures. Uh, and, uh, also as you go deeper into the network, you will see bigger filter sizes as well. Uh, hopefully that answers your question. Any questions on the tried, how the filter is applied and how the future map is created? Cool. Again, I want to to quickly tie it all together, like, you know, we looked at a lot of different kinds of math. So, so this is where with tone, right? So we are looking at small parts of the image by not looking at the entire image. This is how the filter work, and we've seen how those filters can create, uh, you know, small feature maps, uh, by looking at portions of the image. Yeah, we'll talk about padding altogether, uh, in a separate slide, uh, in a moment. Alright? Uh, I think this is an animation slide. Let me try this. Okay, so again, uh, shows here the filter is, uh, three by three, and the input size is five by five. Uh, and we are using, um, what is the stride size here? Just curious, uh, if anybody recognized that, how much is the stride? Yeah, exactly. Uh, uh, so we're, instead of moving one step at a time, it's actually moving two steps at a time. So that's why the output feature map is only two by two. If you saw the previous slide, like, uh, it's the same dimensions, right? Five by five input, three by three filter. But we ended up, uh, coming up with a future map of three by three because stride size is one. So the only ch thing that changed here is right size, and that changed the size of the feature map. Mm-Hmm, yep. Uh, yeah, aj, yeah, right on. So, uh, again, just to visualize how those, uh, convolutions work padding, um, there were a lot of questions around this. So, first of all, why, why do we need padding, uh, from an inclusion standpoint? So let's use a whiteboard for this. So, so we have an image, right? Um, let's, for sake of simplicity, let's do a three by three image. Uh, and let's say we are doing a two by two filter. So, so we're, we're looking at this part of the image coming up with some value for the feature map, and then we shift by one stride. We are looking at this part of the image now coming up with this value, uh, and then we do this, we do this, and, you know, we come up with all these values. Uh, but what you're seeing here is when you do these convolutions, you are inherently giving less voltage to the hedges, because every time a filter moves, so for the first convolution, uh, let's try this again. So for the first convolution, uh, the receptive field area is this whole part. So all these four blocks get, get to be seen by the filter in the second convolution. The receptive field area is this. So you, as you can see, these two blocks got seen twice, uh, versus this block was only seen once, and this block was only seen once. So there is an inherent disadvantage, uh, in the, in the way the convolution and strides work for the edges. So if you think your problem needs has like very important information around the edges, uh, it's important to pad it. So by padding, what we do is we take this input image of three by three, and we add just dummy pixels. So, uh, if you add one layer, it's called one padding. In the previous case, it's zero padding. If you add two layers, it's called two padding. Again, how much weight you want to give to the edges, uh, that's, uh, that's how much you want to pad. Typically, I've seen like, you know, either zero padding or one padding or two padding. That's the most I've seen. Uh, unless you know you have much bigger filter sizes, you might want to pad a little bit more. That's, uh, um, that's the intuition behind why we do padding is to give more important stages. Does it make sense? Hopefully that answers some of the questions earlier. Um, padding doesn't impact the, well, it's the other way around. If you have a bigger filter size, you might want to do a bigger padding, right? Because, you know, your area of, of, uh, dimensional of the filter is bigger, then you might want to do a, a larger padding. Um, that makes sense. So actually, yeah, so here, uh, again, uh, just what we just discussed is padding is a technique to add extra border pixels, um, to preserve the spatial dimensions and avoid information loss at the edges. Uh, so that's why we do, uh, padding. Um, and, um, again, I think, yeah, uh, these are just the same formulas, uh, we use. So, so if you are doing padding, uh, and it's not zero padding, you might wanna put the right numbers there to get to the right dimensions. Uh, we talked about that. So pooling. So next important operation of, uh, CNNs is pooling, as I said earlier, pooling is a down sampling operation. Um, so, um, in the pooling what happens is, uh, we take the feature maps created by the convolution filters. So let's say we created a feature map of three by three, and if we apply max pooling, so it just takes the max of all these values. Let's say one of the values is nine, and everything is like, you know, 1, 2, 5, 6, 0 minus one, uh, two, three. Uh, so the max pooling results in, in the value of nine. Uh, again, you do the pooling same way like you do, uh, in the convolutions, like you take a pooling, uh, filter and you pass it, convolute it, uh, in the same fashion, uh, across the input. Input. Here is the feature map, right? So you take the input feature map, and then, um, you do a pooling filter on top of it and convoluted. So from this feature map, you'll get like four values, uh, based on what the max is in this, uh, for, uh, uh, receptor field at any given point of time. Um, so just to make that a little bit more clearer. Um, so, so let's say this is one of the feature maps created from, uh, so input, you applied your filter and you got this feature map. Now we are applying pooling, and we got this, uh, feature map down sample feature map. So the pool pooling operation is essentially you take, um, uh, pooling filter sites, uh, in this case, uh, let's say my pooling filter size is two by two, and, uh, looks at these four Excels or four values and uses the max from this value, uh, and populates this. And next it looks at these four values, use the max and populate serve. Uh, so, so that's, it's a very similar congregation operation, except here you're not doing the element wise, multiplication and summation. Rather, you're just taking max of, of all the four, of all the parts of the receptive period, uh, ish. I didn't understand your question, but, uh, let's see. Um, yeah, f is the size of the filter here. Um, uh, yeah, so since the filters are equally dimensional, so if it's a two by two filter, applic, two, uh, that makes sense. Uh, again, the intuition, hand pooling, do we always need to use pooling? Uh, yes. Uh, typically it's, uh, uh, typically pooling layer is followed by a convolution layer. Um, again, the intuition is, uh, is to reduce the noise, right? So you are trying to generalize more by removing some, or losing some information that makes the network learn the right parameters. Uh, it's a kind of generalization technique that the network is using. Um, so yeah, cool. Again, um, for pooling layer, if you're trying to come up with the output dimension, uh, this is the formula I can use. Um, um, so, uh, just like similar to the formula we seen for the, uh, the convolution layer, um, here for the pooling layer, it's, it's this formula. Now here, there is no padding because, you know, you don't pad between, uh, uh, convolution and pooling layer. So again, uh, uh, this shows, uh, uh, in a better way, more better than what I've drawn. Uh, so this is how the pooling works. It's just taking the max of these, uh, receptor field, in this case seven, uh, and, uh, it's, uh, trying to, uh, populate that number into the pool feature map. So this would be three, uh, this would be nine, and this would be three, right? So, uh, that's, uh, that's how pooling works. Uh, and average pooling is, it just takes average, uh, of this, uh, entire, uh, set. Uh, so, uh, this is, in this case it could be one, uh, 16, four. Um, and two, um, so just intuition wise, like, you know, if you think of, uh, max pooling versus average pooling, um, average pooling, uh, is if you want to try to smoothen out the features, average pooling is used and a max pooling, we are trying to only extract the most salient feature, uh, of that feature map. Uh, then, you know, max pooling is used, so we already extracted the information from the edges in the convolution layers, uh, by giving them proper weightage through padding. So that's why, you know, when we do pooling, it's kind of t to use padding again. Uh, so it's not a, uh, a popular practice to you to that, right? Hope that answers your question ish. Alright, let's keep moving. So after you do all this feature maps, right, so you got your input image, um, and then you created bunch of feature maps through convolutions. Let's say I use three filters. And, and then, uh, those three filters resulted in three feature maps. Um, and then after pooling, I reduced those feature maps to smaller size by down sampling, and now it's time to flatten this. Uh, so you just take this and stack on top of each other, uh, and flatten this. So this information can be used for, um, uh, for further stages where, you know, a fully connected layer can be used. Um, so it's just a reshaping operation, uh, where you take, you know, these multidimensional feature maps and, uh, create, uh, translate into one di one dimensional vector. And, uh, these, um, uh, this helps to apply a fully connected layer, um, uh, so that, uh, you know, you can make more reasoning in terms of what the task is at hand. Cool. So here we are kind of branching out to, uh, uh, a slightly different topic of, or fitting, uh, uh, this is very important. Um, uh, this can be interview questions. These are very popular interview questions. Uh, and you'll also use this in practice, so pay attention. Um, I think I see intuition and max and average. So, so, yeah. So if you're taking the max, right? So then you are just boiling this whole thing to one most important feature from that area. So, so you're, if you're trying to extract the only, the most important features, like, you know, uh, let's say the darkest pixel, uh, or the, the brightest, uh, area of the image or things like that, then you are trying to, uh, you use a max pooling. Uh, however, with average pooling, you're trying to like smoothen the entire area to one mean number. Uh, like I don't care about the, the most darkest picture, but I just want what the average of this area is in terms of colorwise or things like that. So, uh, so that's, that's the intuition behind max and average. Pooling can be overlapping, uh, number, I didn't understand that, uh, question. It's a convolution operation, right? So, so it is inherently overlapping. Um, like, uh, you know, when you do pooling, you are doing, depending on the stride size, you have laps happening. That answers your question. Okay? So we looked at conation layers, filters, uh, the pooling layers, uh, and flattening layers. So before we go on to the, the entire architecture, uh, there is slight diversion here, you know, what are the different techniques that we use to prevent all fitting? These are not specific to CNN. These are, these can be applied to any, uh, neural networks, uh, sometimes even, you know, non neural networks as well. Um, so, so these are some techniques. So what is, or fitting before we go there, uh, what do you think is, or fitting, can any takers type in the Variance perform well in training Lee and bad in evaluation? Okay, good. Yeah. Tall fitting is when your model fits really well on the training data, but when you apply that to validation or even in test, uh, it is, uh, not doing well. So we try to learn noise more than actual, uh, pattern. Uh, so that's what, or fitting is. Again, high variance is also a good term. Uh, it need not be low bias, but it's definitely high variance. Um, so, so, uh, so yeah, uh, how do we prevent that? There are some techniques that are popular in the, uh, neural network world, uh, that can help prevent or fitting. The first one is dropout, um, and al l to regularization, uh, batch normalization, early stopping. These are some techniques, so we'll look at each of those in detail. Um, so just to be clear, so if you have your epoch and last, um, and training loss, this is for your training and sort validation. So, um, so this means like, at this point, you know, the network kind of, um, so this is the over fitting area. Uh, this is probably under fitting area. So, so, um, that's what, or fitting looks like. You know, when you look at loss, uh, uh, uh, in terms of between validation and training data sets, uh, and these are some techniques, dropout, L one and two, regularization, batch norm, uh, at least dropping out some techniques that are used to prevent this, uh, in practice. So, dropout. So with dropout, um, the biggest, um, uh, intuition here is you kind of, we kind of turn off some parts of the network, uh, randomly. Um, so let's say in epoch one, um, batch one, uh, we're turning off this neuron and this neuron. So this neuron is not used at all, uh, when in all the computations and in batch two PAC one, we are turning off this neuron and this neuron. Uh, so randomly you turn off certain parts of network, certain neurons, um, and, uh, this is designated by a factor dropout factor. Let's say I want to use a dropout factor of 0.3, uh, which means I'm turning off 30% of neurons, uh, randomly at any given point of time. So, so, uh, uh, we randomly by randomly turning off the neurons, we are forcing the non turned off neurons, uh, the, uh, uh, the disease to learn a little bit more about what these other neurons are doing. So intuitively, this is like the new, making the neurons good at their specialization, but also learn a little bit more about the nearby neurons. So, dropout, uh, is, uh, is a powerful regularization technique that helps improve the generalization ability by, um, actually forcing the neurons, uh, to, uh, not just overfit on one particular aspect, but also learn about the other aspects as well. Um, so that's how property is used. Uh, in, um, in practice, you just, uh, give a dropout factor, uh, uh, whether it's 0.5 or 0.3 or 0.4, and, and that is applied to, uh, the network in automated fashion across batches and, and the network learns better. Uh, the next one is, um, L one and L two regularization. Um, so, uh, L one is also called lasso. L two is called ridge. Uh, so again, this, this can come up as a good interview question. So with regularization, what we do is we, we already know a generic loss function, right? So, so in the case of, um, uh, let's say A MSC loss, uh, we know this is the loss function, right? So with regularization applied, we use a, uh, uh, an additional factor to the regression, uh, with a, um, regularization factor called Lambda. Um, and this is the additional factor we use for, um, uh, to regularize the network. So, so when certain weights become too big, um, so let's use a loss of ridge L one or two. So loss equal to, you know, uh, M-S-C-M-S-C loss plus regularization factor times. Um, we use a mod of all the weights, W one plus, W2, plus W three, and so on and so forth. So, so by providing that, uh, the network during the loss, computation and loss optimization, uh, if, if one of these weights is extremely big, um, by using the Lambda, um, uh, and the regularization factor, it tries to reduce these weights to a very small number, a zero. Uh, so if a rate is like, you know, too big, it tries to reduce that to zero. Um, and for ridge, uh, the first part is exactly the same. Uh, it would be W one square plus W2 square plus, so on so forth. So here, instead of making a bigger weight zero, uh, during the loss optimization, the network tries to, um, make it smaller. Uh, so you can think of this continually by, you know, if you, uh, dow by, uh, w so in this case, uh, uh, the weight can times one, right? So Dow W one by Dow W one, so it can 10 to one. Uh, so it can essentially, you can reduce the weight to a zero. Um, uh, if, uh, uh, if, if it's too empowering or too overpowering on a network here, it'll only boil down to two. So the L by W will boil down to two times lambda times. In this case, let's say if we're W one, W one. So in that particular case, you can't really make the weight down to zero, but you can make it a much small number during the optimization phase. Um, so, so this is how, you know, a regularization factor is added to a existing loss function, so that during optimization, um, you can essentially make those weights come down to a small number, uh, in rich, or make it a zero in, uh, uh, lasso or L one. Uh, so, so that way a certain weight cannot become too big, uh, uh, uh, too overpowering on other weights. And, uh, just so that recap, so how this all plays into the overall weight update is, you know, you have your weight new, called to weight, old minus, you know, learning rate times, um, double by OW, right? And that is this one, essentially. So, uh, and that's how kind of can control a larger weight, uh, into the weight updates. So the next, uh, method, uh, to reduce our fitting is called, called batch normalization. Uh, you all have experienced some normalization methods, right? Like standardization, uh, can someone, can someone share? They'll answer your question in a bit, but, um, can someone tell me like what normalization means? Cool. Uh, uh, so normalization is essentially you, uh, to normalize a variable. So you do the, some sort of, uh, you take the mean, uh, and you, uh, use a standard deviation. So, uh, there is an x, uh, variable. So X minus new by standard deviation. That's how you normalize a variable, right? Uh, so that's a standard normalization. You want to take a regular distribution and make it a normal distribution. Uh, you do this across all the values to, to make it a normal distribution. Um, so, so usually you do that in the, uh, initial input layer, uh, to make the network features, uh, in the same range so that the network is, is not struggling to, you know, fit differently to different features. Uh, but what happens as you, um, move through the network, so, so you have your input layer, and then you have your hidden layers, uh, H one, H two, right? And so on. So for the input layer, yes, you did your normalization well and good. Your features are all normalized. Uh, so, so your network is, you know, set up properly to, to learn from the features. But as you pass through this network, uh, and new inputs are created through activations, uh, and, uh, the range of the values change dramatically ba based on the activations. So batch normalization essentially a technique of taking this input, normalization and doing it across the layers as well. Uh, so, so by doing it, so essentially across each layer, uh, you're doing the same technique, you are taking an input, um, uh, and, uh, you are taking a mean from a batch, uh, standard deviation from the batch, and then normalizing it and using those values instead of the actual, um, um, values, actual activation values. One additional thing we are doing in batch normalization is a scaler and shift our operation. So, so this is the regular normalization operation. So you take an input, you remove the, uh, mean divide by standard deviation, uh, and then you take this and do use two, uh, um, parameter learnable parameters, beta and gamma, uh, to do a scale and shift. So, so you're just changing the scale by this multiplication and shifting it by p uh, so these are learnable parameters and give the network the ability to learn this, like, you know, uh, so think of it this way. So your input, your not optimizing all well and good. Now, in the hidden layer, you're doing batch normalization. So the first part is not normalization, and then second part is scale and shift. So you might want ask like, why scale and shift? Why not just normalize, right? So, so by normalizing, it's essentially taking all the input features and putting it in, uh, one normal range, so everything is in the same range, but network might benefit a bit more. Like if there is a slight variation in that, uh, uh, range, uh, like, you know, it might benefit a bit more by, you know, representing some of the hidden activations in a slightly different range. So that's why by adding these beta and gamma, uh, parameters, uh, you are giving the opportunity to the network to scale and shift in different ways, even these normalized values. So it gives a bit more representative power to the network. So, so you do normalization and you scale and shift, and then, uh, use those as your, uh, inputs, activations for the subsequent layers. So why is it called batch? So, because this operation is done in batches. So as you all know, like, you know, a, uh, training happens in epochs, an epoch is a combination of batches, uh, batches, like, you know, uh, so let's take an example. So let's say you have a, a training data set of let's say a thousand observations. And once you go through this entire thousand observations, that's one epoch. But before you even do that, uh, a the entire training data set is broken down into mini batches. Uh, that's, so let's say a batch size is a hundred. So for a batch size of a hundred, uh, you kind of do a forward propagation compute loss and back propagate. Um, and, uh, during that batch size, you are also doing this batch normalization. So, uh, you're doing the, uh, the mean and standard division within this, uh, a hundred batch size and scaling and shifting, uh, and learning the scaling and shifting parameters and, uh, doing the forward prop as well as, you know, backward propagation. So that's how, uh, you know, batch normalization, uh, works, uh, and it helps to, uh, reduce variances in the inputs to various hidden layers, uh, but also gives the network some control over, you know, how much those inputs should vary, uh, so that the network is able to learn better. Um, so the last one here, at least stopping, uh, there is no, no, no slide for that, but essentially it's fairly straightforward. So again, it's the ability to look at, you know, how the loss is progressing, uh, by monitoring both validation and training losses and using some sort of criteria that if validation loss, uh, is not decreasing, but the training loss is decreasing, uh, that's a, a good time to stop, uh, the training process. Uh, or if the validation loss is actually increasing, but, and the training loss is decreasing, that's also a good time to stop. So, so there are different stopping criteria you could use to do the at least stopping. Cool. So let's see what's next? Alright, uh, next we'll go through like, you know, different PNN architectures, um, that are out there. Uh, um, and, uh, these are like, you know, the, the most popular architectures that are used, uh, in practice, um, starting from Linnet. Um, it was, uh, introduced by Yan, uh, for, uh, handwritten digits. It's, uh, basically, I think it's back in 1998. Um, again, at this point, the convolution layers, pooling layers, all of these like, you know, are hand created filters, like, you know, uh, so they're essentially trying to find, uh, the right filters to detect edges and detect different features of the, uh, hand created features. Um, AlexNet, as I said yesterday, you know, uh, I think it 2012, this kind of revolutionized the CNN space. One is because in the paper they also introduced a, a data set called ImageNet, which is a huge open source data set. This is the first time, like, you know, that's such a huge data set, is open sourced, and everyone, everyone can start using and training from those data sets. Uh, so that was a big deal then, back then. Uh, so, so it introduced that huge data set, uh, as well as, you know, introduced, uh, architecture, uh, where, you know, the, how different convolution layers, cooling layers, and fully connected layers are stacked on top of each other. Um, so, uh, was a much deeper architecture. So lean was very simpler. Uh, with AlexNet, you have, you know, multiple stacks of these, uh, convolution, um, cooling layers stacked on top of each other, um, uh, before coming up with a final fully connected layer. Um, so if, if I believe, uh, AlexNet, um, was using, uh, filter sizes like, you know, three by three and five by five, uh, and, um, uh, Vnet is again, an offshoot from AlexNet, uh, that was introduced, uh, from Oxford. Um, uh, it has a very uniform architecture, uh, with very smaller sized filters. So this introduced like, you know, small size filters, um, so consistent small size filters to be able to extract, uh, information. Um, so that, again, these are all of these variations of different offshoots of AlexNet, uh, with slight improvements. So for V Gnet, they introduced small filters, um, with, uh, Google net or inception net, um, as it's famously called. So they introduced instead of one filter size, they introduced like, you know, multiple filters, sizes, like, you know, in the same layer, like there is three by three, five by five, seven by seven filters, uh, happening in the same layer in parallel, uh, to capture features at different scales. Uh, so, so because of this parallelization nature, again, Google Net is known for its efficiency and has achieved very high accuracy, uh, while reducing the number of parameters. Uh, resnet is also another, uh, famous innovation here. They introduce, introduce residual connections, uh, or skip connections, so skip connections or, uh, residual connections. Um, so with this, they, uh, what they're trying to address is, you know, with some of the previous architectures, uh, there was this, um, uh, vanishing gradient problem happening because you know how deeper these network got, um, uh, and, uh, that was kind of addressed through this, uh, skip connection. Uh, again, going through what Skip connection is and, and how it's structured is kind of out scope of this particular class. You'll learn it later in the CV classes. Um, but, um, but that's, that's the core, uh, edition in the resnet paper. Um, uh, dense net. Uh, again, uh, same thing like, you know, it's, it's another offshoot of how to alleviate the vanishing gradient problem, uh, and strengthen the, uh, feature propagation. So it introduced a different way called dense connections, uh, in the feet forward manner. Um, mobile net and efficient head. These are lighter weight versions, uh, of, of the, the more popular architectures. Uh, these are used in more edge device applications, like mobiles and embedded devices. Uh, so the parameters are pruned, quantized for efficiency, uh, while also maintaining the reasonable accuracy. These are kind of distilled variations of existing larger models. Uh, unit, um, is another, uh, important appre heavily used in semantic segmentation. Uh, so, uh, it uses also skip connections. Uh, the unit, the name comes from the fact that, you know, there are so many skip connections between different parts of the network. Um, and, um, it has like, you know, um, an ENC quarter pathway and decoder pathway, uh, kind, uh, which non samples and up samples, the feature maps to generate predictions. Uh, these, uh, unit is typically used like in, you know, if you saw like an autonomous driving, if you saw like semantic segmentation, right? Like different, uh, instances of the object being like, you know, clearly segmented. Like instead of abounding box on a human, uh, you can actually see a mask like very close at a pixel level. It's kind of, you know, uh, uh, classifying at a pixel level. So in those applications, it's definitely like, you know, unit kind of architecture is used, uh, to get to the pixel level, uh, classification and segmentation. Uh, here is a quick diagram of, uh, gold architecture. Um, yeah, so if you see this, yeah, so if you see, uh, this architecture more closer, oops, you can see like, you know, different, um, you know, filtering operations, uh, with different kernel sizes. You have one into one filter, three into three, five into five, um, and all of that, um, you know, are con needed, uh, together before moving on to another max pooling layer and convolution layer. So, again, a unique architecture that was introduced to be able to capture, um, features at different scales. In parallel, uh, this was, uh, become very famous for various, uh, image related, uh, tasks. Uh, so, uh, you can use the Google net architecture and pre-trained model, uh, for various downstream tasks. Uh, here is what I was alluding to in a, uh, resonate architecture. This is the residual connection I was talking about. Uh, so in order to, uh, avoid the, uh, vanishing gradient problem. So in the Resonate architecture, what, uh, the salient, uh, feature they introduced is as, as the inputs pass through different layers, uh, there is a skip connection here happening where you also added the input to the, uh, uh, to be concatenated, uh, to the output of the layers. Uh, so this was introduced in 2015. So, uh, so as the network grew deeper, um, it was observed that adding more layers, uh, led to more training error. Uh, this is not due to or fitting, but due to their degradation problem where information is, is failing to flow, uh, because of the depth of the architecture. Uh, another reason is the improving the ease of optimization. So instead of learning what, um, the, uh, FFX is, uh, now we are trying to learn what the difference of FFX from access. Uh, so this is a much smaller, uh, delta to learn by the network. So it made things easier, uh, through this residual connections. Uh, so the, those are the main reasons why, you know, uh, uh, why, uh, skip connections were introduced in the resident architecture, uh, to avoid one, um, training loss, training error or training degradation, and also improved training speed. So, uh, again, um, you look at the resonate architecture, there are these residual blocks, uh, where you, you're essentially taking the input from this part too, and then, uh, adding it, uh, to the output of the residual block. Uh, and you're doing that multiple times here. Uh, again, this is to, uh, allow for better information flow while also reducing the demand on the network to learn the entire function. Instead, just learn the delta of the function from this input. Um, so, uh, that's the whole intuition behind, uh, these keep connections in resonate architecture. Um, we talked about these, uh, earlier, uh, uh, transformer architecture. So transformers, they become famous in 2018 in the NLP space. Um, and they were quickly adapted in the image space as well. Uh, so some of the, yeah, uh, we can have a break at 11. Uh, I want to finish the CNN part before having the break. I hope you can understand, uh, just bear with me for 10, 15 more minutes. Uh, cool. Uh, so transformer architecture, uh, we are, uh, as I said, like, you know, there are screen transformers that are very famous state of that right now in the, uh, CB space. Uh, uh, the original twin transformer was, I believe, introduced from Microsoft Research, but, uh, essentially transformer architectures are, are, are kind of having state of the art accuracy and performance for image related tasks these days, right? Um, we talked about all of this, so just recapping, um, the advantages of CN and so fnn and MO mostly contrasting to fnn. So there is more parameter sharing. We saw how the same filter can be applied across, and it's just one filter weights. Uh, it's not like 10 filter weights, but the same filter can be applied across the entire image. Uh, and, uh, there are translation in variant through pooling operations and, uh, conation operations. Um, there is a reduced parameter count. We saw how we went from 10 code 12 to, you know, uh, 10, 10 to the order of 10.2. Um, they also use hierarchical learning. So small fitters, big fitters, much bigger fitters. So you go from edges to objects to bigger, you know, shapes to objects. Um, um, this is through the effective use of convolution and cooling layers. Um, and because you know how these network architectures are structured, like, you know, there is parallelism inherently. So multiple filters can happen in parallel. Um, thus speeding up the network, uh, uh, training capability, uh, limitations. Um, we still don't have the ability to retain memory in the network, like, you know, like, uh, what happened in the previous frame, or what happened in the, uh, uh, in the, in the next, next frame, or things like that. So there is no, no inherent memory in the network. Um, so, uh, that's under limitation. Uh, the networks also can't handle like different input sizes. Uh, inherent, you have to do some pre-processing to make sure the measures of, uh, are of the same size, uh, because your input layer is, is kind of standard the same size. Um, and, uh, again, these are super compute intensive, irrespective of the parallelism across filters. These are super computing intensive because, you know, uh, there is pooling operations, uh, uh, convolution operations, uh, fully connected operations happening. And, uh, and you do this across multiple times, uh, through the process of training. So, uh, uh, the requirements for com computation and memory are heavy. Um, also, there is a need for, you know, large scale data sets, uh, because, uh, to train a good network, uh, that is able to classify the reasonable accuracy, you need large training data. And for that, uh, the techniques like data augmentation, which we'll look at later, are used. Um, again, this is a fallacy for all neural networks, interpretability and explainability is not a, uh, a thing for, uh, you know, deep neural networks. Um, so those are some limitations of cnn. Uh, we've seen this yesterday, like, you know, object detection use cases, uh, as well as, you know, uh, object tracking use cases, uh, where your CNNs are applied in real life. Um, so we'll take a quick break, come back and go through the working example on CNN dataset. Um, so in the meantime, like if you have any questions, feel free to, you know, type in, uh, and I'll try to answer, but, uh, but yeah, I think we covered like a good chunk of the material. Uh, uh, and I think we're still on track of time. Uh, R ns, uh, is about like 20, 25 slides, uh, and there is a working example for r and n as well. Uh, so now we looked at FNN, and we looked at FCNN architecture, how CNN architecture is more suited for three, uh, you know, two dimensional spatial data, like images. Now we'll look at like RNN architecture, how it's more suited for, um, uh, uh, uh, different kind of data, uh, sequential data. Uh, and these are kind of like, you know, you, I want you to contrast like how these different network architectures evolved, uh, to serve different use cases and to be more efficient. Yeah, RNs have nothing to do with trustnet. Uh, so, uh, those are two different things, uh, rather not. Alright, uh, let's take a break, uh, 10 minute break and we'll come back at, uh, 1107. Alright, uh, see you in a bit, everyone. Hopefully everyone is back. So let's start going through the coding workbook for CNN. And I think, uh, there is also a poll launched for both TA and instructor. Uh, please give your feedback, uh, that would be very helpful. So, yeah. Um, so for the coding workbook, we are still using the same dataset like yesterday. Um, we are using the data dig recogni dataset, uh, but instead of using FNN for our digital classification, uh, we'll use the ENN network for, for today. So, and we'll see how it works. Um, want to do a quick check since, uh, uh, uh, I wanna make sure I'm audible, uh, and visible as well. Uh, someone can gimme some feedback. Awesome, thanks ish. Cool. Alright, so for this, uh, dataset, um, um, we'll use a different, uh, framework today. We'll use TensorFlow framework. Uh, uh, would you increase font a little bit? Yes, absolutely. So let's see better. So, uh, yeah, yeah. Cool. Alright. So, yeah, so basic libraries, uh, absolutely, um, basic libraries, uh, done by map for plotting cbo. Uh, again, uh, CBO is a interesting library for doing some beautiful plots and skeleton for, you know, basic trained test players and computation mattresses. And we'll primarily use carass and sensor flow. Uh, so from carass, we are importing sequential, that is like the toch NNN yesterday. So sequential is, is, um, is the, uh, module for, uh, models. Um, and, uh, uh, we're specifically importing the, these layer stands, dropout flat and con, uh, max Pool. Uh, we talked about all of these layers, uh, in the class in theory. Um, uh, for optimizer, we are using RMS prep. Yesterday we used aam. And again, just for, you know, variation here, uh, nothing, nothing different. Um, and then we are also using, uh, image data generator. We talked about how CNNs need more data and, uh, data augmentation is one of the technique we can use to, uh, uh, improve the amount of training data. And, uh, we are also using some, uh, stopping criteria by, um, uh, actually this is an, uh, learning data optimization criteria. Uh, we'll talk about it a little bit as well. So, uh, before I go into that, uh, let me see. Yeah, uh, we can keep moving. Um, so just connecting my drive here, Right? Uh, um, just loading the same data sets and looking at them, um, basic stuff, which we looked at yesterday. So I'll just paste the paste through those steps. Um, assigning the label, again, doing some basic sanity checks in terms of, you know, how, if there is any class imbalance or not. Nothing new here. It's the same, same thing from yesterday. So I'm just gonna pass through, uh, looking for any null values, uh, if there exist any. Um, so, uh, cool. So, uh, here we, we'll normalize the input data We talked about, uh, normalization before batch normalization. We talked about input normalization. Uh, here we are just doing a scale normalization. We, since we know that input pixel values are in the range of zero to 2 55, we're just dividing by two five to bring all the values in the range of zero to one. Uh, this will again, help with, uh, better optimization for the net, um, network, um, and then doing some basic reshaping into 28 by 28. Um, so one quick note here. Uh, the, the last one here is the channel size here. Since it's a gray scale at this point, the channel size is one. Uh, there, there is only one channel. Um, so Kara notation requires this additional, um, dimension at the end. Uh, it should be if the car, uh, if it's not, let me know. Uh, so, uh, I'll also paste this here just in case, But, Uh, yeah, uh, hopefully the TA or like operations can help with sharing this notebook. Cool. Uh, so, um, we want to encode the labels, uh, to, uh, categorical. Uh, so doing some basic label and coding, and then basic train test plate. Nothing new here. Here we are doing a split size of 90 10. Um, so 90% of training data for training, and 10% for validation. And then just visualizing some images. Um, cool. So next we'll go through how to define the model itself, uh, through Caras APIs. So here we'll look at, you know, um, the, um, the, so again, we are using the carass sequential, uh, module, uh, the model. And as you can see, the, uh, first layer is, you know, uh, number of filters. We're using 32 filters. So these are all the different kernels we'll use in the convolution layer. Uh, the first layer is the, we're adding a convolution layer and the kernel size, each filter size is five by five with rail activation. Uh, and we already know that the input shape is 28, 28 1. Um, so, um, so maybe tell me, uh, like, you know, um, we talked about this a little bit earlier. Uh, tell me like, if you, if I apply these many filters to the input shape of 28 28, what is the output dimension like? Uh, so this was a detail I mentioned, like, you know how think of, you know, how each filter creates feature maps. So one filter creates one feature map, right? So, so there are n filters in a convolution layer that create n feature maps. And, uh, so which means that, um, so in this case, we are applying 32 filters. So what is the output feature maps? Yeah, so, so yeah, so it would be 32, uh, feature maps, right? So, so in this case, the input dimensions now will be transformed to something like 28, 28 and 32 feature maps. Um, so padding equaled the same. That setting is very specific to the kras API. Uh, what it does is it keeps the input dimension and the feature map dimension the same. So 28, 28 remains the same. Um, so, so it does the padding in that way, such a way that, uh, we, uh, even with using the five by five filter for convolutions, we'll keep the output dimension the same, and that's what padding equal the same does. Um, so again, adding another convolution layer on top of this, uh, with, again, 32 more filters. Um, so, um, and keeping the padding equal to same. Um, so do you know what this would create? So this is the input dimension 28, 28 32. Yes, stride is default size one. Yep. Cool. So, um, it would be the same size because, you know, we're using the same number of, uh, filters. So the output dimension is also, uh, the same, B is the bad size here. So, um, so, but the actual dimensions of the feature maps would be the same. And then we are using a dropout layer with a dropout factor of, uh, 25%. Uh, we talked about how dropout can improve, uh, the normalization. Um, then using the a, um, we are using 64 filters now. So we started do 32 filter conation layers, did a max pull. And, um, after the max pull, we are using, uh, 64 conation filters. So, so that, uh, increased the, the number of, uh, dimensions to 64. And again, uh, again, another 64 layer, uh, with a max pool. So when you do max pool with a pool size, so you can use this formula. So this would reduce the dimension to seven by seven. Um, so we're taking 14, 14 64 and applying a convolution filter, uh, with 64 filters. So that will keep the dimensions the same. And now when we apply max pool, uh, with two by two, uh, pool size and stride two by two, uh, so that would reduce the dimensions, uh, because of the stride size, uh, um, you can apply the formula that we used in the class. So that would essentially bring it down to one by seven and 64. Um, again, we are doing another dropout, um, before flattening it. Um, and the flattened layer will have, um, these many, um, features. Um, we'll do a dense layer activation with, with 2 56, uh, notes, activations, um, also doing a final dropout layer with 0.5 and then final soft max with, uh, you know, how many classes, 10 classes, right? So that's our final output layer. Um, yeah, so we, we saw that max pool did reduce dimension, right? So 14, 14, uh, 64, the number of feature maps would not reduce, but you know, the, the dimensions would reduce, uh, to, uh, no, we started with 14. 14 to, uh, we came up with seven, seven. How do we pick the kennel size? Uh, why add a second conation to the layer? Yeah, good questions. Again, the, the network architecture, like, you know, some of the, uh, parameters here, like, you know, why these, these layers, why these kind of, uh, filter sizes? Uh, the intuition is, you know, if you look at some of the architectures like, you know, resnet or Google net, uh, they have very similar architectures, like, you know, you have multiple convolution layers followed by pooling layers and dropout. So there is some inspiration from those. The second thing is, again, it's something you have to play around for, for these tasks. Like, you know, what is the right architecture said, it's, I'm, I'm going to repeat the same answer I said yesterday, like, what is the right architecture for any given case? One, you can use literature to inspire. Like what is the, what are the different architectures that are being used for similar kind of problems? Two, uh, you can do some experimentation yourself, um, and find out, uh, what that is. Again, there is an active area research called, uh, near architecture set, uh, which kind of like takes the input and tries to come up with the right architecture. Uh, so, so all of these are kind of, you know, by, uh, uh, depending on the problem at hand, depending on what kind of problem you decide, uh, the network size and structure. Uh, sometimes it also depends on like if your input size is, you know, one megapixel and you're trying to come down to, you know, certain answer, like, you know, like a a 10 class, uh, output. Uh, so how do you down sample, uh, across the layers? Like how do you ups sample and downs sample? How much do you want to change? Uh, how much computation power do you have? How many parameters you can learn? All depends on, you know, the problem at hand. So there is no one answer like, you know, why we used, uh, a given network size or network architecture, then 10, because, you know, we need 10 classes, right? Uh, so it's, uh, fairly straightforward. So why we need 10 in output layer, uh, 2 56. We're trying to reduce the dimensions from this number to, uh, a smaller number before we even reduce to 10. So it's kind of, you know, proportional reduce, uh, from this number to, you know, a small number. Um, so, so yeah, why max pool? Is it trend? Uh, we talked about max pooling, right? So it's not maximizing the metric, it's, it's a ma, it's a, it's a, it's the most popular pooling technique. Um, so, uh, so max pooling takes whatever is in the, uh, in the field of view for that particular pooling filter and takes the max value. Um, so I'll keep moving. Um, I'll, uh, answer questions after the notebook, uh, as well. So for the optimizer, we are using RMS prop, um, with, uh, these default values, uh, learning rate 0.001, um, and decay of 0.0. Um, and we also used, uh, uh, reduced LR on latitude. So, um, uh, function. So for that, we will monitor the validation loss, uh, and for across three ocs, if the learning rate, uh, does not decrease, uh, then we reduce the, sorry, if the loss does not decrease, we reduce the learning rate by a factor of 0.5. So the new new learning rate is, is 0.5 of the previous learning rate, and we won't let it go below this number. The learning rate cannot go below this number. Um, so, so that's the, that's a learning rate. Any, any learn will use to optimize the learning rate if the learning becomes too, um, little. Um, so, uh, that's something new we'll use here. Batch size 86, uh, and we'll use number of iCal eight. Uh, we talked about data augmentation in theory earlier, so we'll use the image data generator, uh, function. Uh, this creates various versions of the images, uh, in real time during training phase. Uh, so it increases, explodes the training data, uh, so that, uh, we'll get more training data for, for our training purposes. So here are some, some things will apply, like, you know, we can rotate the image, um, zoom the image, um, you know, uh, and shift, uh, shift, uh, like, you know, if there is a number in this portion of the image, we'll shift it to the side. Uh, so we'll create all newer versions of these images by, uh, doing all these operations. Um, we're not doing horizontal flip and vertical flip because, you know, since these are numbers making, that will actually change the number itself, like nine can become a six. So we are not doing, uh, some of that. Um, and, um, so once we apply the data gen, we'll get, uh, more training data. Uh, this is only applied on the training data. Um, so for this loss, uh, here we are using a means code error loss, but, you know, categorical cross entropy is, uh, can be used as well. Um, so it's just to show the variation that, hey, we can also use this loss, uh, on this particular problem, um, by you instead of using properties, by using logics, we can use this means grid error loss. Um, so, but traditionally for class problems, yeah, categorically cross entropy loss is the right usage. Uh, here it is done just to show that we can also do that. Um, and then once we do that, let's, lemme run some of these cells. Alright, so one OC is one run through the entire training data set. Uh, one forward propagation is one, one sample, uh, and, um, one batch is, you know, when you consolidate the loss and do the back propagation. So we are fitting the model, uh, after compiling it. Um, so, uh, we'll see how the, and we're emitting certain metrics in terms of validation, loss and, um, training loss. Like we have a grid random technique for hyper. Is there any for fine suitable architecture? Yeah, uh, I think, uh, you know, I'll, I'll try to put, uh, the paper I was talking about. So let's see. Uh, so, uh, read through this, uh, saje, uh, maybe you can, you know, get more detail around that. Yeah, it's, it's similar to that, but, you know, use some sort of techniques to, to be able to find the right architecture set. Oops, I think I only sent it to. So while we are waiting on that model to train, um, again, basic confusion, metrics, uh, plotting, uh, so we can see like, you know, how the, uh, true and predicted labels are, looks like the model is doing well. And, uh, also look at some error cases. Um, so we can see like, you know, where the model is making errors and what are the reason for that. Um, so, uh, you can see some instances where, you know, the model is spreading, uh, incorrectly, uh, because you know, the labels are not, uh, properly or the handwritten digits are too noisy. And, uh, yeah, essentially, uh, that brings us to the end of the coding workbook. Uh, so the key things that, new things that you learned here are, you know, one is how to set up the Ks, uh, layers, um, and, uh, the kernel sizes, the filters, uh, the pool sizes, uh, dropouts, uh, and the flat and dense layers. Uh, second thing is, you know how to use a, uh, learning rate alr where you can use some sort of, uh, EMR to change the learning rate based on how the validation loss is progressing. Yeah. Uh, and also how to use an image data generator, uh, to improve your training data. Uh, yes, no brother, let me look at that. These are just a matter of preference, but in TensorFlow, yeah. Yeah, it is mostly a matter of preference. Back in the day, uh, TensorFlow was, was the go-to, for CNNs, uh, because, you know, uh, you know, how, how the CNNs got introduced, like through, you know, Google, um, and, um, um, and uh, through, through, through the researchers at Google and, um, uh, TensorFlow was also backed by Google. Um, so, uh, back in the day, TensorFlow was the go-to, and then they developed Caras framework on top of it to make it much more easier to, uh, it's an API framework on top of, uh, TensorFlow to make it much more easier to use. Uh, torch is, is, you know, coming from back by meta. Uh, so it gained a lot of popularity, uh, in the research world because of open source nature. Uh, even 10 is open source, but you know, there is a lot of research happening in touch and open source papers with code based on touch. Uh, so, uh, and it's also very easy. The APIs are very i and easy to use. So it's a matter of reference between touch and, uh, TensorFlow using directly TensorFlow will give you more, um, more, uh, configurability options, um, if you need, need that. Um, cool. So I think this is the confident metrics you're talking about number. So, uh, so again, this is, we are taking the, uh, let's look at the code. So we're taking the con, uh, data from validation, uh, and, uh, we're predicting, um, the values and plotting the predicted values, uh, across, uh, and, uh, across the true values. So you can see, like, you know, if the true label is zero and the predicted label is zero, then we are predicting it, right? Uh, but, uh, uh, if it's predicted, uh, as something else, then it is wrong. So we want to see high number in this diagonal. Uh, so that's what the confusion metrics is showing. Uh, so that's what we visualizing with this. So this will give you some visual indicators, like, hey, if a class is being predicted, uh, wrongly, like, you know, some class having high values here or here, uh, that will tell me that, you know, there are more errors happening in those instances. Uh, suggest, I don't think this notebook is available in by touch, so you, you can convert properly. Cool. Uh, so let's see if our model ran, uh, yeah, it's running a bit slow, so you can see like we are emitting, um, from OCH one. Um, you have a, a training loss of 0.02, validation loss 0.03, and we are using this much learning rate. Um, so we'll see how this progresses. Uh, let's not wait for it. Uh, we'll come back to it, uh, in, in a bit. So, Cool. Alright, so let's see. Okay, so just want to quickly summarize what we learned. Uh, we looked at what con neural network is. We looked at what their salient principles are, like, you know, locality and translation variance. Um, and we discussed about various layers, con layers, um, pooling layers, um, flattening and dense layers. Uh, we looked at, you know, various popular convolution network architectures like resnet, AlexNet, um, Google Net, and so on, so forth. Uh, we saw what the advantages of CNN, especially our FNNR and limitations in terms of, you know, sequential processing. Uh, as such, uh, we saw some applications and working example on a data set. Um, again, we also looked at, like in the convolution layer, we spent a lot of time on filters, uh, uh, you know, how filters are applied, uh, and how that filtering application, uh, filter application will result, result in a feature map, um, and so on and so forth. So now let's move on to, you know, I want to tease, uh, these things for, uh, to set us up for r and s. So, um, think of these questions, uh, and, uh, you know, have that in the back of your mind. Um, so that, uh, you know, as we learn about RNs, these will, you know, uh, we can connect the dots, Um, sequential limitation of the C nnn. So in the C nnn, we are still giving one input at a time, right? So we're not, um, so, uh, the CNN layer, you're giving input one and you are computing the feature maps and so on and so forth. So next, if you give input two, does the, does the CNN know anything about input one at that point of time? It doesn't, it does because it doesn't have any inherent recurrent connections, right? So that's the whole aspect of sequential limitation of cnn. Hopefully, uh, that helps and, and that, that's what we'll look into in the RN space. Uh, yeah, we're jumping a few steps ahead. Yes, RN is, uh, um, transformer is an evaluation, evaluation of RN architecture, but it's important to know why RN is useful and it's, it's like the backbone. Cool. Uh, let's keep moving. Uh, let's see. This is still running. Yeah, let it run. Um, we'll come back to it. Alright, here comes R Ns. Um, so again, we would like to know why RNN network architectures are suited for sequential problems, just like how we understood how CNN architectures are suited for, um, you know, spatial problems like images and stuff. So we'll talk about, you know, these things like introduction to RN principles, uh, different layers in an RNN types of RN models, advantages, limitations, applications, and finally, according walkthrough in the notebook. So what are sequences, right? So sequences can be, you know, a, a sequence of, um, words, or it could be a, a speech, speech is a sequence of sounds, or it could be, um, um, you know, a sequence of frames in a video, uh, or, uh, it could be like, uh, um, a sequence of decisions. Um, uh, so sequences are everywhere. Uh, it could be like, you know, in stock market example, you have sequence of, you know, stock prices at, uh, different time times. Uh, so that's a sequence of data. So for this example, we'll, we'll try to understand from a text point of view. So let's assume like, you know, you, you have this, um, particular, um, sentence modeling word properties is really difficult. Let's say you're trying to model or predict that sequence, right? So one way to think about naive way is say, Hey, I want to predict the property of the word modeling, property of the word, word, uh, and properties and so on and so forth. And, uh, and then multiply all of those properties. So that would give me the probability of this entire sentence. Uh, that's a very naive way of thinking, uh, because, and, and the naive nature comes from the fact that we are thinking that all of these things are independent to each other, but in reality they're not, you know, uh, uh, this has a lot of bearing on what the previous word is. This has a lot of bearing on what the previous two words are and so on and so forth. So there is this, um, conditional probability based on previous words. So, so if you put that into factor, so you can think of, you know, the probability can be represented as, you know, p of modeling. Now, for the word to occur, it's P of word given modeling. So P of X two given X one, uh, and, uh, it can be represented. P of p of probability is given a X three given X two and X one the world and modeling and so on and so forth. So you have this problem now, like you can, um, create the probabilities like this by, based on conditional probabilities of all the previous words. So how do you, how does this come into practice in terms of, you know, setting up the problem? Um, so you can think of like, you know, for there is a context at any given point of time, like all of this is the context of at any time, t all the previous words until the time T minus one. These are the context words that you need to be able to predict the word at at time T. So, uh, so that's the part where, you know, you have reliance on previous inputs, um, and there is, um, a need for carrying that context. So you can think of, you know, hey, I want to model this problem in a naive way by doing this, so I can create my labels and features like this. So, uh, I'll put like, you know, modeling as my feature and label is, you know, word, I'll put more word as features, and then ties is my label. So we can create these, all these different, um, you know, combinations based on the sequence of word occurrences. And, uh, you can, uh, model the problem. Uh, it, you see the problem right away that that results in a huge explosion of features. So that results in, so even with a 10 word sentence, um, that creates, you know, a lot of combinations. And second thing one is explosion of features of samples, not features. Two, uh, it's inefficient because you are learning this thing in one example, and you're learning this thing in the second example, but there is so much commonality between them, uh, which you're not capturing, uh, again, so that there is a lot of inefficiency, there is a lot of explosion of samples. So these are the things we are trying to avoid by creating an architecture that can be more efficient to take these into account. So for that, if we can think of a way that can summarize the context at any given point of time, like if I'm trying to predict ease, if all of this context is somehow concatenated into one feature, um, before that can be used into the model as a predictor, um, that would be useful. So think of that concept, uh, which will exploit in the, in the, in the next slides. So the key feature that, uh, we want to use with the R ls, gimme one second guys. So the key feature we're trying to exploit is, um, we're trying to, um, use a hidden state, or, you know, we're trying to use a context state, um, by, uh, which captures all the previous dependencies in the sequential data. So it's kind of maintaining internal state. Um, often this is often referred as hidden state or memory that represents the information processed from all the previous time steps. Um, at any given point of time, this hidden state is updated and passed as the network process subsequent inputs. So, so the hidden state is constantly updated with the input at a given point of time and then passed to the next time step. Um, so RNs, we know about back propagation, right? Uh, that's the technique that is used for CNNs and fns. For nns, we are, we use a technical back propagation through type because we are not only processing, uh, the inputs one after another, we are also processing a temporal dimension of the inputs, like, you know, uh, at the input at times, step T is dependent on all the inputs till times step T minus one. So there is a back propagation happening through, through time as well, and we look at how that works and, and, uh, so back propagation through time essentially computes gradients through time, uh, enabling the network to learn and update the weights, uh, based on the signal propagated through the sequence. So that's the core, uh, principle, uh, within RN that we want to explore further what is, how we can create this hidden state and how we can do the back propagation through time to be able to learn across time to be able to, uh, generate the next word in the sequence. Cool. So let me see if I can, so in a generic layer, like you have your input and your, uh, output, right? With RNN. So let's add a time dimension to it. So input at time t uh, output at time t there is a hidden state coming from time T minus one, and there is a new hidden state time T. And now this becomes the input at time t plus one. Um, uh, sorry, uh, there is another input and there is another hidden state, uh, so and so on and so forth. So that, that, that's how RN works by capturing this hidden state and constantly updating it. So it's not only processing the input, but also hidden state at any given point of time to produce the outputs. So, um, process sequential data by maintaining and utilizing this, uh, information from previous times steps. Um, they achieve this by introducing these connections called recurrent connections, uh, which allow the network to retain and update these hidden states, uh, which we looked at. Uh, so the main principle is one is sequential processing, so you're processing it one step at a time. So RNs are designed to handle sequential data where the order of elements matter. Uh, this can be like, you know, time series data or sentences in natural language or any other sequence of data points. The order matters. So the network process input one by one, taking into account the previous information. And then there is a step called hidden state update. So each time step an RN takes an input vector, the previous hidden state as input, and it computes a new hidden state, uh, just like what we looked here. So it takes input at times. Step T uh, a hidden state at times, step from times step T minus one, and produces a new hidden state as well as an output. So the information flow, uh, is in the RNN is through time. Um, and so the hidden state at each time step serves as both an output and an input for the next time step. So, so this becomes an input to the next time step weight sharing. So how do RNAs become more efficient? So the, the weights are shared through time. So what are the network weights that are being learned are not unique for each time step? So the weights are the same across the time steps. So, so what are the weights that the network is learning? Uh, so if you're passing, modeling word probabilities is difficult. So these are 1, 2, 3, 4, 5, 5 times steps. So through these five times steps, the RNN is learning one set of weights. So the net weights are shaded across the time. So that's where the efficiency of r and m comes. This weight sharing enables the network to learn and capture patterns and dependencies that occur across the sequential data. And then the technique used to train is called back propagation through time because the gradients are back propagated through the time, uh, time steps. So, uh, let's put this in in equations. Like, um, so if you roll this RNN network, like the same thing I showed you earlier, so if you have your input time, T HT minus one HT ot, you roll this out, this is how it looks like, you know, H zero, H one, H two, uh, and then you're combining H zero with some weights and with some inputs and, uh, putting some activation on top of it to create the next hidden state, H one and also an output state. Uh, why had one? So, so why is the output here? Um, h is the hidden state. Uh, X is the input. So essentially your, uh, probability of, of y uh, at, at a given timeframe is doing a soft max between y weights and hidden state. Um, and hidden state is computed by an activation of tanh over, um, you know, hidden state weights, previous hidden state input weights and current input. Um, and essentially for each time step, the loss is calculated by cross entropy laws. So, so each word has a probability, right? So, so it's measuring the loss between those two different classes by using the cross entropy. And that loss is calculated of after this is predicted, and it is back propagated through time. So that's, uh, how, you know, you can represent A RNN using math, uh, and different, this is all plain vanilla, RNN We'll look at different varieties, but, uh, this is how the inputs and hidden states are calculated, uh, across the layers of the RNN. So, let's look at it a bit more deeper. Um, uh, there was a slight convention change on this one. So here, um, you have inputs represented with X, um, and, uh, they have using a weight of u um, hidden states are using a weight of w uh, outputs represented as o uh, using a weight of V. Um, the hidden state, I believe is represented with s uh, so, um, so, so, yeah, uh, let's see how, how we can, um, you know, see this back propagation in practice. Um, so, so just to clarify, w here is right of hidden state, um, u here as weights of input XV is, uh, weights of y. Cool. So with that, we can come up with these basic ations, right? So HD is, you know, you take the input weight and, um, multiply with the input at a given times step T, and take the hidden state weight and multiply with the hidden state from times step T minus one. Uh, for simplicity sake, I'm not putting the activation around it, but there is an activation here like we've seen here. You know, uh, so there is an activation that happens here, um, and output at given times. T is always, you know, you use the output weight times the hidden state times. Step two, again, there is, in practice, there is an activation around it, which we're not using here. And loss is defined as overall loss over all the times, steps. Uh, we are comparing the output to the prediction using a loss function. In this case, it could be a soft max loss. Um, so it's, um, the loss is combined across summited across times, steps t equal to 1, 2, 3, and times. Steps are the number of s What is this? Uh, oh, here, uh, that is the hidden state. Instead of h uh, there is a change in conversion here. So, cool. So let's see how we can derive the various, um, uh, loss, um, derivatives, uh, for back backdrop getting. So we need three derivatives, right? So we need, um, so u derivative with respect to UVW. So U is your input weights. We use output rates, and these are hidden state weights. So we need wwl by W double now by Dow V. And now by now, W So if we have all these things, all these three things, we can now, uh, use those gradients to back, back, propagate the loss, uh, and, uh, compute the gradients. So, so we can, uh, do the back propagation through time. So let's see how we can get to that. So, uh, first thing is, uh, we'll try to compute what the, um, with respect to output rate is. So, um, so we know the loss is, you know, is computed as loss of, uh, you know, across all the time. Stepss, ot, YT. Um, so doel by DAO is essentially, um, using chain rule. We can represent it as doel by DA ot, uh, with multiplied by da OT divide. Uh, with respect to, with respect to do so, uh, we can expand this, this, each, each of these functions. So, let's see. I want to use a white page. Okay, so we have do, so you take this, this part. So we already know that OT is, um, V times hd, um, so now, uh, OT by the V that reduce S two hd. So that part is hd. So now, uh, expanding the loss. So we know that loss is, you know, one by T Sigma, uh, loss of ot, um, yt. So you expand that across time, but you're only derating with respect to one particular time timeframe. So that reduces it to double loss. ot, yt divided by ot. So, so dwell by D can be reduced to these two factors, because essentially if you're derating with respect to OT at point of time, everything else becomes zero. So this summation reduces to just one point of time, and this, uh, reduces to, uh, ht. So let's see. Uh, so yeah, so as you can see, uh, that reduces to one particular loss function, uh, at a given point of time with HD pose. And if you want to compute the overall double by, uh, we are, uh, we'll suming this across all the time steps T two T. Now, let's look at, uh, the next, um, der uh, for, uh, UNW. So we already know U is, uh, the waits for input, and w is wait for hidden state. So let's look at, um, how to get WT plus one respect to the W for that. Actually, um, I want to think this from a different standpoint. So think of, you know, how to get to loss. So we have weight at times, step T, and Hidden State at times Step T, that produces our output, right output at times Step T, sorry, um, uh, that produces our hidden state at times step T minus T plus one. And we use that hidden state at T one to produce the output at t plus one, because output is essentially, you know, V times, uh, HT plus one, right? So, uh, we produce the output based on hidden state at t plus one, and based on output at t plus one, uh, we produce a loss, because once you have the output, you can compare the output to the actual Y value and you can compute the loss. So, so we have this information flow from, uh, hidden State Times Step T to Hidden State Times step T plus one to output at t plus one to loss. Now, back propagate this loss. So, so we can write this as this is loss at T plus one, so loss at t plus one with respect to Dow W can written as, let me erase some, some of this stuff. So if you back propagate and write this, that can be expressed as, you know, you are doing the loss of time step T plus one with respect to output or T plus one and chain rule, I'm using chain rule, and you are aerating output of T plus one with respect to hidden state T plus one, and then hidden state T plus one with respect to hidden state T, and then hidden state T with respect to the weight of, uh, then the hidden state. So by applying chain rule backwards from the loss, you can get to an equation like this. Um, so let me use the, yeah, you can get to any question like this by applying chain rule backwards from the loss, uh, to get to, uh, the of loss with respect to hidden state weights. Um, essentially we'll apply the same thing for, uh, the you as well, you as the input weight, and we'll get to a similar, uh, equation per, uh, um, the DER rate of loss with respect to, uh, input rates. Uh, so this is how we can, uh, derivate and compute the DER rate of losses with respect to each of the weights by back propagating through time. I, uh, one quick note. Um, again, these equations are, you know, derived to understand how BPTT works in airplane vanilla, RNN, um, haven't seen this in my experience, like as these, these kind of ations asked in like interest, this is more for your understanding, uh, so that, you know, if you want to think through, like from your mind, like, you know how the data rate is propagate backwards by, you know, looking at, you know, uh, something like this, like, you know how the loss is calculated and then it's back propagated, uh, using chain rule. Um, it, it makes, makes sense for you for the, those kind of understandings. Um, cool. So, um, now we look at like, you know, some of the problems that would create, like, you know, we are using the same weights across the times steps, right? So we are using hidden same weights, WH and multiplying that the same weight. Were at times step zero, we're multiplying with WH to get to H one and times step one. We're multiplying again, the same weight, uh, to get to H two and so on till we get to ht. So if I take a number, uh, let's say X, um, X can be anything, and let's say I'm multiplying with 0.1, uh, what would this number be in related to X? Is it smaller or larger? It's, uh, smaller, right? So, so, and then if you keep on doing the same thing multiple times, so if you take this and multiply again with 0.1, um, so if you, and it may not be 0.1, it can be any number less than one, right? So, and keep on multiplying that. So this becomes super small. So as you propagate through time, if the weights are less than one, it can result in a vanishing gradient. So, so weights can become super small. And let's say if this number is greater than one, let's say it's 1.5, and if you keep on multiplying, it becomes exponentially larger. So that's the, um, exploding gradient problem that we see with, uh, uh, with these RNs. So those are the two, um, uh, uh, limitations or kind of disadvantages of plain RNs, um, that happen because of the way the back propagation through time happens using the same weight again and again. And we'll see how, how to mitigate that, uh, in the later sections. Um, again, this is again, uh, uh, you can do this through code to see, to visualize how the, uh, vanishing gradients happen. Um, um, so, uh, you can essentially replicate this in code and see how that works. Cool. So, um, now that we know why the vanishing gradients happen, um, so let's see, uh, you know, um, uh, learn a little bit more. So one, they occur when the gradients diminish exponentially or explode exponentially. Uh, when you are multiplying it iteratively across multiple types steps. So when you look at long sequences, like, you know, we looked at a 10, a six letter, a six word sentence, but let's say a sentence has a hundred words. So that's a long sequence. So when you do some operation a hundred times, so on long sequences, this becomes a more bigger problem. So those are some things that are limitations of it, plain man. So you can't handle long sequences inherently. Um, so, uh, on the other hand, you know, same thing exploiting gradients can also occur. Like, you know, if your weights are, you know, greater than one and you are using long, and you have long sequences as your inputs, uh, it can cause instability and, you know, prevent the network from converting because the gradients become one of the gradients become too big. Um, so RNs due to that are, you know, are not great at, you know, handling, uh, long sequences. So there comes the other types of RNs, like L SDMs, uh, which stand for long short-term networks, or G gated recurrent units and bi direct bidirectional RNs. Uh, these are different varieties of RNs, um, uh, which kind of, instead of, uh, looking at, um, so for r and n, we are looking in one direction, right? More word ties is hard. Instead, bidirectional RNs, uh, go through this direction as well as this direction with, uh, in one pass, the time steps would be T zero, T one, T two, T three like this. And in the other path, the times steps would be, this would be times step zero. This would be 1, 2, 3, and so on. So bidirectional, learn it in two directions as well, to capture the semantics better, uh, hierarchy and attention based. This is where we are getting into, like, you know, uh, these are the motivation for transformer architectures. So attention was actually used in ENS before it was, uh, you know, used in transformers. Um, and, um, we'll talk about L SDMs and gr, uh, which kind of mitigate the limitations of exploding and vanishing gradients to a larger extent. Um, uh, so we'll talk about that in detail. Uh, we just looked at, um, right now. Cool. So, uh, let's look at some of the salient features and advantages of ENS in general. So again, ENS are suited for, because of the way the back propagation through time happens. They're specifically designed for sequential data. Uh, this makes them well suited for tasks where order, uh, and, you know, temporal dependencies of data are important. Um, there is a context preservation happening. So they have the ability to maintain internal hidden state, like, you know, through updating a hidden state through time hd, uh, they're able to maintain the state of all the previous states somehow. Uh, this allows the network to retain and utilize information from previous time steps. Um, when processing the current time step, um, they, they can handle variable inputs. So, so that's one of the flexibility of r and m. So you don't have to preset the input size. Uh, it can handle, you know, multiple sent, uh, words in a sentence, um, easily, uh, and sentence sentences can have different lens. Uh, so RNs can adaptively processing inputs of different lens by adjusting their hidden states accordingly. Um, parameter sharing, uh, as we talked, the hidden states are shared. Uh, the weights are shared across time steps. So that's how the parameter sharing happens. So you're not creating one weight per time step, but rather the same weight is used across the time steps. This parameter sharing makes R ns computationally efficient, uh, and enables them to generalize better. Uh, time series analysis, uh, again, this is one area. R Ns can excel better, uh, like, uh, since time series analysis. Also temporal and sequential in nature, uh, like stock market production, whether forecasting, signal processing, and as such, uh, language modeling and generation is also another, uh, use case where our RNN is excel. Um, they can, you know, capture dependencies between words because language can be looked at as a sequence, right? And, um, uh, there is some temporal dependency there. Uh, that R ns excel at, uh, streaming data is another example. RNs are suitable for online learning scenarios. Um, they can process data in real time as it becomes available, uh, and maintaining that hidden state and updating it incrementally. Uh, so, uh, it is useful in those scenarios as well. Cool. Uh, so, so as we looked at vanishing and exploding gradients, so there, there are some limitations, right? So, uh, the RNs cannot process like very long sentences, so the weights become super small or super big, and the network struggles to learn, uh, when the sentences are big. So that's one inherent limitation of plain rrn, and there is no explicit memory state. Uh, everything is in the hidden state. Um, so, um, it's hard for RNs to maintain memory, uh, or conditional memory across the states. And this can limit their ability to capture and utilize context that span a large number of times, steps. Uh, since RNs are sequential in nature, uh, it is, uh, computationally, it restricts parallelization, like we saw in CNN's where you have multiple filters that can be paralyzed here, everything is sequential, so it restricts parallelization in that sense. Um, and, um, while R Ns can handle variable length inputs, they can face challenges when presented with like, very long inputs. So, so that's why, you know, it's still, there is this difficulty in handling, uh, sequences that are very long in length. Um, um, we see gradient vanishing and exploding before lack of paralyzation. Again, that ties back to, you know, sequential, uh, computation limitations. Um, uh, this one is interesting. So handling irregular times steps. So RN inherently assume that all times steps are equally spaced, uh, right? So, but let's say you are getting data that is, you know, this is times step T, this is times step T plus three, and this is T plus, you know, nine. Uh, so if you're getting irregular times, step data s don't inherently handle, they assume that, you know, all of these are equally spaced. Uh, so you need to have like some sort of pre-processing outside of RN to be able to handle that. So, let's see, some, you know, uh, real life areas. So, um, let's see this slide before. So let, there is some animations here. So, so, yeah. So essentially, I mean, um, ordinance are used for, uh, you know, in terms of natural language processing, widely used in NLP tasks, like, you know, language modeling, um, and, um, sentiment analysis. Uh, and as such, uh, they're used in time series analysis for stock market prediction, uh, speech and audio crossing. Uh, like Alex advisors, uh, RNs are implemented, uh, image and video captioning, again, sequence of frames where, you know, um, you take an image and take each frame and, uh, uh, and, uh, create, uh, captioning, uh, based on, you know, what's happening in those frames. So imagine video captioning is one other important aspect, uh, where is used. Um, so yeah, uh, so these are different real, real life use cases where an applications where RNs get used anywhere, you see sequential data, uh, especially like, you know, uh, text or signals that are spanned across temporal dimension RNs are, are, are practical implementation there. So, um, we'll go through a working example of RN dataset, but I want to quickly go through, uh, the remaining theoretical slides. So, uh, since, uh, I want to cover the theory first, and then we can go through the coding workbook. Let's go back to this. Um, so, so vanishing gradient problem, right? We've seen that, you know, because of the nature r and n use as the same way it's multiplied several times through time. You can see vanishing and exploding gradient problem. So how can we mitigate that? Uh, LSTM is one of the network car architecture. This is a sub, uh, you can say, uh, it's, it's, it's one of the architecture within the RN ecosystem, uh, which kind of mitigates that, uh, uh, vanishing in gradient, uh, vanishing gradient and exploding gradient problem, uh, through a different, uh, technique. Uh, we'll talk about what that technic is. So, LS TM stands for long short term memory. Uh, again, it's a type of RNN architecture. Um, it's under the same umbrella. So they're designed, lstm are designed to capture and remember information over long sequences. Um, this makes them suitable for tasks where understanding context and dependencies over a longer period of time is essential. So they do that by using a mechanism called gates. Uh, the gates in L ls TMS allow the model to control the flaw of information, uh, making it capable of learning to remember and forget information as needed. So in RN there is no forgetting part. There is all always, like, you know, hidden state gets keep on updating. Um, so here l SDMs have the ability to both remember and forget this makes L SDMs particularly effective for tasks in natural language processing. LS TM reigned the era for, for a while, like, you know, before transformers came. L SDMs are the go-to, for a lot of language tasks. Um, l SDMs, bidirectional L SDMs, as I said, like bidirectional meaning, like, you, you don't go through the sequence in one direction, but also in the opposite direction as well. Um, and, uh, they're heavily used for series like, you know, tasks like time series forecasting, language understanding, where, you know, longer time periods are, uh, are, are present. So we'll see how these, you know, gated mechanisms work in lstm. So the core, uh, areas of LSTM, uh, you know, we have the cell state, and then we have three gates, uh, input gate or GI gate and output gate. Uh, we'll look at each of these in detail, but those are the core components of lstm. So, cell state, think of it as like most of you are aware of, like, you know, some sort of assembly line, right? Conveyor belt kind of thing. So where, you know, things keep on moving through the manufacturing process. So think of cell state as that. So it's like a conveyor belt of information and gates are like, you know, um, or what control what gets put on this conveyor belt. So we have three gates, right? So we have your input gate. So input gate controls, what gets added to this convey belt, right? And then we have target gate so far, gate, gate controls what gets removed from this control belt, uh, con, uh, yeah, conveyor belt. And then we have output gate, um, which kind of, you know, uh, that controls what gets outputted, uh, as an output from this conveyor belt. So using this three different gate mechanism, input, gait, fargate, gait output, gait, we control the cell state, uh, c and that cell state, cell state is what passes through time, like, you know, uh, from T minus one to T just like our hidden state in plain vanillas. So with these more granular control through these gates, lstm achieve the, uh, ability to process long sequences without ex exploding or managing gradients. So as we discussed, um, you know, uh, fargate decides, um, what information we are going to retain and what we are going to filter out. So, uh, it uses this by using, uh, sigmoid function. Uh, you know, the output of sigmoid function is, is what? It's in the range of zero and one. So it can, you can usually use a sigmoid function to as an on off switch. So it applies to sigmoid function on the hidden state and current input, um, multiplied with certain weights and adding a bias. And it applies hidden state to create a fargate gate vector. So it's like 0 1, 0 0, 0, 0, whatever. So it's, let's say it's a dimension of, uh, you know, uh, it has 10 values in this vector. Um, and let's say we have a cell state of same dimension. Uh, cell state is 0.1, 0.3, 0.5, 0.7, so on and so forth. So we have your cell state at t and you have your fargate gate at time. T uh, keep this in mind. And the way this Fargate gate is created is by using a sigmoid function on the hidden state and, uh, current input. And that creates a list of binary values, zeros and ones. And similarly, there is also an input gate, which is created based on hidden state and, um, current input. Um, again, input gate is also a list of binary values of the same size. So you have 0, 1, 0, 0, 0, whatever. Uh, all of these are same dimension as the cell state. And for cell state, for every new input, we create a candidate vector. So this candidate vector is computed again, by using the same hidden state. And, uh, current input, we use 10 hitch activation in this case, because this we're creating a candidate cell state. Uh, this candidate cell state is, uh, is probably, again, same 10 number vector, but this would be, uh, between range of minus one to plus one because it's using 10 hitch activation. So by using the Fargate gate, we computed earlier, which is a binary, by using the input gate, we complete area, which is again, a binary, uh, vector. Uh, we apply that on previous cell state, which is a tanh, again, ranges in minus one to plus one, and on the candidate cell state minus one to plus one to create the next cell state. So here we're telling what to forget from a previous cell state, what to input from the current candidate cell state, and creating a new cell state. So that's how the L SDMs, you know, forget, have more granular control on what to forget what to input. All of these are learned parameters like, you know, wc, um, um, uh, w or learned parameters. And based on those learned para BIBC, uh, based on that, uh, the, uh, and wf, this is a learned parameter. Uh, BF is a learned parameter. So based on that, it's, uh, the target gates, the input gates that learning what to control and what not to control, uh, what to filter, what not to filter. Um, so sigmoid layers are used for binary, uh, creating something into binary tanh layers are used to zero center it, uh, because that inherently tanh makes, uh, squish everything between minus one and plus one. So, so that's how the gating mechanism works to update the cell state in a more granular way. In the LSTM networks, this level of control is not available in the plain vanilla s we looked at where the hidden state is, you know, updated with same set of weights. Uh, so that's, that makes, that's what makes the LSTM sufficient for long sequences. Uh, I'll come back to this. Uh, again, this is a bit more advanced topic, um, uh, constant error corro, uh, we'll come back to this and talk about this in a little bit. So, output gate, um, output gate, as I said, controls what is outputted from the LSTM network at a given point of time. Uh, the output gate is, again, uses a very similar structure. Uh, it uses the hidden state and, uh, input to create a binary output gate, like a bunch of zeros and ones, uh, again, same dimension as all the previous gates. Uh, this output gate is multiplied with 10 H of the current cell state to, uh, compute the current, uh, the next hidden state. Uh, so that's how, uh, you know, um, different, uh, gates are used to control what to, um, forget what to input and what to c uh, carry over for the next hidden state. So again, the conveyor belt analogy is, is the best way to visualize this. So think of that as, you know, information passing across the time steps. This is, this access is time, and cell state, uh, goes through various versions, C zero, C one, c, C two to ct, and all these different gates input, forget and output gates are used to update the cell state, uh, through various mechanisms. So input gate times input, candidate cell state connection, closed by. So let's see. Right. Let's see. Uh, okay, okay, we are back. Yeah, so, so, yeah. Um, so in, so the way, uh, a cell state is updated is, you know, uh, we use, um, uh, the f gate times the previous cell state, uh, plus, uh, input gate times the candidate cell state, and then the hidden state itself is updated using, um, the, uh, output gate at any given point of time. Um, so, so with use of these, all these, uh, gating mechanisms, there is more granular control in L SDMs, uh, to be able to, uh, uh, retain information. Uh, and that helps with, you know, vanishing gradient and exploding gradient problem significantly with L SDMs. Um, so again, uh, we talked about, you know, the L SDMs. Uh, what are the different, uh, okay, what are the different, um, you know, gating mechanisms within LSMs? Uh, I think in the previous section we talked about advantages of RN limitations and applications. Uh, we'll go through the working example dataset, uh, in a bit. Uh, so GRU, uh, I'll just hint, hint on that a bit. That material is not there here. So GRU stands for gated recurrent unit. Uh, they're also good at, um, uh, exploding and vanishing gradient problem. Uh, so you look at, uh, uh, so if you compare these three plain vanilla, R-N-N-L-S-T-M-G-R-U, so, so L SDMs can address vanishing and exploring gradient, uh, so does GRU. So GRU, instead of using three gates, like L sst m they use like, you know, reset gate and update gate. So they, they reduce the number of gates, but kind of implement similar functionality. So reset gate and update gate, uh, l SDMs use input or gate and output gate, um, G use because, you know, they reduce the number of parameters. Uh, they're kind of slightly more efficient than LSTM, but also, uh, less, uh, accurate, uh, compared to l sst m uh, so there is slight trade off between accuracy versus efficiency. So j work well, like, you know, in where, you know, you, you don't have much compute compared to what you have for lstm. So it's all in comparison. It's not like J are like, you know, you can put them on a mobile or something, but j are slightly more efficient than lstm. But, uh, they do that by giving up some accuracy. Um, so th that's, those are the main salient differences between g lstm and plain vanilla irons. Um, cool. So now let's go back to this one. Constant error corro. So in the way the R ns work, um, and in the way the r and architectures work, a particular error can keep on repeating. So for example, in LSDM, if fargate is set to the wrong values, uh, of zeros and ones, and an error occurs in the, uh, RNN, uh, the l SST m it keeps on repeating those errors if the forget gate doesn't forget the wrong thing. So that's what's meant by constant error carros, uh, it's mitigated by various techniques, uh, like teacher forcing and stuff. Uh, again, out of scope of this particular class, um, you might learn that in the NLP classes, but, um, what constant error carros in definition is, is because the way the gates work, sometimes Fargate gates can be set to the wrong states, and due to that, the same error can keep on getting propagated. So that's the, if you look, uh, like if you saw some in carnivals, like caral, they're like turning around and around, right? So it's the constant error carousal. Um, cool. Uh, so now with that said, let's see what other slides. Okay, so don't have more slides. So let's go through the coding workbook and let's see this finished. Uh, this is the CNN workbook. Uh, so, so we are running eight, right? So looks like, you know, uh, the loss training loss went down. Uh, validation loss also went down. Um, given the loss kept on going, our learning rate was never changed. It was the same. This didn't come into effect because we implemented a learning rate any either, right? So, so the learning rate, uh, did not reduce up for three ocs, then we will, uh, reduce it by half rate. So this did not come into effect here. Maybe if you run it for, like, if you try it on your own, like for like 10, 15 ocs, you can see that happen. Um, but yeah, so looks like the model converged. Well, uh, let's see if we can run these now. Yeah. Cool. Um, yeah, let's move on to, uh, RNN workbook. So here, I think we are using an LSTM, that's why I wanted to go through the LSTM material before, uh, showing you this workbook. So for this, we'll use, uh, a New York Stock Exchange data set. Um, the data span from 2010 to 2016. Um, and, uh, we are using stock for like your just apple, uh, for this one. Um, so, uh, and we'll use an LSTM network, uh, to predict the stock price, uh, in the next time step. So for this, again, we are using kras, uh, APIs from TensorFlow and all the basic imports in place. Let's see how this data set looks like, because this is the first time we are looking at it. Okay. So we have the stock symbol date, stock symbol. We have the open price, close price, low, high, and volume, uh, volume of the, like, you know, the stock transactions. Um, we also have some fundamental data related to stock tickers, like, you know, the period ending, uh, the accountable receivable, um, tax related stuff and all of that. I don't think we are using fundamentals for this particular exercise, but yeah, good to know. Um, so we'll only use Apple sticker, so we'll just filter the talk trading data for the sticker. Um, so again, just for Apple, we're looking at open, close, low, high volume, uh, values. Um, so we have 1762 rows here, um, over a period of, uh, five, six years. So just doing head and tail to see how the data set looks like. I think, I believe we have daily data from 2010 fourth January to 2016, December 30. So we are using the close prices for this. So not the open, not, not lower or high, but just the close prices. So just plotting that, uh, or a period of time. Um, so those are the close prices. Uh, looks like a big variation, uh, in the prices and doing some basic date, uh, manipulations, um, so that we can use the date appropriately. Um, so, uh, we are using a MinMax scaler. Uh, this is again, another form of normalization, just like we did with the pixel values. Uh, we are using a MinMax scaler here, so, uh, on the stock prices, because there is huge variation in the stock prices, right, like from 40 to one 20. So by using this carer, uh, that helps the LSTM to generalize better and learn better. Cool. Um, so splitting the dataset. Um, so we are using, um, 70% for training and, uh, 30% for testing. Um, so, um, we'll split the dataset as such. So maybe tell me one thing, like how do you, how can we split here? Can, does random split work? Just curious. So, so we have our talk data, right from 2010 to all the way to 2016. So how does, uh, what is the best way to split this data? Can I do just, uh, a scale train to split on this? So, uh, for temporal data, it's important to separate, uh, the data temporarily, uh, for 70%. Yeah, I think that's a good one. Wean. So especially when you are using temporal data, uh, it's important to separate them temporarily. So you can use, you know, trained data, let's say from 2010 to 2015 and test data from 2015 to 2016. Otherwise, uh, it, there will be a data leakage. So if the model is we are doing random, um, splitting, uh, then you're saying that the model can look at future data to predict the past, uh, examples or test it on the past examples, which is not right. So it's always important to temporarily separate the test validation and train samples, uh, especially when you're dealing with sequential data. And that's one difference you have to keep in mind compared to working with, uh, any normal, uh, data. In the previous example, like handwritten digit recognition, you, we don't care, like, you know, it could be split randomly, uh, and, uh, that, that, that wouldn't make any difference. But with temporal data, uh, you need to keep attention to the temporal scale and split randomly. Cool, uh, again, uh, doing some basic, um, metrics, uh, um, uh, conversions, uh, to, to be able to use the data, uh, for our lstm. So here, uh, just, uh, going back here, we are splitting it this way by using these indices. Uh, here, since the dataset is already sorted, it kind of works, but, uh, sometimes dataset is not always sorted, right? So, um, here looks like it's very well sorted already. So that's why by taking the first 70% indices, we are creating the train train set, and the next 30% of the samples, we are creating the test set. But sometimes the data set may, may not be sorted temporarily. So that's something to keep in mind. So for LSTM network, uh, as we discussed, like there are different kinds of, uh, gates and such, uh, again, we don't have to configure all of that. And, um, uh, when we are creating the network, um, so just like in the Caras implementation layer for CNN, we, we'll use a sequential module and use LSTM layer, uh, with the input shape, uh, of one. And, uh, so here we are using mean squared error because we are predicting the stock market prices, right? Closing prices. So we'll use the MSC error because it's a regression problem. Uh, after the LSTM layer, we are using a dense layer. Um, and, uh, for model optimization, we are using aada. Um, let's see, uh, what else, what other settings? Uh, so for the LSTM layer, the key things are, you know, we are using, um, 20, uh, LSTM blocks or 20 LSTM neurons. Um, and, and for lookback, I believe we are using lookback of, let's see, uh, 15. So we are using previous 15 time steps for look back. Uh, so, uh, so for any given data point, we are using a look back of 15. Um, so what that means is, uh, like, you know, if your data is, um, like, you know, 0 1, 2, 3, 4, 5, 15, 16, so, uh, for 15, we'll use everything from zero to 14 times steps as our, uh, uh, input window for times, steps 16, we'll use one to 15 as our look back. So that's what look back means. Um, Another thing, uh, I want to mention is like, you know, for LSTM, like, you know, you have your input layer and you have your LSTM layer. Uh, this is 20 block LSM, right? So all the recurrences are happening here, so don't think of, you know, for recurrent connection, there are additional layers. It's all happening internal to that layer, all the recurrent connections. So all the 15 look backs are happening here as you pass the input. So, so you only have input layer, LTM layer, and then, uh, fully connected layer, and then your output layer. So this kind of the recurrent nature of the LSTM back propagation through time is abstracted into this one particular layer, uh, which is kind of, uh, neat. Uh, you don't have to worry too much about, you know, all the time steps and all of that. No, uh, Vincent, uh, those are two different parameters. Uh, you can change these and see how the network, um, uh, will learn differently. So the Altium block is the size of the block. Look back is how much based on the input data, Hey, I only want to look back for 15 days. That's a look back. Uh, if I want to change that to, I only look back for 10 days of previous stock prices, that's a different look back. So those two are in not related, independent to each other. Cool. Uh, so that's the Lian structure. So let's start running this. Uh, we are running this for 20 bucks, uh, and see how the loss, uh, changes. Uh, luckily the lium is running pretty fast, So it looks like the loss is gradually, uh, reducing. Um, and, um, kind of, I don't know, kind of, uh, going back and forth in the last few steps. Um, so maybe there is an opportunity to reduce the learning rate here using the learning rate granular, but let's use the model for predictions and see how we do. Uh, so essentially using the model predict function and, uh, predicting, uh, given we use a MinMax scaler, uh, we should use an inverse transform to transform back based on the, um, MinMax scaling, um, and then calculate the mean squared error. So RMSE for train is 1.93. Uh, test is 3.3, not, not that great. So given the simplistic model, we are not doing that great in terms of, you know, test RMC. Um, so let's, uh, plot these, uh, see how this looks. So for that, we'll plot the actual values as well as the predicted values. Um, so even though this graph is not, you know, super informative, but it looks pretty close, uh, I think given RMC of 3.38, I think we're within like three or $4 range of the output for the most of the time. Yeah. So again, that's a quick demonstration of how LSTM can be trained. Uh, what are some, um, basic inputs to LSTM, uh, and how to set it up in the model block. Um, so here we are just training basic LSTM with, you know, 20 LSTM blocks and, uh, one dense layer, um, and an output layer. Um, but, uh, but yeah, uh, that's, uh, that's the end of the coding walkthrough. Um, so let me quickly go back and recap stuff, and then I can stay over for, um, you know, some questions. So in yesterday's session and today's session, we looked at three things. Uh, Adam is one of the optimizer type. Uh, so, so it's a combination of AdaGrad and RMS prop. Um, so read, read up on those optimizers. So, um, RMS prop uses a method called, um, it uses the, the mean square, uh, losses over a period of time, um, to, uh, to compute the gradients. AdaGrad is kind of, it's customized to each parameter. Uh, so it's a combination of both. It's a very efficient optimizer algorithm, uh, used, uh, in a lot of practical scenarios. Um, so going back to, um, you know, uh, our class over the last two days, so we looked at fs, um, we looked at CNNs, and we looked at s and essentially we kind of understood, uh, what are the core differences between these three architectures and why they're applied, or why they're, they're actually designed that, that way, um, because FMS are, you know, densely connected, right? Um, they're good for, you know, uh, when you have like, you know, features that are, uh, not spatial in nature or not sequential in nature. Uh, so they kind of learn very well, uh, across a densely connected feature set. CNN's kind of exploit the, uh, the principles of locality and translation variance, and use different kinds of techniques like, you know, um, convolution layer, uh, pooling layers, uh, to, to be able to, um, exploit that for more efficiency and, uh, better learning capabilities. Um, RN Ns, on the other hand, are good at dealing with sequential data, uh, and they kind of do that by using a technical back propagation through time and, uh, use various mechanisms like, you know, uh, for example, lstm uses gates, um, uh, and, uh, uh, gates to, to be able to retain and information through cell states and hidden states. And, uh, we looked at, like, you know, from a coding standpoint, we looked at like, you know, how to use, uh, uh, how, how PY touch framework can be used for FNN application, or, you know, a kras uh, sequential can be used for CNN application or kras. Uh, again, sequential LSTM block can be used for RNN application. Um, so, um, that's a quick summary of what we learned or went through in the last, uh, day and a half. Um, so I'll, I think that's the end of the lecture. So we'll, uh, we'll stay over for a few minutes here, uh, for any questions. Uh, so, uh, thank you for attending. Uh, I know it's a lot of new material to go through, but, um, I think, uh, once you get a more intuition around these basics, F-N-N-C-N and RN, uh, that will set you up well for the next, uh, lectures. I think this is one of the more, um, deeper classes because it's kind of touching base on several different concepts at the same time. So, uh, so if you can spend some time to learn a little bit more about these three architectures, that will set you up well for the next, uh, classes in the course. Uh, but again, thanks for attending. Uh, wish you all the best. And, um, yeah, uh, I don't think we have any GaN slides in this, uh, class. Uh, just to be clear, I'm happy to discuss from my own knowledge, but I don't think we have any GaN slides in this particular class. Cool. Um, again, thanks everyone for attending. Uh, I wish you all the best. Uh, we'll see you on the coaching class as well as the as interview class. Uh, I'll stay over for a bit for any questions, but feel free to drop off if you don't have any. Um, have a good rest of the weekend. Oh, um, so I think GaN is, uh, post class GaN video. Um, I'm not aware of that. Um, so maybe, um, point me to that or, you know, you can watch that and we can discuss like, if you have any questions on gans in the coaching session, uh, but, uh, yeah, in your given slides, Yeah, I'm not sure, um, typically gans are not covered in the sessions, or at least in my understanding. Um, but, you know, um, if the given video presentation is not, um, informative, I'm happy to discuss that in the coaching session. Vincent, uh, would it be good idea to add dropout players after every layer? Um, so what are the negative effects of too much dropout? Right. So, so too much dropout means you are essentially blocking the network from learning. So that's why you need to calibrate the dropout factor, uh, uh, uh, in the right way. Uh, so, uh, if you put like dropout equal to 0.7 or 0.8, um, that could cause like, you know, a significant, uh, learning impairment for the network. So, uh, dropout after every layer is not, uh, uncommon. I've seen that, uh, in CNN's after, like, you know, after every block, not every layer, uh, like every block of CNN pooling, then there is a dropout. Um, maybe after every layer, it's a bit too much. Um, so you think that would cause, uh, low bias in the network, uh, and it would cause lower learning capabilities. And also as I think about dropout, um, it is more beneficial to use after like a fully connected layer, because in convolution there is a sort of generalization happening, right? So you are not, uh, trying to, uh, um, uh, so the convolution process itself tries to look at, uh, different parts of the input data at different points of time, and there is like a pooling operation happening, which kind of de noises, um, the whole thing. So, so it's, it might be beneficial after every fully connected layer. Um, so that's, that's how I would see it. Uh, we are providing access to collab notebook. Um, I don't have access to the link you shared as well. Um, so a good feedback for the operations. So maybe send that note there. Uh, if you are not getting any response, I'm happy to help you there, but, uh, yeah, I don't have access to that notebook. You shared now number, but does open CV will play a role in CV glasses or, yeah, um, let me see. Best way to answer that. So, so open CV is a framework, right? So CNN is like more of an architecture. Um, so we, I don't think we use open cv. So Open CV is traditionally used in a lot of, you know, software engineering. Like, you know, there is open CV libraries for, um, you know, JavaScript or Node or, uh, Python to do a lot of image processing, like, you know, image manipulation, application transformations. Uh, so it has less to do with the actual ML aspects of it that we'll be learning. Um, so, so no, uh, so the answer to that is no. Okay, cool. Um, so, so I think when we can decide on time series, uh, so I, I'm guessing time series forecasting, you're thinking of arima or something like that, right? Yeah. Okay. Um, let's see. Um, I think one is the scale of data, right? The, um, and type of data. So, so if you are trying to model a very complex pattern, um, usually, you know, neural networks or like in this case specific type of neural network is LSTM is more applicable. Um, also model interpretability. So do you need the model to tell you why you came to that particular conclusion or not? So for that, you might want to use arima. So ARIMA is like, you know, you using progressive methods, right? So with moving averages, so it's very explainable. So simplicity and interpretability are important, go with arima. Um, and also it's very fast compared to lstm. LS DM takes a lot of time and needs a lot of compute. Um, so, but again, if there is less, uh, linear relationship, if the relationships are more non-linear, uh, ARIMA won't do great because it's just a moving average method. So in those cases, ts will, will be more beneficial. Uh, yeah, uh, uh, please beat the market and let us know. Uh, uh, uh, no, I, I, I think I know it's, it's a very, uh, uh, I, when I looked at it, it kind of, I kind of thought similarly, but, uh, the set is kind of a, a manufactured data set in my opinion. I don't think it's, uh, uh, uh, it's generalizable. What we are seeing that there is not generalizable. So stock market is very random. So, uh, uh, obviously there are a lot of hedge funds that use like very sophisticated models, um, uh, like, um, you know, not just past data, but also, you know, uh, factors from external, like, you know, news articles coming out, kind of news articles, the sentiment, uh, the volatility in the market, economic events happening. Uh, so there are a bunch of features that go into these models, like even data from, you know, what are the top stocks on Wall Street bets and stuff like that. Like all of this data is pumped into models these days to, uh, create algo algos. And these algos kind of, you know, uh, currently work in the stock markets, like a lot of hedge funds use those algos to trade, uh, especially during, you know, lunch hours and, uh, other, other, uh, low peak covers where you see like algos taking over manual trades. Uh, but again, uh, I don't think anyone perfected this, uh, market, uh, prediction in my opinion. So, yeah, you're welcome everyone. Uh, yeah, feel free to, you know, I'll stay for a few more, uh, uh, and I'll drop off at one time, but, uh, keep the questions coming. Uh, you know, happy to answer. I think, uh, at the end of the slides, there are some good topics. Uh, I want you to go through those, uh, on your own time and, uh, maybe, you know, ask me questions. Um, uh, a lot of things we already discussed, like, you know, our learning rate can affect, uh, or, you know, um, um, talked about learning rate schedule or how to change the learning rate based on what we are seeing in terms of loss. We talked about loss functions. Um, uh, I think contrast to loss is an important one. You can take a look, read about that. Um, loss versus metrics, I think this is a good one. Uh, it can help you. Like, maybe this can come up as an interview question. I don't know. Uh, this is a good one though. Um, why, why we use laws versus just the metric itself, um, or fitting under fitting. We talked about it. Um, we looked at it like, you know, training laws versus validation laws. Um, I think this goes back to gradient descent topic, uh, batch versus stochastic, how the updates happen. Um, we talked about this, and CNNs and RNs, like how weights are shared, like in RNs, uh, sharing is, uh, applied across time steps, right? And CNN's uh, sharing is used in convolution layers across the filters. Um, yeah. Right. Uh, thanks everyone. Um, I'm gonna close the session now. Um, um, so wish you all the best and, uh, see you all on, I think Tuesday and Thursday for the, uh, coaching and assignment, uh, review sessions. Thank you.