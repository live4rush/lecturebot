Uh, hey everyone. Um, this is, um, Navin. Uh, uh, I'm here to teach you the new architectures class today. Uh, quick, uh, brief overview about me. Um, I want to quickly introduce myself. Uh, I currently work as VP of AI and Engineering at Vouch. Uh, prior to that, I led teams at Amazon Private Brands Discovery, uh, for their applied science and engineering teams, uh, to deliver recommendations. I live in Seattle. I have a master's, uh, in computer science from Gers. And, um, most of my experience over the years is in NLP Computer Vision, uh, spaces. Um, so overall, 17, 17 years, uh, in, in various roles, uh, across these companies. Um, so that's a little bit about me. Uh, if you have a moment, uh, please pop in your, uh, name, um, your role, your company, or, uh, where you're working at, uh, and where you're based, just to get an, get a sense of, you know, uh, I want to know, uh, who my, uh, students are for today. So, uh, so take a moment to do that, uh, and post that in your chat windows. I'm nice to meet you. Morado ran awesome, uh, messaging IK team real quick. So, alright, let's keep moving. Um, so overall, um, um, I think you guys have been going through several different classes. So, neural architectures is, um, this is a class today, the ML 17 class. So today we'll learn about, um, various aspects of neural architecture, the Feedforward neural networks, CNNs and RNs. Uh, so we split the class into two sessions. Uh, so first day is, is about fns, and the next, uh, session is about, uh, CNNs and R Ns, just to give you a high level context. So, uh, yeah, so today we'll discuss about, um, uh, f and n. Uh, before that we'll do a quick review of, um, uh, various concepts you learned about neural networks, uh, and also intro to deep learning. And, um, um, and then we'll, we'll also go through a coding practice, um, for Feedforward neural networks. Uh, we'll have a short time for q and a, but, um, as, as you experienced in previous classes, uh, you can just ask questions during the class. Uh, just type it in the q and a section. If I ask questions and I'm requesting some participation, just type it in the chat window. So, so there is a distinction. So if you have question, type it in the q and a, either me or the TA present can answer those questions. I'll look at it, uh, in, in short time intervals. And I'll, I'll answer those questions, either live or through the chat, uh, feature. Uh, but if I ask questions for your participation, just type it in the webinar chat, just like you did. And, um, so that's the agenda for today. Uh, and, uh, if, if possible, I want to cover a little bit of CNNs today as well, uh, depending on how the, uh, uh, how much time we have left, um, that will give you, uh, give us more time for the next session so that we can spend more time on the second part of CNN's and, and instead of rushing them through. Um, so that's, uh, that's a brief agenda for today. Any questions on that before, uh, moving on? Awesome. Um, cool. I'm still working with the operations to help me become, uh, turn on my video, so just waiting on that. Uh, but we can keep moving. So, high level, um, want to get a sense like, you know, uh, from, from you guys, like, uh, like, uh, what are some techniques do you think that are involved in, you know, how YouTube generates subtitles for videos or, you know, when you unlock your iPhone, uh, using your face id, what's happening in the background? Um, just, just at a high level, like, you know, uh, no need of technical details. Like, what's, what's going on there, you know? All right. Uh, looks like my video is on. Um, cool. Back in business. Alright, good, good, good thinking there. So, think of inputs and outputs. So what is the input in each of these and what is output happening? Think of, think in those lines. That will give you some ideas, like what's going on there. So if you take the YouTube, right, so you are taking the, the video stream as input and generating text stream as output. So it's a, it's a conversion from one mod, one modality to another, but it's also a sequence. So it's a sequence to sequence problem. Um, it's a more advanced, um, uh, RNN type of problem. Um, sequence to sequence modeling, uh, which you learn, I think in the later stages of, um, NLP uh, co classes. Uh, but that's, that's what essentially is happening, uh, in the background, uh, for iPhone. Um, again, if you look at inputs and outputs, your face is the input and output is like a binary decision, yes or no. So it's kind of an object detection and classification problem. So, so the output is binary, so it's always giving you a zero or one, uh, whether to unlock or not. So think of along those lines. Uh, same thing with translate is it's a sequence to sequence problem. You have a sequence of text in one language and a sequence of text as output in another language. So a lot of, uh, good answers there. Um, so, so the goal is to, you know, look at, you know, some concrete ways on how these can be done, uh, which is the, uh, the motivation and goal of this course. Any more examples you could think of, like, you know, uh, where you see like, deep learning being used, uh, just like in the previous cases that we saw detection. Okay. Um, Understanding recommendations. Recommendations. Sure. Uh, it's a popular use case, um, where you are, uh, you know, you might, everyone of you might have experienced recommendations in various facets of your life, like, like, uh, shopping on Amazon or other e-commerce websites. Uh, search, yes. Um, so recommendations is again, a similarity problem or, uh, ranking problem. So where you have, uh, given an input list of, uh, x and a user, what is the right list of, uh, right rank list of X that we need to show to the user? Um, search is again, again, given an input string and a list of documents or links, uh, what is the right ranked list of documents. Um, so these are all ranking problems. Um, fake videos, I think, um, could be like, you know, um, an objection problem where you are looking for certain kind of artifacts in the image or video frames to see if this is real or fake. Um, yeah, all of good examples, right on the money there, Tesla, like lane recognition, autonomous driving, um, stock market predictions. Yeah. Yeah, absolutely. Uh, so all of these are good containers for deep learning problems for sure. Um, and we'll see why that's the case, uh, in a little bit. So just to quickly, uh, understand like, you know, uh, some quick fundamentals of neural networks, uh, we'll go through some of these topics, uh, that you might have already, um, learned. So let's look at what's in this particular section. So first, you know, we'll quickly introduce, you know, what deep learning is, uh, different ways to identify deep learning problems so far, you, you given based on your intuition, like, you know, what are some problems? Uh, but, um, uh, we'll try to understand, you know, based on characteristics of those problems. Like, you know, why this is a deep learning problem, what's this not, um, we'll look at, uh, deep learning versus, uh, new networks. And, um, and then also some real life applications of deep learning. Um, um, different advantages, challenges, uh, we'll also wrap it up by, uh, recap of loss functions, um, activation functions and gradient descent, uh, algorithm, which you might have already, uh, gone through. So that's a quick, uh, ou for this sections agenda before we move on to feed forward neural networks. Cool. So, so what is deep learning, um, in your experience? Like, you know, uh, um, if you want to take a shot at what deep learning is, what do you think it is? Cool. So it's essentially a branch of machine learning, uh, and, uh, it refers to architectures of neural networks where, uh, you know, which are composed of multiple layers, uh, of interconnected nodes. So, so, you know, uh, neural networks are kind of, you know, a generic term where you have, uh, different layers like, you know, input layer, hidden layer, and an output layer. But with deep learning, the, those number of hidden layers are huge. Um, and, um, so that's, that's why the term deep comes. Um, so it's essentially new network with lot of hidden layers, uh, and each layer receives input from previous layer, applies mathematical transformations, and produces their output. So that process keeps on repeating. Uh, in practice, a deep neural network can have, um, anywhere between dozen or, you know, even a hundred or even thousands of layers. Um, this, this sort of structure allows you to learn hierarchical representation. So, so each layer is learning from what the previous layers have already learned. So it's kind of building that hierarchy of learning. Uh, so that's why, um, the, the hierarchical learning comes from, uh, deep learning. Um, they're also capable of end-to-end learning. Uh, let me explain what that means. Um, so if you have built, uh, traditional classical ML models, um, you might have done like steps, like, you know, um, uh, transform the data, do some feature engineering and create features, um, and then apply the model, um, and, uh, evaluate and fine tune the model. So, deep learning is end-to-end in the sense that it doesn't involve creating features. You can create features and supply to it, but it's kind of t so deep learning is, is set up in a way that have to, you can rely on the network to learn the features as part of the model, uh, building process. Uh, so, so you don't have to hand create a lot of these features. And, uh, uh, the model itself learns them as part of the process. We'll look at it, uh, uh, in bit more detail in the next slides. But, um, um, that's one of the, the great, uh, aspects of, uh, deep learning. Um, as I said, each layer in deep neural network transforms input, um, uh, to a slightly more abstract, uh, representation. Uh, and for example, like if you take image, image recognition, uh, the first layer, um, let me see if I can use another slide, uh, for this. Okay. Um, so the first layer might identify like, you know, like an image recognition, um, might identify like, uh, abstract things like edges and stuff. Um, and the second layer init can, can learn from. These are just like small shapes, uh, and the next layer can learn bigger shapes from these small shapes. So, so that's the hierarchical learning I was referring to. And then in the final layer, you're making some logical conclusion about what, what the network has learned, like, you know, classify this as dog or cat or, or classify, uh, this as fraud or not, or, uh, in the face Id example, like, you know, classify this, whether it's it's the face that belongs to the person or not, things like that. So I told you how we'll look at how to identify deep planning problems. Uh, so these are some tenets. Um, so one is, you know, high dimensionality, uh, of data. Also, there is a lot of, um, uh, your data is very, very large dimensional. Um, uh, it's, uh, yeah, there could be unstructured data, uh, like, you know, images, uh, videos, uh, text, uh, um, um, you know, to begin with. Uh, it may not be tabular anymore. Um, and, um, you don't have a lot of domain knowledge. Uh, so, so like, you know, like, you know, uh, you don't, uh, have knowledge to do any feature engineering. So, so, so then you, you kind of require the model to automatically learn those, uh, patterns and features and representations from the data. Um, so, so these are some reasons or characteristics where, you know, um, you can, um, think of like, Hey, if you see a problem that kind of fits into this, these kind of buckets, that's a deep learning problem. Um, quick, quickly answering question from Rama. Um, FDL is a subset of ml y card ml take advantage of using. Absolutely. So, so this is how the, uh, the architectures have evolved, right? So, so in the, in the back in the day when deep learning is not popular, um, scientists and engineers really relied on doing a lot of feature engineering, uh, handcrafted feature engineering. And as deep learning evolved, uh, nowadays, um, even if you're using a non deep learning model for your modeling purpose, I've seen, uh, where, you know, you use, uh, auto encoders or other deep learning techniques to do the feature engineering and supply those features, uh, the layers as input to a ML model. So, so yes, it, uh, it is being taken advantage of. Uh, it's just how the things have evolved. Hope, hopefully that answers your question. Uh, cool. Um, so yes, so, uh, deep learning is a subset of, you know, how neur neur architectures, uh, neural networks are, uh, um, uh, architected. Um, so, so yes, it is, it is a subset of new networks. Alright, let's keep moving. Um, So again, as we, um, as deep learning was introduced, um, and become popular, um, what, what we've seen is, uh, deep planning is highly per performant. Um, so, uh, if you give, give deep planning models more data, um, it performs really well. So, so as you can see a representation in this graph, uh, like with the amount of data, um, the performance kind of, you know, scales linearly, um, very well, um, a lot of, you know, um, classical algorithms, um, sometimes, you know, with high dimensionality with, with more, uh, training data, they may not scale really well. And so, uh, so they have a plateau, uh, because it's, it's the capacity of the model to learn representations, uh, is plateaued for older algorithms or classical algorithms. Uh, deep learning excels in this, um, area, uh, because you can introduce a lot of complexity into the model, uh, by architecting it in the right way. It could be logistic regression or, you know, gradient boost, or, uh, it could be, uh, so I'm answering question for, um, what would be an example holder learning algorithm. Um, so you can put anything, uh, non deep learning there, um, SPMS or uh, HBUs and stuff like that. Alright, So this is something I was referring to earlier. Uh, so, uh, again, giving you a bit of intuition in how, uh, the hierarchical learning works. So, uh, so here is an face recognition problem applied, uh, using deep learning. And, um, here you can see in the layer one, the uc is kind of learning like all the, all the edges. Um, uh, and then in the layer two, learning the small shapes like eyes, nose, and mouth, and layer three, uh, you know, kind of learning the whole, putting all these shapes together and learning the face as an object. Um, so, so that's how the, you know, hierarchical learning happens in deep learning. Um, So, uh, as I said earlier, like, you know, um, I think we covered one, like lack of domain specific feature engineering is one of the consideration. While identifying deep learning problems, um, availability of compute is also a big one. Um, so, uh, so deep learning problems inherently require a lot of, uh, computational power. Um, so, so that's something, um, you need to keep in mind if you are trying to run a deal model, um, on, on your laptop, you know, uh, versus, you know, you have access to an, an AWS EC2 instance, for example, um, that, uh, you know, um, that is something to consider. So, so when you're trying to solve these problems, do you have that compute needed, uh, even if you have the data? Um, so, and, and deep learning, like, you know, the concept of it existed since eighties, uh, but it became popular more recently. Um, do you know why, uh, any, any, any takers there? Like, you know, uh, some history here, why it became popular intuition into why, why deep planning is so big thing today? More data, more compute. Awesome. Yeah, yeah. Um, what else? Yeah, yeah, yeah. I think these are all right answers. Um, uh, when we go to CNN's, I'll talk about this a little bit. Um, deep learning was, became popular through CNNs, actually, um, uh, convolutional neur networks. And, um, there was a paper in 2012 called AlexNet. Uh, you guys should go back and read it, uh, on your own time. But, uh, that sort of popularized the whole deep learning, uh, era. Um, and, um, there are several things that were introduced in the paper. One is the architecture itself, but the, the availability of this image net data set, uh, which is a huge data set that is available for, um, image recognition tasks. Uh, and then, uh, the whole, um, popularity of cloud computing at that point of time. Um, so I think cloud computing is becoming bigger and bigger, like around that timeframe, like 2012 ish, um, or even earlier. So, so those are some things that came together, um, and played a massive role in popularizing deep learning, um, uh, um, from, from that timeframe. Awesome. So, quick, quick overview of like, you know, how, what's the major difference between, um, deep learning versus in your network? As I said, in a neur network, you can have like an input layer, uh, and you combine inputs through, uh, several transformations and activations, and you create an output, um, layer in a simplistic neur network, you can have, like, you know, one input layer and one output layer and nothing happening. So you are just combining all the outputs in one go. But deep learning is essentially taking this and doing this several times, uh, in sequence or in some, in some networks you're also doing in parallel. Um, so, um, I think this, this diagram that you're seeing below is, is inception at Google net architecture, uh, which we'll look at later, but, um, uh, but essentially that's the whole big deal about, you know, uh, deep planning. It's the depth and complexity. Um, so there are multiple layers, um, whereas in neur networks, there is, it's shallow and, uh, with only, uh, one, uh, or no hidden layer. Um, um, and that gives advantage for deep learning, deep neural networks to learn hierarchical representations. Um, um, also, as I said, representation learning is another big thing. So, uh, deep learning emphasizes automatic feature extraction or representation, learning from raw data. Uh, so they're designed to learn and discover relevant features, uh, directly from the input data, uh, reducing the need for manual feature engineering, uh, workflows, um, and, uh, performance wise, like deep learning has like demonstrated exceptional performance in various domains, whether it's NLP or cv, uh, often sur surpassing, like, you know, uh, all the traditional, uh, ML approaches. Uh, this is, again, looking at, looking back at some of the slides earlier, this is new to the fact that they can learn, take advantage of large amounts of data. Uh, they can capture complex relationships, um, and, uh, but they also require like a lot of computation to do this. Um, interpretability wise, deep learning, uh, models are less interpretable. Uh, they're like a black box. Um, but, uh, you know, if you take a neural network compared to deep learning, uh, it's, it's a bit more interpretable, uh, than a deep neural network. So, um, uh, that's something to keep in mind in general. Uh, if you want interpretable, don't go with any of this. Probably, uh, uh, uh, linear, uh, models or tree based models, uh, like logistic regressions or, or, or random forest, or those are probably a better approach for interpretability. Alright, let me take some questions. I see some here. I think, uh, they're being answered by the ta. Thank you. Alright, we looked at some of this earlier, like, you know, um, why, uh, the need for deep learning. Um, so, um, complex patterns, uh, can be, uh, learned, um, uh, and representations can be learned automatically, um, uh, through deep learning methods. Um, deep learning also excels with transfer learning and pre-trained models. Um, uh, I don't know if we'll look at it in this chapter, but maybe in, in next classes. So pre-trade models or some foundational models that are trained on huge amounts of data, you can essentially take those models and, um, use that as your starting point instead of starting from scratch. Um, and fine tune them for your own use cases like, you know, uh, uh, but, uh, our GPT models are a good example of pre-trained models. They're trained on vast amounts of, um, uh, data and, uh, uh, language data, uh, especially. And, um, you can essentially, if you're building a sentiment analysis model, so you can take bird as your, uh, starting point and then fine tune that based on your training data to, uh, to create, uh, the sentiment analysis. Um, so, so that's, uh, deep learning excels in the, in the aspect of transfer learning, uh, taking preexisting foundational models and then using it for your own use case. Um, uh, and typically that's not possible with, you know, if you're taking like a tree based model or logistic regression model, um, it's, it's, you can do a lot of transfer learning there. Uh, these models are not curable, uh, or, you know, uh, extendable in that sense. Um, again, multi-model data, uh, can be handled inherently and naturally, uh, with deep learning. Uh, so that's one of the big advantages. So you can handle like different unstructured sources, uh, across different modalities, um, to, uh, and you don't even have to do any feature engineering. You can, you can start using those sources and, uh, build models, um, as an input to deep learning models. Um, in addition, like, you know, um, uh, there are several others, like, you know, deep learning models can, um, you know, automate end-to-end tasks because of the fact that they can do representational learning. Uh, they're more efficient with, with lot more data. Um, and, uh, they're very versatile in that sense. Um, so cool. I think this is an animation slide, so let's see. Yeah, so, so yeah. So again, these are some examples of, uh, uh, deep learning, uh, in real life. Uh, so, um, for example, like, you know, in the first one is the autonomous driving example. Uh, you are getting a, the model is getting a stream of frames, and it is, uh, doing object, uh, detection and classification here. Um, so, so every object is, uh, you're put, uh, the model is putting a bonding box around it, like where that object is, uh, uh, across all the frames and then classifying what that is. Um, so, um, this is, uh, objection is one of the core examples, uh, core, uh, uh, examples in CNN, uh, in computer vision. So it's, it's a use case, uh, that is used extensively and, and autonomous, um, driving. So, so that's, uh, that's an exact real life example, uh, where, you know, deep learning comes into play through object detection and classification. Um, uh, if you look at the bottom, uh, left, that's a classic spam filter example. So given a bunch of text classified as spam or not, um, uh, in the, in the, uh, oil days, this used to be a good, uh, model for, you know, nav based classifier or something like that. Uh, but, um, uh, with the, uh, language understanding that we have from deep learning and, uh, transformer models and, and the pre-trained models, um, uh, the, the text could be very complex, and those complexities are easily better identified through those, uh, deep planning models. So, spam fitters are nowadays mostly deep planning models, uh, that take advantage of, um, some pre-trained models as well. Um, so other examples like, you know, uh, like, uh, image generation here, like, you know, taking input image and creating, um, uh, various versions of that image, um, to like, you know, whether they're talking or doing some sort of action. Um, and, uh, stock market prediction is another example, uh, where you're taking a sequence of, you know, past data and predicting what's, what's going to happen in the future. We'll look at some of the examples, like I think in coding notebook for r and n, we use a stock market example, uh, if I remember correctly. All right, uh, moving on. Okay. So, as I said, uh, so these are the three kinds of networks we'll look at in depth, uh, in this class. Um, uh, feed forward neur networks, um, or, uh, FNN, uh, convolution, neur networks, uh, CNNs. These are, uh, heavily used in the computer vision space. Um, image, uh, recognition, object detection, um, object segmentation, kind of use cases, um, and RN ns or, or recurrent neural networks. Um, a lot of usage in the NLP or sequential data, like, you know, even stock market data, um, uh, language understanding. So we look at, you know, different, uh, aspects of RMS in that, uh, in this class as well. Lemme check, uh, there questions. Alright, um, uh, you might have gone through, uh, the loss functions in previous classes. So let's quickly recap. Um, uh, 'cause we'll, we'll be using some of those concepts in this class. So, um, maybe, um, uh, someone can tell me, uh, you know, uh, why we need a loss function, uh, or, uh, type in your answers. Uh, I wanna understand like, you know, um, if you guys, uh, know clearly why we use a loss function. Yeah, I think, uh, mostly right answers there. So, yeah, so when you are learning the network, you have a, the network is trying to predict something, and then there is a true value. So loss function is trying to minimize the gap between these two. So the network is trying to learn, um, as close to true as possible. So, so that's, that's a whole essence of loss function. Um, and, um, it's not an evaluation metric, but it's mostly, uh, a way for network to optimize, uh, the, the model, uh, so that we are learning things as close to true as possible rather than, uh, you know, missing those. So, with that, uh, in mind, uh, what are some loss functions, uh, that, you know, mean? AB error, MSC cross entropy. Hmm, okay. MSC. Cool. Um, so I think those are good starting points. Um, um, I will put all of these three in the same category. What, what kind of problems do, are they used to solve for R-M-S-E-M-A-E-M-S-E kind of losses? Yeah, so regression problems where you have, you're predicting, uh, some sort of continuous value and, uh, for cross entropy. Uh, so there is, like, you know, there is a binary cross entropy, uh, which is used for, um, classification problems, like, especially binary classification. And the, there is also categorical cross entropy, um, which is, uh, an extension of binary cross entropy, uh, where you use like, you know, uh, for multi-class classification problems, um, uh, bonus. Uh, so let's say your, instead of predicting, so you looked at, we looked at, you know, what kind of losses we need for regression, we, uh, what kind of losses we need for classification. Uh, let's say instead of, you know, these kind of problems, you're trying to predict a distribution, like, you know, like, uh, you're taking a probability distribution as input and trying to predict another probability distribution. Uh, do you know what kind of loss you would use in that particular scenario? I, So there is a loss called KL divergence, um, which kind of, uh, looks at, um, you know, takes two probability distributions and looks at the difference between the, uh, uh, two losses. Um, so that's, that's the kind of loss that you would use in those kind of scenarios. Um, so let's quickly go back to the slides, see what else? Um, um, there are several other losses like, you know, even like, uh, the, the existing MSE losses, binary losses are kind of, um, modified for a given use case. Uh, like when you do, um, um, recommendations, for example, um, we use, uh, uh, a loss called, uh, triplet loss. Um, so, uh, um, uh, and when there is lot of imbalance in the data sets, for example, like, you know, when you want to give more weightage to certain classes, there is a special kind of loss called focal loss. Um, so, so there are, again, uh, the choice of loss function depends on, um, what particular problem you're solving, what are the characteristics of that problem. But these are the base ones that you would use on a day-to-day basis, like, you know, BC or MSC. Uh, you never go wrong with, you know, choosing one or the other for, for most of the problems involving regressions or classifications. But, uh, you might want to look into more advanced losses, um, uh, which is a bit out of scope of this particular class, but, uh, um, something you, you can read on yourself, like there is a paper on loss functions itself. So, so, um, so, so yeah, just want to give you some, some knowledge there. Um, all right. So let's see. Um, what do also quickly recap, uh, gradient descent, um, uh, it's essentially an algorithm, uh, used to train the models. Um, and, uh, the way this algorithm works is, um, it ly updates, um, the network weights, um, and, um, biases. Um, so it starts by initializing weights and biases for, uh, a network with random values. Uh, these are initial values. Uh, this was a starting point for the optimization process, so maybe I can right here. So initialization and then, uh, the forward propagation is, is done through the network, so forward propagation, and based on this, you'll have some predicted value, right? And so now that you have the predicted value and true value, you calculate the loss. And then, um, once the loss is calculated, uh, the back propagation is performed. Um, so that's essentially the, the four step process of, um, uh, gradient descent. So, uh, initialize the network with some random rates, uh, do a forward propagation through the network and calculate the outputs or the predicted values, uh, and then calculate, calculate the loss between the predicted and the true values based on the loss function chosen. And then once you have the laws, you back, back propagate the laws. Um, so maybe you already know this. So what is the, um, uh, calculus rule that's, uh, that's used for back propagation? Yeah. Chain rule. Yeah. Yeah. Uh, cool. Um, let's, let's look at some of the, or maybe we can recap, um, some of the math here if possible. So, let's see. Um, uh, let me see if I can whiteboard this. Um, so essentially, so you have your loss, as you know. Um, let's say it's, um, MSC loss of true and predicted. So it's essentially a squared error. So, so you have your, um, um, sorry, gimme one second. So the chain rule states that, um, you can, uh, take partial ative of, you know, the, the output, um, with respect to input, and then you can take the, uh, partial ative of input with respect to weights. Um, and, um, so you have your loss with respect to output. Um, so, so that's your, so, so that's how, uh, a loss, uh, is used to calculate the, the weights. And this loss is essentially propagated during the, um, the update process. So you have your weight o uh, weight, new, equal to weight, old, minus some sort of learning rate times, whatever this gradient is, uh, which is what your input here. Um, so, so that's, uh, essentially how, you know, in every network step, you apply the chain rule, calculate the gradient of the loss with respective weights, and then use that to update the weights, uh, for each of the steps. Um, um, so that's a high level how, um, you know, a gradient isn't works, um, from a mathematical perspective. Um, again, some of the things we already talked about. Um, so, uh, this is, uh, the diagram here is, is a 2D representation of, you know, how the loss changes with respect to one of the weights, um, and how it reaches a minimum. Um, so as you can see the steps, you do the initialization of weights and biases, forward prop, calculate loss back, back propagation, uh, update the weights, um, and then kind of repeat the steps two through five, um, until there is some condition that's being met, that the loss is not reducing by much, or, you know, uh, or, you know, there is a divergence between training and validation losses, things like that. So the different stopping criteria that you can use to stop the learning when you think that it reached a, uh, minimum. Um, all right. So let's talk quickly talk about activation functions. Um, um, why do we need activation functions? Uh, any takers? Uh, just curious. So we are trying to model complex, uh, problems, right? Like, for example, you know, object detection, phase recognition, or even learning from, from text, um, is, is, is a complex non-linear problem. It's not very linear, like, you know, house price prediction. So, so to add that non-linear to the network, uh, and a lot of good answers here. So, uh, we use functions and transformations called activation functions. Um, there are several, um, like, you know, some of the things that you're seeing on the page here, like, you know, sigmoid, um, tan edge, relu, um, Galu or leaky loo, for example, uh, soft max, which there the bunch of, uh, activation functions. There is a whole research area dedicated to activation functions itself, and we choose the ones that are useful for a given scenario, like, um, uh, we'll get into some intuition, how to choose them, uh, in later stages. But, uh, at a high level, uh, it's important to know what these activation functions are doing. Like, for example, if you take sigmoid here, like what would be the output range for this activation function? So it's always between zero and one. Um, and for tanh, um, uh, likewise it would be centered around zero. So it would be between negative one and plus one. So if you're trying to do a zero centric activation, uh, then probably tanh is, is is the reasonable choice here. Uh, if you're trying to use classify something as binary or not, typically sigmoid activations are used in, um, you know, uh, in, uh, classification problems, binary classification problems. Um, um, so if you're trying to do like a on off kind of scenario, um, uh, true false kind of scenario, that's where sigmoid activation is come into play. Um, uh, relu, uh, is an interesting one. So, so relu is max of zero x. Uh, so it's always positive. So it kind of caps, it caps, uh, a function like that. Um, so it doesn't go negative. Um, so, um, so it's used to, uh, introduce non-linear to the network without much complexity that is, uh, like, you know, like sigmoid and tan hedge functions are complex, uh, to calculate. So, but relu is super fast. So, so relu is introduced to, to mitigate some of the complex piece, um, but also introduce non-linearity, and galu is, is another, uh, modification of, uh, relu function. Uh, for example, um, I think, uh, galu is, you know, something like instead of Klu making it, you know, um, a a zero, uh, for negative values, um, uh, so Galu uses something like X into five x, uh, where five of X can be some function, like, you know, like, you know, some, um, sigmoid, uh, or, you know, tanh or whatever. So it kind of weights the values, um, uh, instead of gating them. So, so, so relu kind of gates them like anything below, uh, zero is gated. Uh, galu is kind of weights them instead of getting them by, by a factor of another function. Um, uh, with respect to that, uh, input value, um, um, there is also versions like, you know, uh, leak that was introduced to, uh, to, uh, take care of the negative effects of, um, alu functions. Uh, for example, um, softmax, you, you guys have, might have probably, uh, uh, known, known about that. Um, it's, it's, it's a extension of sigmoid function, essentially. So when you have multiple classes, uh, and there are logics, uh, being produced, so softmax, essentially what it does is it creates probability one, probability two, probability three, and the sum of all of these will be always equal to a hundred percent or one. Um, so softmax layer essentially takes a bunch of numbers and puts them into, uh, a probability, uh, value distribution, where the sum of all those probabilities is always equal to one. Um, so that's a, that's a quick recap of different activation functions, um, um, that, uh, that are very popular. Cool. Alright, let's quickly summarize the section. So, um, we looked at, you know, what are the salient characteristics of deep learning problems? What are different, uh, uh, ways that, you know, deep learning problems can be identified, uh, what kind of areas they excel at, kind of unstructured data representation, learning end-to-end learning, lots of data, high dimensionality, uh, things like that. Uh, we looked at some real life examples where, you know, deep learning is being used properly. Um, some advantages, uh, and challenges with deep learning. Um, we also recapped on the loss functions, GD algorithm and activation functions. So, uh, let's a quick recap of the section we just went through, uh, in the next session, our learning object is to go through, uh, feedforward neural networks. Um, so let's take a quick five minute break, uh, before going to the next session. Uh, so let's stand 10 now. Uh, uh, uh, so let's, uh, start at 10 15 and, uh, we'll look at we power Neil Networks. Alright, hopefully everyone is back. Uh, let's get started. Um, cool, uh, we'll dive into f in this particular section of the class. So a quick section agenda. Yeah, so, uh, we'll talk about, you know, uh, quickly introduce fns, uh, what is the underlying principle there, uh, how different layers in f and n works, um, and, you know, some limitations, advantages, um, applications of F and n. Uh, and then we'll, we'll wrap it up with a coding walkthrough of an f and n on a dataset. Uh, so that's a brief agenda for this section. Um, so as, as we talked earlier, f and n stands for feedforward Neural Network. In some places it's called FFNN, uh, or FNN, they, they all mean the same. Um, it is, uh, sometimes also referred to as MLP, uh, multilayer, perceptron. Um, it's a bit older technology, but, uh, it's, it's also referred as such. Um, So, um, there might be some confusion around like the, the naming convention, like, you know, um, if, you know, why is it called feet forward, uh, versus, you know, um, uh, since the, uh, the loss propagates backward, there is a back propagation there. So, so why the naming, uh, technology? So, um, the reason for the naming there is, you know, the information, the actual information always flows in one direction, uh, in this way. Um, and, uh, so that's, and there are no like recurrences, like, you know, there is no like information being retained and being passed back to a previous layer or to the same layer, uh, which we'll see in. Um, so, so the information is always passing through from, from, from the left to the right, uh, across the layer. So that's, that's why it's called feedforward, uh, neural network. Um, uh, and there are no loops or cycles of, uh, where the information is being passed back. Um, so, uh, that's the reason why they're called feedforward neural Networks. Um, uh, the back propagation there, uh, the confusion could also come like, Hey, the, the, the loss is being propagated back. So we are not talking about how the loss is being propagated. We are only talking about how the information moves. Um, so, so bap propagation essentially moves backward, uh, in any network. Um, so, so that's a separate concept compared to, you know, how the data is flowing through. Um, uh, and, uh, we discussed like, you know, during training, um, f and n learns by a process called, um, you know, grad indecent as the optimization algorithm. Um, and by adjusting the weights and biases, um, through the process, um, uh, using the loss, that's how the, all these mini weights and biases are learned, uh, during the, uh, uh, learning process, training process, uh, using the, uh, back appropriation technique and gradient central algorithm. Now, the end goal is to, uh, minimize the loss, whatever loss, uh, which, uh, which you choose, like, you know, whether it's, uh, cross entropy or MSE or some, some of the laws that are being used for a given problem. Um, so, uh, to recap, uh, in feed four neural networks, there are several layers. Um, there is an input layer, um, multiple hidden layers and an output layer. Um, and essentially the information flows from, you know, from the input layer to the output layer in one direction. Uh, hence the name. Um, they are optimized using gradient descent algorithms, uh, using back propagation techniques. Um, various losses can be used, uh, to determine how the predicted values are, uh, different from the true values and, um, the network cloud using the, the gradient dis design process. Um, so that's a quick overview of, you know, how f and n works, uh, from, you know, um, from a training perspective. Um, again, I think we looked at, you know, uh, this earlier. So just want to quickly, uh, recap how the loss is calculated with respect to each weight. So you have your, um, rule here. So essentially, uh, the loss is the differentiated with respect to each of the weights. So this could be your W one, uh, W2, W three, so on and so forth. Um, so for with respect to each weight, we're, we're doing a partial der and then we're essentially coming to that, uh, value by using these derivatives, um, uh, differentiating the laws with respect, output, output with respect to input, and then input with respect to weights, um, and then using those values in the, um, uh, weight ation function. Um, so, uh, that's essentially what's happening during the grad indecent process, right? Um, again, um, I think this is an animation slide. Let's see. Uh, yeah, so this is, uh, an animation of how the different activations are used, um, uh, and, uh, how data passes from input to output layers. So, um, um, essentially as you can see, it's always forward propagating. Um, there are multiple, uh, neurons in each layer, uh, with, uh, which are activating at different points of time, and they're calculating the transformed output, uh, through that activation. Um, and, um, essentially that becomes an input to the next layer, uh, and so on and so forth. So, um, one thing to understand here is you can see all these dense connections here, like, you know, um, by that I mean, what I mean is every node in one layer, uh, is connected to every other node in the next layer. So, uh, let, let me go back to this one. So you can, you can see, like, you know, um, all these, uh, all permutations of, of, or combinations of these connections happening between every input node and the output node. Uh, so that type of layer is called, you know, densely connected layer. Uh, and, uh, that's a characteristic of, uh, feedforward neural networks, uh, is, you know, every node in, in a layer is connected to every other node in the next layer. So essentially, uh, if, if you look at, to take this particular layer, let we use a different color, um, so let's call it node layer two, node one. So for layer two, node one, it's inputs from all these four nodes. Um, so, so layer one, node one plus layer, um, one node, two plus layer one, no, three plus layer one node four. And essentially it's using a, um, activation function on all of this. Um, like, you know, in this case, maybe if the activation function is relu, so it's using relu on, on top of all of this, and then outputting some sort of value, which is your layer two, node one, and that becomes, again, um, an input to the next layer. So each layer is a composition of nodes, and, uh, each node or activation calculates, uh, the activation by summing up the, uh, activations from its input notes and applies a transformation around it, which is the activation function. And, uh, that becomes an input to the next layer. Cool. So, um, typically the input layer that represents the dimension of the input vector. Uh, so if your input vector is, let's say, um, a 10 dimensional vector, um, so that you know your input layer, the number of nodes in the input layer is the dimension of the, uh, input, uh, vector and output layer represents the dimension of what we're trying to predict. So if it's a, uh, classification problem, like binary, yes or no, it could be just one node. Uh, if it's a multi-class classification, uh, use the soft max with, uh, number of nodes equal to number of classes. Um, um, so, uh, input layer of input. Uh, so, uh, let's take an example. For example, let's say you are using a, uh, image as an input, uh, and image is essentially pixels, right? So let's say you are using a image of 10 by 10 pixels, uh, right? So you have a hundred pixel values, so you can flatten this and supply to the input, as, you know, a hundred dimension vector. Um, so, so that's, that's essentially how we think of input. So especially when you're, in this case, we're supplying an image of 10 by 10 pixels. Um, so, so that's the input size a hundred dimensions represent, uh, uh, each dimension represents the pixel values of each of the, um, uh, pixel. And then, uh, in the middle layers, you lose some processing. The choice of how many nodes in the middle layers depends on the complexity of the problem. Usually there is a sort of, you know, uh, upsampling and downsampling happening, uh, in the hidden layers. And then, uh, the output layer, let's say you take the image and you want to classify it as dog or cat, it's a binary problem. Then you only have one node in the output layer, but let's say you want to classify it as dog, cat, or you know, a rat. Uh, so you have, you know, three classes. So in that particular case, uh, you'll have like, you know, three notes in the output layer, uh, which represent the probability of, you know, a dog, probability of cat, probability of cat. Um, um, so these are the middle layers, and then you have your hundred dimension input layer. So think of, think of, you know, setting up the network architecture that way. Any questions, uh, uh, before we move on? Cool. So let's look at this example where we have just one hidden layer. Um, uh, in this case, you have an input layer, uh, with the number of, uh, input neurons SSL to three, uh, and we have hidden layer with number of hidden neurons, SQL to four, and an output layer with just, uh, one year on. Um, so, so that's the, that's the network structure for this particular f and n model. Uh, do you know how many, uh, uh, parameters, uh, uh, need to be computed, uh, uh, for, for this particular neur network? Any takes given the network structure, like three inputs, four hidden, and one output, how many different weights and biases? Uh, let's ignore the biases, even if it's the weights. How many weights needs to be computed? Yeah, absolutely. So, so the way 16 is the right answer here. So the way, uh, that is computed is you have three times four weights. So you have your all, uh, 12 weights through these, um, connections, uh, how much they're weighted at, and then you have another four weights here. So, um, that is four times one. Um, so, so that's how you come to 16 weights without including the biases, uh, in this particular case. Cool. Um, let me know if you have any questions, but, um, essentially that's, that's how you ca uh, you calculate how many parameters are needed for a given, uh, neur network structure. Um, again, some examples of, um, neural networks where there are multiple, uh, layers as well. Uh, so in this case, in the top case, the multilayer f and n has, uh, more hidden layers, uh, more than one, uh, in addition to the input and output. Uh, so this is your, uh, input, and this is our output. And there are hidden layers here. Uh, these, these have more than two here in this particular case. Um, and the number of hidden layers and number of neurons in each layer can vary. Uh, it depends on the problem and the design complexity of the model. So as we increase the layers and the number of neurons in each layer, that increases the complexity. Um, and we discussed about fully connected layers in fully connected, uh, layers, uh, also known as, uh, dense layers or dense network. Each neuron in a layer is connected to, uh, every neuron in the adjacent layers. Um, so, uh, we talked about this like, you know, uh, like every neuron is connected to every other neuron in the next layer. So in this particular example, uh, can you tell me, like, now we include the biases as well. Can you tell me how many, uh, uh, parameters we need, uh, between, uh, let's start with, uh, how many we need between how many parameters we need between input and H one. H one is hidden layer one. So, uh, so we have three nodes in input layer and four nodes and, uh, hidden layer one. So we need 12 weights. Uh, and then we need, uh, four biases, right? So, so we need for between input and hidden between these two. We need 16 rams total. Now, with the same logic, uh, how many parameters do we need between hidden layer one and hidden layer two? Yeah, it's the same 60. Um, so I think, uh, it's flipped the other way around here. Four, uh, hidden, hidden layer one. So actually not 16, actually four into three. There's only three biases, right? So it's actually 15. Um, so, um, do you know how, how that's calculated? So we only need three biases, right? But these three nodes, so it's, uh, actually 15. And then finally with the output layer, uh, between H two and output, uh, how many do we need? May? Good, good question. Um, so share, uh, let me answer that. Um, so far, um, the output layer, uh, yeah, it's three times two plus two. So we have eight, uh, parameters. Um, so total, uh, we have 16 plus 15 plus eight, right? So 39 parameters in, in total. Um, so even with a small network, we have like, you know, uh, like this, uh, 39 parameters, uh, if you look at like, you know, some of the more, uh, popular models like, you know, GPT, uh, class models, uh, the number of parameters is in like, uh, several millions to billions. Um, so, so that's the scale we are looking at. Um, there is an interesting paper, uh, called Chinchilla paper, uh, where, you know, there is a kind of analysis between the input size and the number of parameters required to learn efficiently from the input. Uh, uh, that's, uh, an active area of research, like, you know, how many parameters is too much, how many parameters is too little to learn something. Uh, so that's, uh, that's also, uh, an active area of research on how, how we decide how many parameters are needed, uh, for a given network. Um, cool. So biases is essentially, uh, I think it came up in one of the questions. So, so the way this calculation happens is, so let's take, um, um, this node as example, right? So, so it has inputs coming from all of these. So, so I one times w one plus i, two times W2 plus i three times W three, uh, and then you add a bio term, right? Um, and then you do the activation, whatever that is really, uh, oops. Um, so I don't know what I did there, but essentially you need a bias, uh, to, to be able to, uh, add to, to that activation so that, you know, uh, you can control, uh, not just the weight, but also the, uh, areas where, you know, you need to scale the, uh, the input, uh, to be able to learn better. Uh, I think, uh, just a follow up from previous slide. So, alright, so let's, uh, uh, quickly go through how many, is there any understanding how many, how we decide how many hidden layers? Uh, good question. Um, I'll give you some pointers. So depends on the one is, it depends on the complexity of the problem. So I always start with, um, two ways. Uh, start with, um, you know, if you, if you know the problem you're trying to solve, let's say a classification from images, and you probably read some literature, right? So like, you know, how these, uh, um, object detection kind of problem or classification problems are solved and you can inspire from those architectures. So that's one way, uh, and you can try to, you know, mimic some of those architectures. Um, the other way is to, you know, uh, uh, the problem is like an easy, uh, learning, like, you know, cats and dogs, very easy to learn. Um, and the complexity is pretty low there. Uh, maybe, uh, you know, you, you're already seeing like, you know, even with baseline models, you're seeing high pressure. So maybe you don't need a deep, uh, complex network. So maybe you can start with, you know, a few layers, um, uh, to learn the latent features, uh, with, uh, you know, with, uh, some sort of scaling from, let's say it's a pixel of a hundred by hundred. So you have, uh, you know, uh, 10,000 input features. Um, so you are looking at some sort of scale down to maybe, uh, you know, 5,000, uh, features in the hidden layer one and another 5,000 hidden layer two. And then you, uh, you, you're trying to create a classification of, you know, output neuron. So, so based on that, yeah, you'll get some intuition into, you know, how many neurons in each layer, uh, and so on, on forth. So, so there are multiple ways. There is also, uh, a active research like, you know, uh, I think neural architecture set. Uh, so, so there is, it's kind of trying to find what is the right neural architecture for a given problem. So it's kind of learning from the data, what is the right neural architecture. Um, so that's, that's another, uh, uh, way, uh, more sophisticated way to find, uh, how to decide what the neur architecture should be. Uh, hopefully that answers, uh, uh, I think someone asked number, I think. Cool. Uh, let me know if you are in the chat. We know, like, you know, if your questions, uh, haven't been answered, I'm happy to follow up on those. Alright, so let's go through, uh, new, actually. So yeah, let's go through an f, f and n example. Uh, so for this, uh, we'll use a digit recogni dataset. Um, let's enter, introduce that dataset, uh, quickly. Um, so, so these are handwritten digits, uh, that are, uh, that have 28 by 28 pixels. Um, so, so total 784 pixels in total. Um, and each pixel has, uh, so these are gray scale. So each pixel has a value between zero and 2, 5, 2, 5 being the darkest and zero being white. Um, so, uh, so essentially trained dataset has these sound 85 columns, uh, 7 84 being the pixel values. And then there is a label, uh, uh, which is the, uh, the digit, uh, like if it's a seven, then it's, uh, labeled as seven. And, um, and essentially, uh, that's, let's look at this dataset, see, see how it looks. Uh, it'll be more intuitive when we look at it. Uh, so for this, uh, I'm using touch, uh, uh, framework. Uh, it's a very popular framework for developing, uh, neur networks, uh, neur networks. Um, likewise there is, uh, tens of flow side of things like and terms of flow, which we'll also look at. Um, but torch is, are very popular, uh, because of its ease of use. So just doing some basic imports. Um, um, I'm importing the torch, uh, and then package, uh, which is the Neur Network package. Uh, and using s scale, learn for doing basic functions like train split, uh, and as you might be already aware, uh, for basic, uh, handling of errors and data frames, uh, and using map plot for plotting. Let mount my cool drive, hopefully. Oops, right. So I think I have it mounted. Let me see where I need to change the path. Alright, cool. So, cloud class. Okay. Looks like it. Looking good. So yeah, so just reading in the CSV files, uh, for train and test and, um, looking at, you know, basic sanity checks for the shape of the dataset. So as you can see, the train dataset has one extra column, uh, for the label. And, uh, uh, essentially the 7 84 pixel values. Uh, and then there are 42,000, uh, observations in train and test has 28,000. Um, so, so that's the, uh, a quick look at the head of the dataset. Uh, so you can see like, you know, a bunch of these pixel values, um, starting from column two, but there is also a label. Um, so that's how the data is set up. So let's do some basic, uh, understanding for, you know, label counts. Um, so as you can see, they're fairly equally distributed, uh, in the trained data, uh, around 4,000 per each, uh, label. So you have 10 digits, uh, zero to nine. Uh, and, um, yeah, they are fairly equally distributed. So, uh, there are no, uh, edge cases or, you know, sparse labels for certain categories. Uh, doing some basic conversion here, um, like, you know, um, um, converting to MPA array or, you know, to be able to, uh, use it for our, uh, models. Um, and then setting the label, um, properly. Um, let's visualize some of these images, how they look like, uh, like, um, essentially I'm plotting the, uh, each pixel value of the image here by reshaping it back to 28 by 28, uh, instead of the flat and 7 84. Um, so, so, yeah, so these are, this is how the handwritten digits look like. Um, uh, it's showing in color here, but these are all gray scale. Um, so, uh, as you can see, like some of the digits, like, you know, 1 0 4, um, the, and the labels associated with them. Uh, here, um, I want to quickly check, is the zoom okay, on the, on the coding notebook? Uh, sometimes the code can show up as pretty small. Uh, just want to check, uh, is the zooming, okay. Okay. Um, so again, uh, now, now that we have the train side, let's split that. Uh, so that's the train file. Uh, we'll split that into, uh, you know, training and validation. Um, just, uh, what's a good rule of thumb? Like, you know, um, for splitting between training and validation, what's a good percentage? Yeah, Uh, I think fairly good answers here. Like, you know, um, 60, 20, 20, like, I think that's train te train validation test. Um, 70 to 80% for training and remaining 20% for, um, validation. I think, uh, that's, those are fair. Um, any, any takers like, you know, um, why we need a test set? Uh, just curious, uh, if we have validation, uh, why, why do we need a test set? Yeah. Um, fair enough. But, um, inval, they're essentially not training on the validation data. So curious, uh, why, why, why do we need an another test set? Yeah, so, um, essentially, uh, during the train validation testing phases, uh, uh, right before running, yeah, so, so you're still using validation data to a certain fashion, right? To inform your decisions, what to stop it, um, how to, where to stop in terms of, you know, the last convergence. Uh, or, uh, sometimes even with cross validation, you are seeing some parts of validation data. So there is some leakage, uh, not even if it's not through direct data, but there is some indirect leakage between validation and training. The intuition behind having a separate asset is to completely separate, uh, uh, the dataset from all of that. Uh, typically, uh, in real world, like, you know, um, uh, use, uh, uh, let's say if you're using some sort of, you know, stock market prediction, so typically you use like validation data from like, it's, uh, future time. Like if train and test is like 2023, train validation is 2023. You use, uh, test dataset from 2024, uh, to see how, you know, that's how it works, right? You in real world, like you build a model today and you apply it to future data. So, uh, so that's how you kind of separate, uh, completely, uh, test dataset. Um, and, uh, validation and training are typically from similar timeframes, but, uh, test is like, you know, is mostly a future timeframe. Well separated temporarily Now that we've seen the dataset, how it looks like, um, um, let's also split, uh, this dataset. And here we're using a split size of, you know, 80 20. Um, so, so we'll split the initial 40 2K across these, um, 33 k and eight 8,400. Uh, also check, you know, quick sanity check, uh, after that should be like, you know, are both these datasets equally distributed? Um, so, uh, again, looking at the same bar charts across these, uh, so looks fairly similar to me. Um, so no issues there. Um, and, and then, uh, we'll use, uh, the AYA to tensor, uh, to, uh, before, uh, uh, training the model. So tensor is essentially think of like, you know, like, uh, a multidimensional, uh, matrix. Uh, so, you know, scaler, vector matrix and tensor is like a multidimensional, uh, matrix, essentially, uh, nothing more than that. So, and we're, we're using touch function to the flow tensor for that particular use case. Uh, for this particular, uh, since, you know, we have, uh, values between 0 2 5, uh, even though I think we are good with into values, we're using flow tens here, and I'll tell you why we are using actually, uh, and there is, I think, uh, we're doing some normalization later on. Um, so next thing is to, uh, define the model. Uh, so for this, um, we'll use the, uh, touch and in model, uh, module. Uh, from that, uh, we'll use the, the different layers. In this case, we are using the linear layers, which are, uh, densely connected. So here we are using, uh, four layers, uh, and they're named as FC one, FC two, FC three, FC four, uh, and as you can see, FC one is the, uh, the number of input features is same as the number of pixels, 10 84. And the output features here we are scaling down, um, uh, if you remember me telling, like, you know, how to decide how many, um, nodes in the head and layer, so kind of, you know, kind of take the input and kind of scale down gradually to reach the final set here. So here we are scaling down to 600, uh, in the first layer to 500 in the second layer to two 50 in the third layer, and then finally to 10, uh, and 10, because we, we have 10 classes. Uh, so that's how we go from 7 84 pixels to finally 10, uh, classes. Uh, and then, uh, um, we are using, uh, uh, as you can see from here, uh, what kind of activation function are we using? Uh, we are using a relu activation function. Uh, can someone tell me what is the output, uh, range for relu activation function? Yeah, zero two plus two infinity. Yeah. Uh, Vincent, um, and, um, let's see. Yeah, cool. So essentially, uh, yeah, we have four layers, uh, downsampling, uh, gradually across these layers. And, uh, so if you ask me why four layers versus maybe can we do this with three, we can experiment, but, uh, uh, I think this is, uh, um, this network structure is, uh, I think, uh, several instructors before we played around this network structure and came up with this architecture. Uh, but, um, you guys should play around like, uh, see what happens. Like, you know, what the accuracy levels are. Uh, you use like, you know, uh, just two layers, uh, versus three layers. Uh, it's always, uh, you know, find the best smallest network possible to solve your problem. Uh, the more simpler, the better, obviously. Um, so, um, so something you can experiment a little bit. Um, so, uh, another quick thing I want to point out. As you can see, even with this small network, like your number of parameters are kind of explosive. Like, you know, if you're looking at, like in previous calculations, um, so you're looking at 7 84 times 600 plus 600. That's, that's a big number, uh, like even with, you know, with this architecture. Um, so, so, so that's, uh, that's the impact of, you know, the number of layers and, uh, number of neurons in the layer on the number of parameters that the network need to learn. Alright, just defining some accuracy function here, uh, so that, you know, we can evaluate this model once we get it going. Um, how to define that, um, accuracy function, what correct means. Uh, it's just comparing the predicted to true label and, uh, and then, uh, using an accuracy, uh, function. Alright, so we define the, the network architecture earlier here. Uh, we just define the class, and then now we'll define how to, uh, do the training. Uh, we'll define a training, um, method. So for that, uh, we need, uh, a data loader. The number of, we need to train for, uh, the model we are using, the, the model definition from, from, and, um, what's the stopping criteria and optimizer. So these are all the things we'll, uh, define here. Um, so, um, let me run this. Um, so, and again, we are also printing a bunch of stuff at each epoch, like, you know, uh, what is the, uh, training loss? What is the training accuracy? What is the we are at, uh, so that we can monitor how the loss gets, um, um, uh, you know, optimized over the iterations. Um, uh, and we'll talk a little bit about what data loader is. Um, uh, this is a very specific method. Uh, we are using to load data as batches in this particular use case. Um, this is an, uh, an interesting one. So we are using a torch manual seed here. Um, so every time you take a data set and run the models, um, and essentially the models, uh, the way arrays and sensors work is your data is loaded in a random manner, uh, not sequential all the time. So the results slightly vary, uh, across different runs. So if you see a run with the same exact parameters, which here run with, you know, 0.0, uh, uh, sorry, 90, 90% accuracy, the next run could be slightly different with 90.1% accuracy. So, but, um, if you want to remove that variability, it's good to set a seed. Uh, and seed is how, you know, it always goes back to the right, uh, observation. And then, uh, and then it, uh, kind of think of it as, you know, I want to replicate and reproduce the same result by supplying the data set in the same way. Uh, so that's, so that's when you set the seed. Uh, and then in this case, we are using a batch size of 1 28. So we are not supplying all the 34,000 training data at once. We are using mini batches here of each of 1 28. Um, and yeah, so for the loader, as I said earlier, we are using the, uh, toch, uh, utility function data loader, uh, with the batch size of 1 28. So essentially it takes the 1 28 batch from the training dataset, shuffles, and then takes under batch shuffles and goes, so on and so forth. Uh, and once it goes through all the training data, that's one epoch. Um, and, uh, we are essentially doing, running this for tab 10 box. You can run it for more, uh, uh, but, uh, that's essentially what we're doing. And, um, um, yeah, so this is a multi-class problem, right? Uh, so what kind of loss function would you use, uh, in this particular case? So, softmax is a layer, uh, you would use to, um, is a kind of activation essentially you would use to, um, create, um, the probabilities. But in terms of loss that you would use, like how would you compare, uh, the true to predicted in a multi-class problem? Cool. Yeah. Uh, that's, uh, uh, that's correct. Uh, cross entropy. Uh, so, uh, we are setting that here, uh, for the criteria, cross entropy loss. Uh, and then we are using Adam Optimizer, uh, which, which is a very famous optimizer. Um, uh, it's essentially a combination of, um, AdaGrad and RMS prep, uh, kind of optimizers. Um, you can, you can do some reading on, on them, um, uh, and what they do especially. Um, and then, let's see. Um, we're essentially computing the training IES losses for each epoch and so on and so forth. So let's, uh, start running this, uh, actually, okay, so looks like there is some here. I probably didn't run this. Alright. Um, so let's see what the output shows. Like, you know, for each output, for each, we are emitting the training loss and the training accuracy. Uh, huge drop in epoch two, uh, from 91% to 97%, and a huge drop in loss as well. Uh, so, so training wise, you know, it's going in the right direction. In the next, uh, uh, labs, we'll look at like, you know, what is a good criteria to stop and stuff like that, uh, uh, learning rate, any ation how to change the learning rate if the training is not occurring properly. Uh, but, uh, for this one, we'll, we'll just do the basic, uh, we'll just stop based on our criteria. We'll just run it till epoch 10 and stop it. Okay, cool. So, looks like it's, it's a, it's not a very complex, uh, problem. Uh, so we, within, even within 10 ocs and, you know, training data set of 33 k and a small network architecture, we are able to achieve like 99.3 accuracy here. Um, so let's look at how, you know, uh, how to, uh, this visualizes and across iterations, uh, the same thing we looked at here, but, uh, you know, uh, just visualizing, um, and the looks like, you know, from a training standpoint, it looks like everything is good. Uh, what is the logical next step? I mean, uh, now that you've trained the model, uh, how, how do you, what do you do next? Yeah, before testing, uh, maybe, uh, also, yeah, I think maybe by test you are also saying validation. So let's see how this does on validation. Um, so we'll look at, you know, validation dataset and how the predictions look on the validation dataset. Um, so first thing is we are doing a confusion matrix. Uh, so essentially you have your predicted and true values, uh, as rows and columns. And essentially you want to see most values being populated across the diagonal and less values on, on the non diagonal numbers. Uh, but you can see like some numbers where, you know, there is like, you know, 30 here, 19 here, uh, bunch of stuff like that, uh, where these are all misclassification. So, uh, those are some things that, uh, we need to, uh, take care of. Uh, we, these are good to investigate for that, like, you know, where is the model mis mis predicting misclassification, and what can we do about that and stuff like that. Um, so, um, yeah, so in the, in the way this network is set up, like if you go back, so here, uh, the final layer is also linear, uh, and, uh, uh, so you're not essentially doing a soft max here. Uh, so the output is, is, is in the form of a, um, a logic. Uh, so what is the, uh, range of that output? It's, it's, it's not in a property fashion, because it's not, we didn't apply any soft max function. So let's look at it. I mean, uh, we can visualize. So, so this is how the, the output is like, you know, now if you put that into a soft max, uh, then you can get predictions. Um, but essentially we use them directly here. We just took the max of it and then, uh, use that as our class prediction. So here, we just took the max of this. I think in this case, it's, uh, you know, the, for this observation, the prediction is nine, uh, or eight, uh, uh, actually. Um, so, so essentially, yeah, you can directly use logics and can, but, you know, doing a soft max on that would have been nicer, um, because that way you can see these as probabilities, uh, rather than, um, you know, just logic, uh, values that don't make any sense. So here are some predictions, uh, like, you know, where, uh, we predicted, uh, uh, like, uh, the pre prediction versus what's actually there. So most of these, uh, are, are correct. Uh, you see some instance, like in this particular case, we predicted eight, uh, but this looks like, I don't know, a seven or two. Um, so, and here it's, uh, one, and prediction says four. Um, so we can also look at like, you know, where the prediction and the, uh, label are not matching more closely. Um, so yeah, you can see some instances where it's misclassified, uh, like, you know, this looks like, you know, uh, some noise there, like, you know, it's not well written, uh, stuff like that. So, again, ways to, you know, you can go deeper and see what's going on with the dataset, what, so why are mis predictions happening? Um, so that's essentially what we're doing. And then, um, um, just, uh, saving the, the model and, um, um, um, predicting on the test dataset, uh, essentially, um, so for test dataset, we don't have, uh, labeled, so we'll just rely on visual inspection here. Um, so again, quick visually, like, you know, um, how, how we did with predictions on that test dataset. And, uh, yeah. Um, so that's the end of the, the coding workbook. Um, so we essentially looked at, you know, how to set up the using torch and, and framework, uh, how to set up the layers. Uh, and then, uh, once we set up the layers, how do we set up the forward paths, uh, uh, and what the right kind of activations are. Uh, and then, uh, we looked at, you know, how to set up a training function, uh, with, you know, uh, batch sizes, data loader, and, uh, stop, uh, criteria to stop, which is the number box here, uh, looked at, uh, how to set up the optimizer and, uh, you know, how to emit accuracy and loss across each of the training steps. Um, so, so that's, um, you know, uh, I'll pause there, uh, uh, if you have any questions, happy to answer. Cool. So, um, the next part of the class is, I know, um, quick check here. I saw this class to, uh, we can, uh, uh, Ram, uh, it's, uh, essentially, uh, uh, just wanted to show that we can directly also use Logics. Um, so, uh, we'll use softmax in later labs as well. Um, so, um, since we are not using the probabilities in any fashion directly, uh, and we can just use the Logic Max, all those logics for, for predicting what the digit is, but softmax, using softmax is the cleaner way, uh, right way to do it. Uh, yeah. It wasn't used to just make a point here. Uh, there's no particular reason. Uh, cool. So, um, let's see. Uh, do you guys have this class for two hours or four hours today? Just want to check. Okay. Okay. I know we're, we're a bit over time. Uh, I want to quick take a quick poll from the class. Uh, do you guys want to go through, uh, like, you know, few slides on cnn, or do you want to, you know, just pick it up tomorrow? Either is fine. I just want to take a quick poll. Uh, okay. Even if one says tomorrow, we'll start tomorrow, uh, because, you know, I wanna make sure everyone is comfortable with that schedule. So, so tomorrow, uh, let's quickly review the agenda for tomorrow, and we'll wrap this class up today. Uh, and, uh, yeah, and we'll meet back tomorrow to discuss. So, uh, let's quickly dis, uh, discuss like, you know, advantages of F and n. Um, so, um, again, lot of deep learning advantages, uh, are applicable here. Like, you know, end-to-end learning, um, um, transfer learning, uh, scalability and parallelization, um, um, uh, universal approximators, uh, and a lot of nonlinear modeling, like, you know, using multimodal data and, uh, and as such, um, and, um, some of the challenges, uh, again, these are, uh, uh, addressed with the pictures that we'll look at in the, uh, next sections is, you know, there is no sequentiality in the, in the small, right? So, um, um, for example, you know, by inherent nature, they like the ability to model sequential. There is no temporal component, like, uh, the data is flowing in one direction, but there are no loops as such. Um, so, so these are addressed through other types of network architectures like, uh, recurrent, uh, neur networks, uh, and there variance like l SDMs and G. So for, for tasks involving sequential data, uh, and are less, uh, fns are less, uh, applicable, uh, and these are not, uh, um, very good at those. So, so for those who use R ns, um, also like, um, uh, inefficient parameter sharing. Let's, let's talk about that. So, so we'll talk about this concept in depth in CNNs, but, um, essentially, uh, n and Ns, uh, are using all the data elements in a densely connected manner. Um, so, uh, as you're learning the, the input, um, there is less parameter sharing, uh, because, um, uh, the connected layers where every neuron is connected to every neuron, uh, every other neuron in the, in the next layer, uh, this leads to like, you know, large number of parameters, explosion of parameters. Uh, so let's fake, for example, like if you take like a one mega X cell image, um, so that's like, uh, you know, um, um, thousand times thousand, right? Uh, pixels. Um, so, uh, so even when you're starting, you, it have like an explosion of, uh, parameters, uh, already a million. And then as you go through the layers there, if you were, if you're doing it in a densely connected fashion, like let's say 1 million times, the next layer is probably, I don't know, 0.5 million. That's, that's a huge explosion. Um, uh, yes, FNS are fully connected, uh, uh, uh, networks, uh, all the time. Yes. Um, also, uh, s by in, by their inherent nature, like, you know, um, this comes back to the sequential aspect of it, um, are not naturally equipped to handle inputs of variable length and sizes like, like, you know, for speech recognition or input sequences that have variable length. Um, uh, it's, it's not, uh, uh, they can't handle that efficiently. You need to handle it outside of the network. Uh, s uh, come to rescue in those particular cases. Um, there is also like, you know, lack of state or lack of memory in the network itself, uh, like, you know, capturing previous state and using that information to, uh, as a context to, to, uh, learn better. Um, so again, that sort of capital doesn't exist and is more addressed directly in LSTs and gs, uh, uh, architectures. Yeah. So parameter sharing will look at in depth, uh, for CNN's, uh, Regina, what I meant by that. Uh, so, uh, it's, it's a, it's a much deeper concept, which I, I think it'll take some more time, uh, and we'll look at it in depth in the cnn. So, uh, let's part that to the next session, uh, and we'll, we'll talk about that. Um, so, so, uh, let's keep moving. Interpretability, uh, again, this is in general a challenge of, uh, deep neural networks. Uh, so, uh, nothing new here. So, FNS, uh, they operate as black box models. Um, uh, that makes it difficult to understand how the network arrives at a given conclusion or a prediction. Uh, as I said, if you want interpretability of opt for, uh, addition trees or more linear models, uh, that are typically more transparent and interpretable. Um, yeah, some, some applications, uh, of f and Ns, like, um, widely used in, uh, pattern recognition tasks, uh, such as like character recognition, which we just saw, a handwriting, recognition, object recognition and images. Uh, they can learn to identify, uh, you know, making, um, and classify patterns in data that makes it, uh, valuable in those use cases. Uh, used a lot in, uh, fraud detection, uh, like, you know, credit card transactions, insurance claims, uh, by learning patterns in historical data, uh, f and Ns can identify anomalies, um, and flag potentially, uh, fraudulent transactions, uh, for, uh, you know, they're used in, you know, like things like, you know, customer churn, product churn, prediction, like, you know, customer attrition prediction. Uh, those are some use cases. F and Ns are typically used. Um, they can be used for sentiment analysis, uh, given a corpus of text and their embeddings, you can classify that as, you know, positive or negative sentiment. Um, s are used in financial forecasting, uh, like, you know, stock market prediction or economic prediction, stuff like that. Um, so, and recommendations. They're used like, you know, s uh, play a big role in collaborative filtering and content based filtering methods. Um, so, so those are some real world applications where s are heavily used, um, uh, in practice. Alright, so, yeah, I don't know why the font is kind of showing it this way. So, so we talked, uh, uh, uh, maybe lemme go back to this. Yeah. So we talked about, you know, uh, f and Ns, um, how the, the, the basic structure of f is like, you know, the fully connected nature of them, uh, discussion of input layer, output layer, and hidden layers, um, and, uh, you know, limitations of f and n like, you know, uh, the recurrent nature or parameter sharing, which we'll talk in depth in the next class, um, or, uh, uh, inability to maintain state or some sort of memory. And we also looked at a working example, like, you know, how to classify handwritten digits using F and n. Uh, so that's a quick summary of, uh, today's f and n class. Um, so we'll stop the instruction here and I'll stay around for like five minutes to answer any questions. Um, and we'll pick it up in the next class with CNNs and s um, and we'll wrap this up. So, uh, in the CNNs we'll look at like, you know, what are the salient features of those networks, uh, and you know, how the parameters are effectively shared there and all of that, uh, and, and s uh, as well, like, you know, why, uh, RNs are important and, uh, what kind of problems they're suitable to solve with, um, and all of that. So, so that's for next class. Uh, we'll discuss in depth there. But, um, let's, uh, uh, pause there, uh, for today and I'll hang around for a little bit if you have questions. Alright, thanks everyone. Um, um, again, um, just, uh, uh, review the material if you can today, but it shouldn't be a blocker for tomorrow. Uh, we'll look at, you know, how CNN architectures and RNN architectures contrast to how operate and what is the motivation behind those architectures. So we'll have a nice, uh, learning around that tomorrow. So, uh, look forward to meeting you all tomorrow. Um, um, so have a good, uh, rest of the day. Thank you.