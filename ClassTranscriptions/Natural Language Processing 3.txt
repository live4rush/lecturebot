Hey everybody. Uh, welcome to the session. We're gonna wait like two minutes just to make sure that everybody is here, um, before we get started. So let's just settle in for another one or two minutes and then we will discuss natural language processing part three. Great. Uh, we got a pretty packed agenda, so let's get started. Um, let's see if we're good. Yep. Uh, pen is working. Let's get moving. Uh, so natural language processing part three. So we first had, uh, natural language processing one and then two, and then now we are in three. My name is Ben Hanks. I will be your mc slash teacher, uh, for the morning, afternoon, evening, depending on where you are currently located. And I am a data and applied scientist at, uh, Microsoft, currently sitting in Seattle, Washington. Um, probably about 20, 30 minutes drive away from the headquarters. Uh, I work from home in my little shed here. Um, and I really like working there. Um, it's been fun. Uh, the same title would at different companies potentially be people, science analyst, uh, pe, um, data analyst, data scientist, like that kind of realm. Um, but Microsoft's interpretation of this role is data and applied scientist to, Uh, I have my master's in data analytics from Oregon State University. It's from the College of Statistics or within the College of Statistics. So it's largely theoretical, um, and a little bit applied. Um, and my undergraduate degree is also from Oregon State University and it's manufacturing and industrial engineering, which is, uh, building stuff and statistics. Let's get to know each other a little bit. Uh, so it's going to be me talking today for about four hours. Uh, so as much interactivity and back and forth as we can get, especially in the chat, uh, is gonna be super helpful for making a good engaging class. So let's start that right now at the start of the class. Um, so drop into the chat. Your name, your role, your company, like rough area where you're based. Thanks, Vasan already. Uh, dropping into the chat, some from California. Uh, GM software engineer from Texas. Another Ben, a wonderful name. I've been to Austin a couple of times. Um, great cycling around there. Um, someone from Lima, a data engineer or San Francisco. Um, how is an EM in San Francisco? And then while responses come in, uh, just a fun fact about me. Um, I ride my bike a lot after this lecture. I'm going on a mountain bike ride. Um, and bikes are a great way to get exercise and have an interesting hobby all at the same time. Oh, if a sun rides a road bike. Awesome. Uh, behind me in the blur, you can see my track bike. I've had a couple of tracks as well. Um, actually in a race, I broke, uh, the frame with my kneecap once, uh, crashed and smashed the frame with my knee. I was surprisingly fine. Uh, all right, let's talk data science. Uh, so like I said before, um, I am here for you. I am here to discuss things with you all. This is a class, um, that you are supposed to get value out of. So don't be shy. There are no stupid questions. Um, we are here to learn. If you already knew the topic, you would not be here. So feel free to ask any question, no matter how potentially embarrassed you may be. Um, I will be happy when we get any question. Um, so don't be shy to speak up. We do have a TA support today, uh, and they will be active in the chat to help answer questions that I'm too distracted to get to. Um, we're gonna be doing a review session on Thursday for assignments. And then, um, there is a career, uh, review session and like collaboration session on Wednesday, uh, Pacific time. I think we're, a lot of us are on Pacific. Um, and yeah, just let me know if we want to recover or review a subject, um, on any of these slides. All right. So just a quick review where we are in the natural language processing, uh, framework. We are here, natural language processing three. We are almost done. We are here today at the end of this session, we will end here. So I am stoked to get to the, the end of this vector. Um, so natural language processing one, that's where we talked about text, pre-processing, uh, exploratory data analysis using some libraries, sentiment analysis, image captioning. That was really cool. Um, natural language processing part two. Uh, we discussed a little bit of like sequence to sequence modeling. We're gonna touch on that again today. Um, and then transformers, part one. Um, today we are going to talk about transformers, part two, revenge of the fallen. Uh, and then Bert, uh, which is a really cool algorithm, um, that we will, uh, focus on a little bit towards the end of the class. Like I said before, sequence sequence models. Uh, we discussed some well-known archetypes architectures, um, and we leverage some nns for current neural networks to drive the sequence encoder context, vector decoder output sequence, um, pipeline. And then in this class, we're gonna be using transformers to fix some of the shortcomings that we had in, uh, the previous more vanilla algorithms. So we are here. Um, we're going to start with, uh, the recap, which we already discussed a little bit. Then we're gonna discuss some transformers, um, transformer components, dive a little bit deeper into transformers. Then, um, we're gonna take a little bit of a break. I'll try and do two to three breaks that are around five or 10 minutes. Um, we do have a lot of content to cover today, so I will try and balance, uh, how terrible the test of endurance is with, uh, how well we are doing progressing through the class. Um, and then after the first section, we're going to discuss Bert, Roberta, uh, a couple of other Bert algorithms and Burt adjacent algorithms. Um, and then we're gonna do some code demos. All right, let's get into transformers. So problems with what we've learned so far, we've had kind of the traditional setup of, um, the sequence to sequence pipeline. So s two s sequence, two sequence. We have the input, um, and then we have our encoder, and then we have our context vector. Um, and then that is decoded. And then we have an output of a sequence. So we have sequence in and then sequence out. Um, also feel free to drop in the chat. If, uh, you cannot read my writing, I will try harder. Um, so the biggest issue with this methodology is that everything needs to fit in this context vector. Uh, so like, let's say we have a huge text, like pages of a book, like an entire Iliad or something like that. Um, that means that everything needs to fit into this context vector and then be translated. Um, and then like if we pass on the vector after that, I believe in Santa very short vector, um, then we, um, are not adaptable in size. So that sequence could take, um, a significant amount of extra storage space, so it's not quite as efficient. Um, also decoders decode one token at a time sequentially. And in this presentation, we're gonna be talking a lot about parallel work, um, which makes the process much more efficient. So especially in large strings of text, which we're, um, gonna be doing a lot, and this extra power we get through parallel, um, work really helps us, uh, in more robust corpuses lots and lots and lots of texts. So what is a transformer? So long story short on transformers is we take a text sequence as an input and we produce another text sequence as an output. So we transform, a classic example is input in a specific language and then output in a different language. So like Google translating, um, tra uh, encoding that, and then decoding it as, I am a student, this is French, or I am a student. And this is English for the same concept. So structurally, transformers consists of two main concepts, the encoder here, and the encoder processes and embeds the input sequence information, and the decoder here, which generates the desired output while preserving the context, which is very important. And we'll get into and semantics. So in language translation, encoder process, uh, on the input language, embeds that value into a context vector. So you can see context vector right here. And then the decoder uses the context vector to generate the translation, maintaining the original meaning and context. And with these concepts in the next coming slides, we're gonna explore how transformers paved the way for advanced models like Bert, and even a little bit chat, GBT, which is the coolest model in the world right now, debatably. All right, so there's a lot of stuff going on in this slide, don't worry. We'll get through it together. Um, so here's translation. Translation is really, really complex. And in modern day, it's almost, uh, a non-issue as long as you have a computer by your side. And that's because of natural language processing. We have solved a lot of this problem. Um, so one of the biggest problems in translation is going to be things like word by word or ordering. Um, so if we look here at the top, can you help this sentence to, or can you help? See, even I'm having a hard time looking at the order. Can you help this sentence to translate? That's if you take this sentence and have direct translation, because in English, we have one set of grammar rules that's not the same in other languages. So to make sense, we need to have the AI algorithm use this methodology. So that is a much more robust, more difficult analysis, because this word needs to move all the way over here. We can't just translate it, um, straight as shown here. So to address this, attention mechanisms were developed, which allow models to access all input sequence elements simultaneously, and select words that are the most contextually relevant at each time, at each step. So right around 2017, the transformer architecture, uh, took another step forward by introducing, uh, concepts like self attention, which enabled it to process words all in parallel. Again, there's that parallel word, uh, which bypassed the need for recurrent neural networks and enhanced the scalability and efficiency of natural language processing. So with the foundation of how attention mechanisms enhance sequence modeling, uh, we're gonna move on to explore the ins and outs of more advanced models and ensure that we understand the building blocks that make them very, very effective in modern day. Um, again, feel free to ask as many questions as you can. Um, let's get stoked on natural language processing. All right, let's talk about transformers. Um, so first we're gonna kind of talk about the training phase of the transformers. Um, and then we're going to, uh, potentially make inference and, uh, prediction, uh, through these next few slides. So, unpacking transformer training, uh, let's demystify how transformers operate during their training phase, where models learn to make accurate predictions by adjusting its parameters based on the input and the target output. So here you can see the transformer. Let me switch to a highlighter, so it's a little bit easier to see the background. So this here is the process. The input text is, you are welcome right here. Oh my gosh, that's terrible. Highlighting. You are welcome. And then if we follow this pipeline, we enter the encoder phase, part one, and then we have a context factor out. Uh, and then here is an exciting concept. We can encoder it again to improve, um, the model accuracy. We'll talk about that in the next few slides. Just we can do it for now. Uh, and then we continue training, have the encoder out, and then we go through some decoder processes, um, and then have the very simple output of donata at the end of all these processes. So just voicing that a little bit in more detail. Um, initially the input sequence undergoes several transformations. It's tokenized into smaller units, uh, such as words and sub words and things like that. Uh, one hot vectors, so that it's computable and that it's supplemented with potential embeddings to preserve the order of the words and the context which they're in. Um, and then as we go through the encoders, um, the process input navigates through lots of different layers, sometimes as many as six encoding layers, um, where each layer works to refine, refine, refine each time it goes through another encoder. Uh, and then looking at attention vectors, which we'll discuss a little bit more coming out, this is kind of the 30,000 foot view. Um, once we reach the final encoder, we obtain the attention vector. This vector has a ton of contextual information for each token in the input sequence. And that is gonna be pivotal in generating the output, uh, subsequence and the decoding phase. During the decoding phase, uh, that attention vector is utilized within the decoder, uh, to produce the desired output sequence, which ensures that it is contextually and semantically aligned with the input. And as we transition into discussing, uh, the encoding phase and then the decoding phase after that, uh, just to remember that this attention vector and context vectors, they both play crucial roles in, uh, bridging that encoded output with the decoded output. So during this first part of the presentation, we're gonna have encoder, which is the start, and that encodes the input. And then we're going to have decoder at the end, and that decodes the input. And then after that, we've got the outputs. So again, at a top level, um, we can look at the decoder phase right over here with trending of the transformers. Um, unlike traditional methods, the entire target sequence is gonna be fed into the decoder, which is a technique kind of like teacher forcing. So teacher forcing allows all output tokens to be predicted in a single step, which is really cool. Um, the processed target sequence is gonna be passed through several decoder layers, just like we had with encoder. This could be repeated many times, um, and the output is going to be the most accurate embedding possible. Ultimately, the decoder outputs a sequence that corresponds to the tokens with the highest probability of being the accurate Next word, crafting a coherent, contextually relevant output sequence. How? That's a great question. Uh, looking at how teacher forcing is done through the next few steps. Um, we will be getting to that a little bit in the next couple of slides. All right, looking at the inference phase. So the only difference between the training and the inference phase target sequence is fed sequentially to the decoder part of the prediction. Um, and we don't have access to those future tokens. Um, so let's explore what occurs during that inference phase, which is when the trained model is deployed to make predictions on unseen data. Um, different differentiating the training and the inference here. Um, training and inference phrases might seem pretty similar, but the fundamental distinction is during inference, we lack access to the actual target sequence. So predictions have to be made sequentially as opposed to in parallel. Um, unlike in the training phase, the entire sequence is available, the entire target sequence and can be fed into the decoder. Um, and during inference, the target sequence is input into the decoder token by token. And predictions are made in a stepwise manner as opposed to parallel. Um, the token by token sequence, uh, can have implications on the output quality and consistency, uh, given that the model does not have future context to rely on. And, uh, we have to make the best prediction possible based on the current and past tokens. And with a grasp on what differentiates the input, the inference phase. Uh, let's get a little bit deeper into further aspects of transformer models and kind of explore, uh, how they're adapted and fine tuned for specific natural language processing, um, tasks in the upcoming discussion. Great questions so far. Uh, thank you, uh, to the TA for answering those quickly. Um, oh, and it looks like we've got already an answer on the, uh, teacher forcing function. Thank you. Keep the questions coming. Um, best way to learn. So some advantages, some advantages of transformers. Like I was saying, um, parallel processing really improves the speed of the models. And while sometimes we can run those, uh, models and algorithms in the backend, and speed is not a huge concern, um, what makes parallel processing and speed important is because we can do larger tasks. So in prior days before all this parallel processing and increased, uh, computing power and speed, um, we would try and do these big tasks and fit models on, you know, billions of records, and it would just not work. Even if we gave it a billion hours of time, it would just have overflow errors and things like that. Um, so now with all these improved parallel, uh, methodologies and cloud computing and things like that, we can solve these problems that before would just error out and just, uh, have overload issues. So that, um, while it may not seem super critical, is a huge importance. Um, and then another thing is having effective handling of long-term dependencies. Um, increased model capacity, which is really related to this step as well, um, or advantage. And then the flexibility, uh, with variable length and sequence, um, again, really compresses our models and things like that. Right, let's keep rolling. Uh, so some limitations. Um, we're still gonna get pretty high computational costs, so we're just kind of doing a ton of stuff. Um, and with transformers, while we have severely improved our computational expense, it's still going to be a lot of stuff. Um, there is some inefficiency. Um, we're always with AI going to be, have to, uh, going to have to consider overfitting, uh, transformers is especially, uh, non robust to that. And then hyper parameterization tuning and, uh, performance optimization is something that is really going to improve our models. All right, so that is most of the transformer section. We're gonna be talking about it more in the coming slides. Um, but that's most of the education on transformers. Um, now that we have that complete checked off, uh, let's talk more about in more detail encoders and decoders. So we're going to start this conversation by looking at incoders. Here's, remember before when we were talking about how we have encoder one, encoder two, and coer three and coer four, et cetera, et cetera. Uh, so here's that in a little bit more detail. Um, this could be encoder one, this could be encoder two, this could be encoder three, this could be encoder four, and we can repeat this whole block. We can repeat each one of these processing steps as many times as we want for additional model accuracy. Um, so let's talk about it at kind of a high level overview first. Um, so we're gonna talk about encoder layer unveiling Within every encoder, there's three primary layers, all interconnected with residual connections to avoid vanishing or exploding gradient or other issues. Uh, first the self attention layer, which we're gonna talk about first allows the model to focus on different words for a given input. Next, the layer normalization, commonly known as layer, norm, uh, stabilizes activations of the neurons. And finally, the feed forward neural network, uh, performs specific transformations of the activation layers. We're gonna be focusing mostly on the self attention and layer norm. Um, and then feed forward will be in a couple of examples, but we're not gonna touch on that explicitly. Um, and then after that we're gonna talk about decoder layers, which is this half over here, right before the output. Um, it has some similar things going on. Uh, you'll recognize a few words like layer, norm, uh, and self attention and feed forward. This is different though, encoder decoder attention. We are gonna talk about that at length. Um, but just in general, that additional layer helps the coder focus on the relevant parts of the input sentence, ensuring that the output is contextually and semantically aligned with the input. Um, and then exploring residual connection. Briefly, it's worth noting that the inclusion of residual connections within these layers is really important. Um, these connections allow the gradients to flow through the network more easily by bypassing certain layers, and that aids in training deeper, more robust models. And as we step into the intricacies of each of these layers in subsequent slides, just remember that the strategic construction of these layers is what empowers transformer models, uh, to effectively manage sequential data in natural language processing. Alright, embeddings. So, um, before we get into the, um, encoders, uh, let's talk about embeddings, which is usually, uh, the entry point. So when we wanna start using the transformer models, we create these embeddings, these word embeddings. Um, it allows the computer and the models to understand and process natural language data. These embeddings are achieved through algorithms like we talked about in NLP one, NLP two, uh, like word vec and glove. Uh, and these transformed each word into a high dimensional space where semantic similarities between words correlate with spatial proximity. The key in this is converting linguistic information into a format that's computable, while maintaining semantic relationships. Um, and focusing on the bottommost encoder, we're gonna be looking at this embedding process happening only in the bottommost encoder. Uh, so despite all encoders sharing a common structural design, it's this initial encoder that ingests the original output and translates their words into vector representations. And then another thing is gonna be uniformity across encoder blocks. Uh, every single encoder is going to receive a list of vectors, each of size, n uh, representing the input words. So that n is gonna be flexible depending on the input words. Um, so that uniformity or max input words I should say. Um, and then the overall uniformity is where each encoder block processes the input and the shape and the type, and that allows the model to be scalable and manage various que uh, sequence lengths and complexities. We're gonna talk a little bit about pads, um, and how that helps with, uh, managing sequence. Um, and going forward, just think about how embedding and initial encoding lay the foundation for processing steps in the transformer models that we're gonna be talking about. Okay, so today we are talking about transformers, and transformers have encoder steps and decoder steps. Before we get to encoders, we have the embedding, and that is the input to the encoder. And then the output, uh, is going to go through lots of decoder phases, and then we're gonna get some theoretical output that might be a translated sentence or something like that at the end. So that's kind of like contextualizing where we're currently at in the lecture. All right, next slide. Positional encoding. Uh, so let's talk about a little bit about positional encoding an important step that helps us kind of model and understand where each word is in a sentence. Um, like we saw in the previous slide with, uh, the language translations, it's gonna be really key to remember just the context of the word, not necessarily the order or sorry, and the order as well. Um, so the cat chases the dog, for example, means something very different, opposite, in fact, than the dog chases the cat. So we need to use this positional encoding to make sure that the model is aware of word order, otherwise there's gonna be he hallucination in word salad, and, uh, issues like that. So positional encoding involves adding special vectors that you can see here, these vectors, um, or lists of numbers to the word embeddings. So we start with these word embeddings, and then we add the positional encoding here. And then this gives us a more accurate contextual vector that has positions and the word embedding. Um, and adding that allows our models to get extra help, understanding word order and creating outputs that make sense in a natural language processing algorithm. Um, and in the next few slides, we're gonna be moving on and talking about how transformer models use this positional encoding to make models handle this language in specific ways. All right, so there is some math on this slide. We're not going to do any proofs today. We are just going to talk about it very briefly. And the important thing to remember is this is kind of how it works here. So we have this positional encoding, um, and we have the position here. Here's the input, uh, text we have, hello, my five word sequence. And then we just use sign and cosign as a simple coding function, uh, to have, uh, positional on coatings. So sign is going to be all of the even cells. So like even cells, we start at zero. 'cause this is based in code. Uh, so this is going to be sign. So this is gonna be even, even even. And then co-sign is going to be all of the odd cells. So we have co-sign, co-sign, and then that allows us a little bit of mathematical mapping. Uh, looks like there's some questions coming in. Thank you to the TA for answering those. Um, and this is mostly on this slide, just kind of a peak at the math, um, of positional embedding. So let's keep rolling. Alright, looking at the input for transformers. Um, diving in a little bit deeper in our transformer models, the input is actually a three dimensional structure here, and we can describe it like B, L, and E, where B is the batch size, and that's the number of samples in the batch. Bigger batch sizes mean we're giving the data more data at, or model more data at once. So that's gonna be computational load, but also additional context. Uh, pros and cons there. Sequence length is l number of words slash token in a sequence. Uh, so like if I is 10, uh, then our sample batch has 10 words, for example. Um, e is embedding size. So this is how long our embedding vector is for each word or token. So like, if e is 300 word or 300, that means that each word is represented by a list of 300 numbers. And looking at it overall, our output is A 3D structure of B, L, and E where we define how many samples we put into the input. B, how many words are in each sample L and how many numbers are used to represent each word. E. And understanding these input dimensions is gonna be key as we move forward and explore how these inputs are processed by the transformer. Uh, what is B, l and E in the figure? So looking at this figure, uh, let's see, we've got B as batch size. Uh, so number of samples in that batch. So that's going to be this five here. Um, so that is going to be the length. And then four is the number of tokens in a sequence. So that's gonna be l that's going to be, sorry, let me do a different color there. So this is gonna be l that's gonna be the width, and then e is going to be the embedding size. So that's gonna be three, and that's gonna be the depth. How does that answer your question? Uh, answer live. All right. Cool. Great question so far. All right, here we go. We have all of the context we need. Uh, let us zoom in and talk about ENC coders. So you'll recognize this slide. This we've given context to, uh, embeddings. We've given context to positional embeddings. We've given context to, we are now here specifically, we are here. Self attention is what we're gonna be talking about next. So, self attention is a mechanism that allows neural networks to weigh the importance of differing elements in a sequence relative to each other. It's been particularly influential, influential in NLP, and it's a key component of transformers in general, specifically the encoders that we're talking about now. Um, attention mechanisms. Mechanisms were initially developed to kind of like mimic the human ability to focus on a specific part of a scene or a sequence when processing information, which is pretty cool. Um, in the context of neural networks, attention mechanisms enable models to selectively focus on different parts of the input sequence When making predictions, uh, specifically the self detention mechanism, um, allows a model to consider the relationships between different elements within the same sequence. In the context of NLP, these elements are usually words or tokens in a sequence. And the key idea here is that each element can attend to all other elements in a sequence including itself, kinda like a correlation matrix, similar to what you're gonna see in the next couple slides and some benefits, uh, we can capture long range dependencies. Things like RNs struggle with capturing long range dependencies. Um, self attention on the other hand, allows the model to consider all elements in a sequence simultaneously. So it's gonna be really helpful for those translation and, uh, switching those words around, uh, helping handle that. Uh, and that makes it a lot more effective for tasks that require understanding, like the global concept or context. Uh, paralyzation, like I was saying before, very important. Um, and self attention methodology is very, um, plays well with paralyzation, uh, and that allows for more efficient processing of sequences. Um, this is in contrast to recurrent neural networks, uh, which are usually very, very sequential. All right, here's the first practical example. Um, so self attention is often commonly referred to as intra attention. I think self attention makes a little bit more sense when you're communicating it personally. Uh, so here we've got a couple of sentences. Um, the monkey ate that banana because it was hungry to, and what we're going to ask here is, um, what is it referring to? Is it the banana or is it the monkey? And enriching the context of each token will really improve our results, and that is a, a visual demonstration of the use of self attention. And here's another one. Um, so we're looking at the cat here, and we have two examples. The cat drank the milk because it was hungry. This is pretty clearly talking about the cat. Uh, the cat drank the milk because it was sweet. Very similar sentence structure, but it is referencing the milk, not the cat. So how do we get the model to determine which one is which and where it's representing. Um, another example would be like in job title, natural language processing, um, and job title, natural language processing. You can consider a picker working for Amazon would have a very different job than a picker working for, um, a farming company. Um, so that is going to be, um, very different depending on context, and we can understand theoretically. Um, but in the next couple of slides, we're going to be looking at it mathematically. So calculating self attention involves lots of different steps, and it's commonly used within the context of the transformer architecture like we're talking about now. Um, the main organizing concepts are going to be the key, uh, query and value vectors. So here we have query vectors. Here we have key vectors, and here we have value vectors. Um, all of these vectors are going to be, the embedding vectors are gonna be created by taking the dot product of the embedding of each word, um, that we learned during the training process that the model learned during the training process. Um, and for each element, they're derived from the original input embeddings. So the attention score between the query vector and a key vector is calculated using that dot product for each element I in the attention score, uh, scaling the attention scores to prevent the gradients from becoming too small during the training, the attention scores are usually scaled by the square root of the dimension of the key vectors. So that's gonna be kind of like a moderating denominator that we're gonna look at in a little bit. Um, we're also gonna be using the soft max activation, um, to, uh, normalize the weights even further. And that ensures that the attention weights will sum to one, uh, for each element in the sequence, which, you know, probabilistically probabilistically something to one, um, is going to be important as well. So we ensure we have an output each time, um, weighting the sum of, of each of the values. The final step of this process involves taking a weighted sum of each of these value vectors, using the calculated attention weights. Uh, and that results in context vector for each element is a combination of all of those values and all those elements, and all of those steps are performed independently for each element in the sequence. And that results in a whole new set of context vectors, uh, the process allows the model to attend different parts, uh, for each sequence in the element, and that captures the context information really effectively. Uh, lemme pull up the qa. Um, it's important to note that the context of the transformer architecture, um, these computations are typically implemented using operations for efficiency during training and interference. Uh, the self attention mechanism contributes to this model ability to capture dependencies and relationships in the input sequence. Um, got some shout outs from Benjamin. Uh, thank you. I'm assuming we're talking about the TA and, um, appreciate the, uh, stoke on the class. Um, all right, diving a little bit deeper into self attention, I know this is scary, don't worry. I'm gonna go through it with you. We'll get through it together. Um, so starting with our words, and for each one we're gonna calculate three things. We're gonna calculate query, which is the query, uh, process here. Uh, and then we're gonna calculate key vector, then we're gonna calculate, uh, the value vector. And with those powers combined, we're gonna get a strong output vector. Um, so getting it step by step, we need to figure out how much each word should pay attention to every other word. Uh, remember that correlation vector, or sorry, correlation matrix concept. Um, this is our attention score, and that's kind of like a measure between the similarity of each words, between each words. And it's calculated using, um, this soft max formula here, Z is getting red. Um, here we're gonna be dividing by the square root of dk, and that is gonna be that moderating influence that I was, uh, discussing previously. Um, and the size of our key vectors is going to, um, keep our numbers nice and stable. Um, what happens is we will turn these weights or turn these scores into weights using the soft max function. Um, this makes sure that all the weights of each word add up to one. So all of this Z is going to add up to one. Um, and then that's pretty much it. So this single vector made up from mixing all of the sub vectors here, uh, gets passed down to the next layer of the model. And then we repeat the process over and over and over again. And it may sound tricky, but all we're doing here is helping the model figure out what to focus on when it sees a bunch of words. Think of it as helping the model know which words are best buddies and need to stick together to make sense. Great questions coming in. Thank you to the TA for answering. All right, calculating the attention score here. So now we're on the final step. Remember, the scale attention scores are passed through the soft max function to obtain normalized attention weights. This ensures that attention weights some to one for each element in that sequence, because remember, this is all probability based and probability has to add to one. Um, and the attention score here is multiplying the dot product of the query and the key matrix normalized by the square root of the embedding vector here. Um, so that's just normalizing so that we can more easily process things. We have this vector space, and when we normalize it, um, it plays a lot nicer with, um, the model. So softmax function is used on the resulting matrix for that final output. And that's pretty much it. Great work that is self attention encoding. We're gonna touch on it again briefly when we, uh, talk about decoding process, but for now, let's get into layer normalization, frequently known as later norm. Um, so done, done. Wait, this is done. Uh, and then self attention. We just finished. Good job, everybody. And now we're gonna talk about, so layer normalization aims to normalize the activations within a layer that helps us, uh, con or that helps the model converge faster during training. Quick summary, um, normalization within a layer normalizes the values within the layer of a neural network that is applied independently, uh, to each neuron activation in each layer. So some learnable parameters here, layer normalization introduces learnable parameters, uh, which are gamma scale and beta shift. Um, and that allows the model to adapt and learn the optimal normalization for each neuron. Um, some benefits of layer normalization. Um, it's really good for stabilization training, uh, helps stabilize the process by reducing covariate shift. Covariant is varying together, um, and making it easier for the model to converge, uh, to an answer, reducing sensitivity and initialization. Um, so it just makes it more robust to departures from, um, uh, a central tendency. Um, applications like where we would use this, it's mostly used, um, it right, right after the self attention mechanism. Um, and it's sometimes applied just independently, uh, to each position in a sequence which enhances the, uh, our model's ability to handle varying scales and input data. Uh, lots of times it's used in machine vision, um, which is pretty cool. It, so machine vision, we're just talking about pictures. Um, and layer normalization allows us to normalize the layers in the image. So that does things like bring it to a standard format. So starting with a full, full color image, think of a big old giraffe picture. Really pretty lots of, uh, yellows and browns and, uh, Savannah. Um, so that brings it, uh, the normalization of that image allows the model to process it more easily. So if we start with a big huge rectangle picture, we might normalize it to a black and white four by three, uh, picture. And that allows the model to more easily, uh, represent it or fit it represent is a less accurate word. So diving a little bit deeper into layer normalization, um, in this model, uh, or in this cut of this model, uh, we're gonna talk about the add and normalized layer. It's often referred to as the residual connection followed by the layer normalization. Um, it also serves to stabilize the activations and facilitate the training of deep networks. Um, a couple of components we can add residual connection and normalize the layer normalization. Um, so we have this positional encoding, um, that we have as an input. And then here's the excellent positional coding vector. We have pass through self attention, which we all are super versed in right now. Um, and then we have this additional vector that we pass into layer normalization, and now we are normalizing. Um, so during this process we're going to do that draft picture, and then we cut into black and white, which makes it a little bit easier. And then from there we bring it into another step of, um, making it to the same aspect ratio. And that process we're gonna look at a little bit, uh, later in the practice. Um, and we're gonna be, uh, doing some coding demos and things like that. So, um, like we were saying before, layer normalization stabilizes the activations in the neural network and make sure that they don't reach extremely high or extremely low values. Um, and it normalizes the output of each feature across the dimension. So it's another encoder layer, um, that aids in the encoding of this process. And there's a couple different types of layer normalization. Uh, predominantly it's gonna be layer normalization and batch normalization. Um, so you can just think of it as batch x normalization, layer y normalization. So batch, we're gonna be looking at rows, row, row, row, and then layer. We're gonna be looking at columns column, column column. Um, and there's pros and cons of both. Uh, layer normalization has some advantages which mitigates the risk of unstable training dynamics, uh, by maintaining activations in a normalized range. Uh, this partially alleviates the covariate shift problem, which remember we were talking about before, before. Um, and it keeps the distribution of the inputs to a layer a little bit more stable during training. Batch normalization becomes a little unstable with small batches. Um, and that's gonna be needed when the input batch normalization are needed, when the inputs are large. Um, and for example, in like segmentation and detection tasks. Um, so different normalization approaches mainly differ in how, um, the average and mean estimates for the, um, eq. And that's the main layer versus batch normalization. So that's kind of the top level overview of encoders versus decoders. So we've got the input process, and then after that input process, we have the encoders, the encoders self attention and layer norm is what we talked about. And that can iterate many, many, many times, as many times as we need to get, uh, the most accurate model, um, while preventing overfitting. Um, and then after that process, that final context vector, which is the output of the encoding process, we pass that to the decoder, which we're gonna be talking about now. Um, Vasan was asking, can you explain more why there are two input machine and thinking three slides before, um, oh yeah, I see what you're talking about. Um, so these are passing, let's just go back to the slide quickly. So this is reflecting how we can pass in parallel. Um, so thinking and machines are just, um, two different words that we're passing through. Um, so in this, uh, vector of interest, we're going to be talking about thinking, and we have the positional encoding for thinking. And then we pass that word through the self attention and then through layer norm, and then all the way through the rest of the model. And then we repeat that process, um, and repeat is simplified. Um, this happens in parallel machines is another word. Um, and in parallel we pass that through self attention and then layer norm, et cetera, et cetera. And then we look at the relationship between those. But great question. Sorry, I should have clarified that earlier. Um, Let's keep moving. We are on track. Good job, everybody. Um, so we've got about 30 minutes to a break just to kind of like prepare yourself, uh, getting into decoding. Let's set shed some light on the decoder section of the transformer model. Even though it mirrors the encoder process in lots of ways, there's a couple key distinctions that tailor it to its specific role in the generation of translation tasks. Um, so all of the stuff we talked about, that kind of ends right here. So the prior slides in the presentation are all over here somewhere. And now we're focused right here on the decoder. So we have the input of the, um, embedding the context vector, um, and that goes into the decoder. Um, so masking the input sequence, the decoder needs to, uh, to handle data in a way that prevents it from seeing future tokens in a sequence during training, which is crucial to ensure that it doesn't cheat the training process by using information it shouldn't have access to the masking technique, um, is used to hide feature tokens. What is masking? We will find out soon enough. Uh, we've got some really interesting demos, um, where we're gonna be talking about masking at a deep level. Uh, but in general, it ensures that the prediction for a word doesn't depend on the subsequent words in the sequence. Um, in practical terms, it's like making sure that the model doesn't peek ahead into the sentence when it's trying to predict the next word. Um, and what the heck is this, right? We didn't talk about this encoder decoder attention. We know this. Uh, we know layer norm, uh, we don't know encoder decoder attention. Um, so while the encoder digests the input sequence, the decoder ought to produce an output sequence that can be of different lengths and is conditioned on the encoder output. And the deco encoder decoder attention layer helps the decoder focus on relevant parts of the sequence no matter what the length is. Uh, in simpler terms, it allows each position in the decoder to attend over all positions in the input sequence, and that's captured in the encoders output. This forms a kind of bridge, allowing the decoder to consider information, um, from the input sequence as it generates each word in the output. So in top level summary, while our decoder also has the self attention and feed forward networks like the encoder process, the two distinctive out aspects which are masking and encoder decoder attention, uh, adapt it for its role in generating coherent and contextually relevant sequences, taking into account both the proceeding words and the encoder insights. And as we get deeper into this presentation, uh, just remember understanding each component in isolation will help us appreciate how they work together and make these models so powerful. Hashtag better together. Um, yeah, let's keep going. All right, starting at the top, just like we did before. Done, done, done. Same thing. Done, done this whole process. Done. Good job, everybody. We are now here, specifically here, self attention. A decoder is the component or module playing a crucial role in this sequence to sequence language model, like I was saying before. Um, and in sequence to sequence tasks, attention mechanism, decoders often incorporate this attention mechanism process to selectively focus on different parts of the input sequence when generating each element of the output. And this allows the model to capture long range dependencies and improve the quality of generated sequences. Alright, attention masks. Um, we've established transformers, like the model we're using right now, use self attention mechanisms to weigh the importance of different words in a sequence. But here's the catch. How do we ensure the model doesn't cheat and look at words ahead of time that it shouldn't see? During training in RNs, recurrent neural networks, all we would do is have like a training set and a test set. Um, here we have attention masks, so we can kick off the sequence looking at special token denoted as a starter token here at timestamp zero, oh, let me change. Here we go. So at timestamp zero, when this training algorithm starts, we use the starting token. At timestamp one, we decide on the next token. So during training, um, we know the actual expected output, so we attach it to the input sequence. For example, if our target phrase is, you are welcome, the input becomes start, which is you right there. Um, but during prediction, we don't have the actual output, which is, uh, generated by the model, uh, P zero at timestamp zero. And the output, um, when we mask will be one of these selected mask words. Um, at timestamp t where t is the end of the process, um, we have these masks, um, that the model then can guess to get the final output. Uh, so a set of different words. We've kind of established the sequence length where the sequence is you are welcome. Um, so that's sequence length is three, but our actual content is T minus one. 'cause remember, we start at zero, um, tokens long at that final timestamp. So looking at future tokens from T to the final, um, uh, to the final length of the vector, uh, we do hide them using masking, and that ensures that the model doesn't peak at future tokens during the training process. And then during the, uh, final prediction process, that's when we, um, predict those masks. And the next couple slides are gonna be really helpful for looking at that process. So in the self attention layer, you remember these value key query very similar to what we had before, value, key query and self attention. Um, so the first layer of this self attention process, um, we have, um, the decoder, which computes its very own query key and value vectors using the input it receives with the objective of allowing each token in the input sequence to consider others establishing contextual relationships within the sequence itself. Uh, remember that correlation matrix we were talking about before. And then, um, the next step is looking at the encoder decoder attention layer. The second type of attention is encoder decoder attention, bridging that connection between the encoders, outputs, encoders outputs, and the decoders generation process. The key in value vectors here originate from the encoders final layer, bringing them information from the input sequence. And while this is happening, the query vector right here, um, is sourced from the output of the decoder self attention layer. Um, and that contains insights from the partially generated output sequence. Um, so just to make an analogy, to make things a little clearer, imagine the encoder process here, um, has read a nice book, the input sequence and summarized it. The self attention in the decoder is like discussing the summary, the summary, um, among various friends like a book club. So here we're in the book club, um, and looking at each word in the output sequence to understand different perspectives. The encoder decoder attention is in this analogy. Um, it's continuous, continuously referring back to the original book while discussing and ensuring that the conversation stays relevant and accurate to the source material. So it's kinda like a referee in this process. And the source material, again, is that input sequence. So in a nutshell, self attention allows the decoder to contextualize the tokens within its own generated sequence. And in parallel, the encoder decoder attention allows this output to remain relevant to the input, preserving the connection to the original message. And remember how I said in parallel, but then this isn't parallel. This is sequential. And the reason for that is we have to demonstrate this process, um, so that it's as easy as possible to understand. But each one of these processes are happening in parallel at the same time to allow for, uh, a more robust process and, sorry, uh, faster. All right. Remember how we talked about attention masks? So this is just kind of a, a very low level demonstration of what that attention masking process is. Um, so if the expected output of this sentence is re esan in Spanish, apologies for the pronunciation, um, at times step or at timestamp zero only. The first token is passed at this T zero here. This is T zero right here. Um, and for all the statisticians, this does look quite a bit like a correlation matrix, doesn't it? Um, at the timestamp zero only, that first token is passed. And that, looking at this visual here, um, this is the first process. And then at timestamp T one, this is T zero, this is T one, T two, T three. Um, so at t one sequence with two tokens is pass the decoder, and then three tokens, and then four tokens. So remember, it's T minus one, T minus one, oh my gosh, T minus one. Um, and then in the next slide, we'll talk about it a little bit further in a little bit more detail. Um, so that parallel concept, using the attention masks with that encoder stage, um, just like with the decoder, the encoder also uses attention masks, ensuring that self attention mechanism operates appropriately recognizing and respecting the general structure, um, and the intent of the input sequence. So looking at like varied length sequence and padding, remember we were talking about padding before, um, real world scenarios. Not all input sequences align to a uniform length. So our model necessitates fixed size input, um, and that's where we used padding. Um, the way I think about it is, if you remember back to, um, the, uh, calculus where we have the integral and then we have like X squared, and then we take the final integral, um, and then we get plus constant, right? Because we don't know what's after that. So I think a padding kind of like this plus constant, um, it just allows for extra space, um, for the, um, context vector to be processed. It's like a placeholder. Um, so we do append that special pad tokens, um, to shorter sequences to match them up, uh, to a consistent length. 'cause that length has to be consistent across the whole model. Um, but the caveat is we don't want the model to pay attention to those padding tokens because they're not meaningful words, they're just spaces. And with that, um, attention masks kind of act like those guides and they steer the model's focus towards the tokens that carry actual value, um, and away from the pad tokens. So here in this sentence we have, you are welcome. And then, um, previously in the model we had, uh, a four, a four word sentence. So we have to now replace that with pad. Uh, when we, uh, look at you are welcome vector. Um, so this attention masks are going to prioritize. You are welcome and deprioritize pad. Um, and it's kind of just telling us, Hey, focus on these tokens and ignore the padding ones because they're placeable, there's and not real. Um, in mathematical simplicity, the attention mask is often the binary sequence kind of mirroring, mirroring the length of the input. Um, and a signal, um, signals pay attention to this token and zero it meaning ignore this one. So these are all ones, And these are all zeros. We're gonna ignore all of these. Um, so if we have a sentence like, I love NLP, uh, where we assume NLP is one word I and the padded sequence becomes, I love NLP with a couple different pads. Um, here where you have, I love NLP, um, and then have the same level of pad. Um, and that would have the same, uh, ones and then all of the zeros in the pad areas. Why is the attention mask three by three, uh, versus stare in a previous one? Oh, so it's similar to the, um, it's just simplified. Um, so this, we have the pad, if you recall. Um, so that pads out the whole vector. And, uh, this also kind of demonstrates the parallel processing as opposed to the other, which was, um, explained in a more sequential way. So, good question. Um, and just wrapping this up, attention masks and padding together ensure that the transformer model processes variable length sequences in a constant manner and maintains focus on meaningful tokens and, uh, disregarding the placeholders. All right. Um, and next we're gonna get into, uh, encoder decoder attention. Um, I'm looking at the time, let's take a quick, uh, 10 minute break. So on Pacific time, we would get back at, uh, 10 31. So all time zones would reflect 31 minutes after, uh, and then reconvene and yeah, uh, talk about encoder decoder retention. So I'll see you back at now, 22 after, or sorry, 32 after it's 22 afternoon. All right, everybody, let's get back, get started, um, on encoder decoder attention. And feel free to, uh, drop questions into the question and answer section as we process all of the incoming information that we're getting. So we talked about it briefly, but diving a little bit deeper into the internal architecture of the encoder decoder relationship. Um, each of those plays a pivotal role in the functionality of transformers and guides. The model through, uh, the distinct processing steps that we're gonna have. Um, with every encoder there exists, those three primary layers that we've discussed about, uh, the self attention layer, the layer normalization, um, and then those repeated, uh, feed forward as well. Um, getting into those decoder layers we've already discussed, um, just as review, we've still got the layer norm, we've still got the feed forward. Um, encoder decoder tension, which is new. Um, and then self attention. Let's discuss encoder decoder attention. So keep in mind here, encoder decoder attention is also known as attention mechanism, but that's often confused with attention masking, which is like very, uh, proximal to this discussion. So let's use encoder decoder attention terminology. Um, it's a key component in this sequence sequence, um, thought process and process. Um, particularly in the tasks like machine translation, uh, like translating English to Spanish or something like that. Um, it enables the model to selectively focus on different parts of the input sequence when generating each element of the output sequence. The, the attention mechanism here, um, encoder decoder attention enhances the model's ability to capture long range dependencies like I was saying before, and align the input and output sequences effectively. So again, we've discussed this part. Um, and here we have their embedding output. Um, and we are looking at the decoder, specifically this area here. Um, so the initial decoder state is initialized with this context vector that we get from the out, uh, the encoder process. Um, often the last hidden state of that encoder. Um, and that context vector contains information about the entire input sequence, so that, um, input sequence in both the embedding and the sequence and the contextual vector, um, altogether, um, during the decoding process. The intention mechanism, uh, encoder decoder attention allows the decoder to focus on different parts of the encoded input sequence dynamically, depending on what is relevant for generating the next element in the output sequence. Um, attention scores are calculated to determine the relevance of each encoded input element to the current decoding step. Um, and so how, good question on sequence length. So there are limitations. You can't drop the entire ilead into this process. Um, but that's mostly gait or limited by overflow errors. Um, so when you're generating your model yourself, um, it, um, it's quite robust to long, uh, vectors, but you can't drop the entire internet in there. So it's kind of just something that, um, your model will have to, uh, process on its own, but you can drop quite long vectors in there. Um, so for attention scores, the scores are computed based on the similarity between the decoders hidden, uh, current state and the encoded representations using a mechanism like the dot product attention. Um, and we're gonna be getting a little bit more in depth, uh, looking at some soft max and things like that in the next couple slides. And, um, here we've got the process of encoder decoder attention. Um, here we have the key vector. We have the query vector, and we have the values vector. So very similar to what we had before. Uh, the, um, input array is going to be I am eating an apple. And at this layer only the output of the encoder here is used in that decoder. Um, and the, what these vectors represent, uh, q may represent the query like in a Google search or something like that. And that, uh, comes directly from the encoder, whereas K and V, those are gonna represent the already mapped input sequence in a vector representation, uh, from the encoder. Um, and that encoder decoder block multiplies the takes the dot product, the q and the K vector to find the match between the Q and the K vector, and then generate the weights for each value vector. And then looking at the loss function, um, which is a calculation in this process. Um, the cross entropy loss is commonly used, uh, here, uh, lots of natural language processing applications, but one of its most popular application is going to be in, um, the encoder decode process. Um, so given the true distribution, um, which is here, um, and then the predicted distribution here, um, we, we can calculate the cross entropy loss. So you can think of this similar to kind of like residuals in, um, RNN, uh, recurrent neural network and, uh, logistic regression and things like that. Um, so for training, um, the aim is to minimize cross entropy loss. And typically that's done with things like gradient based, uh, optimization, um, and methods that, uh, improve model fit and things like that. Adjusting and fine tuning these model parameters to reduce this loss function. Um, the model learns through sign higher probabilities to the correct classes or tokens, thus improving its predictions. So it can, the model can calculate loss function and improve as it iterates. Um, it's critical to select an appropriate loss function, uh, in accordance with the task that you're, uh, trying to accomplish. Uh, cross entropy loss is pretty well suited to, uh, specifically classification algorithms, uh, and generation tasks where the outputs are things like probability distributions, um, which is super helpful for, uh, transformers specifically. So we usually do use loss function to, uh, measure our model success. All right. So just to summarize quickly, um, self attention is the key to the transformers, um, and it's just simply calculating correlation between each token In the input sequences, um, self attention has the query key value vectors and just as the dot product, uh, to of those vectors to generate the self attention scores. Um, we need multi-head attention to capture different correlations between tokens and the, uh, the sequential parallel nature. So going from sequential to parallel, um, of the transformer, um, really improves model speed and allows for additional complexity compared to the previously presented, uh, sequence to sequence model. Um, there are some limitations of the transformer high comp, um, which is like high computational complexity. Um, but with the improvements of, uh, cloud computing and improvements in computing in general, um, they are less problematic than they were previously in the past. Um, and transformers are used in a ton of different applications, um, like in trans, uh, translation, uh, English to other languages or any language to any other language, and also in lots of domains like the computer vision and classification and things like that. And then, uh, I'm, you want me getting these slides afterwards so you can see, um, what the, where the references are, but these are some great references. Um, I also really like Stat Quest, um, on YouTube, uh, has really good lectures if you wanna dive deeper onto a different topic or things like that. Um, really great resources. Also, Google Scholar has, uh, lots of interesting articles on state-of-the-art algorithms and things like that. All right, we're going next. We're going to talk about Bert. Um, so Bert is an acronym, um, and it is build, or it's for building and training, uh, the state of the art natural language processing models. Um, and the acronym is Bidirectional Encoder Representations from Transformers. So Burt Models, um, in using Burt models, we can generate text like humans. Um, so we know how we can use transformer models for learning attention mechanisms. And what we need is to generate new text using encoder decoder architecture. So we have all of the tools after we learn Bert in, in our, um, tool belts, which will help build a model, uh, which will not only generate new text, but understand languages like we do as humans. So at the end of this section, you will not only understand the Burt model, but also understand popular Burt architecture and lots of different fine tuning, uh, for a specific task. So just as a top level overview, we have the Bert model, uh, which was developed by Google, uh, a few years ago through a research paper. Um, and it's a pre-trained model. Um, and then there have been lots of improvements to this model, um, using things like Roberta and Electra and Albert. Um, and we'll be discussing, um, how to use those, um, and also why we would use that, um, model over Bert, um, and why people still may use Bert for some examples. Um, and we do actually have quite a few code demos. We have two books, um, that will be going through that will, uh, help kind of explore Bert a little bit. All right, let's jump in. So we're gonna get started with just a simple representation of words and tokens. Things like one hop lecture, which we've talked about in the previous NLP uh, classes, uh, TF IDF word embeddings, um, things like that. But solving more complex things like language translation, question answering, um, and high level performance. Um, we do need model with some capacity of language understanding. We need syntax and semantics of each language. We need context, and we need context, cognitive linguistics, and a ton more. And, um, the model with these capabilities, um, have to be quite large. And that brings us to large language models. We're finally here. We're finally talking about chat, GPT. Uh, so chat GPT is just kind of like a specific instance of a large language model. There are lots of, um, less world renowned large language models that we'll be talking about mostly here, but mostly of those rules are going to apply to whenever your roommates or friends or family ask, ask you how LLM works and how she GBT works. So transfer learning is a really exciting concept. Um, transfer learning is machine learning paradigm where a model trained on one task is leveraged or adapted, uh, from or for a different but kind of related task In a traditional machine learning approach. Uh, models are trained for a specific task from scratch requiring a substantial amount of labeled data to train on. Um, transfer learning on the other hand, takes advantage of pre-trained models, pre-trained models. That's a key concept here. Um, and those model pre-trained models are trained on large data sets and transfers the knowledge gained from that training to perform well on a new possibly smaller data set or related task. So transfer learning involves two steps. First step, pre-training. The Bert model does this for us. So a model is first trained on a huge data set with a specific task, mostly in a supervised manner. And that task is usually chosen, uh, to be a general or related problem for which the amount of labeled data, uh, is available. The model learns relevant features and patterns from that data, so it's trained really well on that pre-training data. And then, um, how we can improve it is by fine tuning. That's the second step in the process. Fine tuning or feature extraction. Uh, and the pre-trained model is adapted or fine tuned for the target task or the target data set. The idea of this process is to kind of leverage the work that's already been done for us, um, that knowledge that was gained during the pre-training phase to enhance the model's performance on the specific task of interest. And this adaptation can involve training the model on a smaller data set related to the target task, um, or just adjusting the specific layers of the model. Um, there's lots of advantages of transfer learning. One is data efficiency. So the big lift of pre-training that model has already done for you. Um, and since the model originally is trained on that huge dataset, it learns those generic features that are applicable to a wide range of tasks. And this can be especially beneficial when dealing with limited label data for a specific task or limited computational um, load, or a situation where you, um, are putting, uh, an excessive load on computations. 'cause a lot of that math is already done for you. Um, also advantages in faster training. Training a model from scratch on a large dataset can be really expensive computationally and very time consuming. Uh, and transfer learning allows us to start with a pre-trained model, which saves a ton of time and resource and improved generalization. The knowledge gained from a diverse set of data during pre-training, uh, usually helps us and helps the model generalize really well to new and unseen data. Um, and that improves overall performance. Um, it's really effective for related tasks. Transfer learning is, um, super effective when the pre-training task is related to the target task and the shared features during that. Um, pre-training can be beneficial when capturing relevant patterns, uh, for those new tasks. And for applicability across domains is another strength of, uh, transfer learning. Um, pre-trained models can really be transferred across different domains. So like a model trained on image classification may still be useful, uh, for features related to computer vision, even if the specific classes may differ. So transfer learning can be applied in lots of different ways. Um, some of the ways I already kind of discussed and the advantages, um, but machine learning models in general dealing with Latin national language processing can really be strengthened with transfer learning. Anything that has like a huge computational load, um, if we can offset some of that load by using transfer learning, it's gonna be really beneficial for us. Um, models for translating between languages can also be modified, um, using transfer learning and models that were developed. Um, and trained using English language can be modified, um, for tasks or languages that are similar. Um, and also in, um, some basic like changing from uh, one code, um, to another. So like changing, uh, analysis you did from Art of Python, uh, could be an area where you can use transfer learning. Um, sometimes it's required for, um, general code book changes. Here's a, a pretty good graphical display of transfer learning. So in traditional machine learning, we have a task and then we train a model. So here's where we are traditional machine learning. We have a task, we train a model, um, and then we evaluate that. Um, and if we have another task, we train another model and then we evaluate that second model. Uh, whereas with transfer learning, um, we have a task, we have that model, we have the knowledge from the model, um, and then that model, the model A improves the task B comprehension and analysis. So we get to have all of the learnings that we got from the previous model analysis, um, and we get to leverage that when, um, creating and generating that second model. Alright, um, looking at Bert as a example of transfer learning. So Bert, if we recall bi-directional encoder representation, uh, from transformer, and here is Bert from Sesame Street. Um, if we have the example here, sentence A and sentence B, sentence A, he got bit by Python sentence B, Python is my favorite programming language. So we as humans are aware what the two sentences differ by sentence. A talks about the snake Python sentence B talks about the programming language, uh, Python, which we all know and love. So as humans, we are aware of that difference. Um, if we get the embeddings for the word python in the previous two sentences using embedding models like word deve, um, the embedding of the word python would be the same in both sentences, which is a problem because we need to know which one we're scared of, right? Um, word deve is context free and it will ignore that context and always give us the same embedding for both Python re uh, respective of the context. Bert on the other hand, is context base, so it will understand the context and generate the embeddings, uh, for the word based on the context. So for the above sentence A and B, um, it will give different embeddings for the word Python based on the context. Um, and like I was saying before, it was this Bert algorithm was created initially by Google, improved by some of the other big tech companies. Um, and it, it was, and still is considered a, a great breakthrough in the field. Um, there are a couple of papers, um, on Birch, uh, that will link, um, in the presentation references slides. Um, and like I was saying before, one of the major reasons for the success, um, of the Burt model is it's context based, which allows us to really improve, um, the mappings here and advantages of Burt by bi-directional, um, and works for task specific models. It is trained on these huge corpuses, so lots and lots of pre-training data, and that makes it easier for small, uh, more defined and LP tasks, which in our day in day out work is, uh, uh, frequently what we're working on. Typically, we're not working on billions of records. We're working on, you know, hundreds of thousands or millions, um, or even smaller, um, metrics can be fine tuned, um, and used almost immediately, which is super great. Um, and we'll see some of that in the demo. Um, the accuracy of the model is really high, um, because it is frequently updated. Um, and that can be, uh, really beneficial for successful fine tuning and training. And, um, Bert model is available and pre-trained in lots of different, um, which is really helpful for a global scale. Um, answering the question from, um, Baskar, uh, for Python vector embeddings, we will have different embeddings for the two sentences be, um, that is because of the words surrounding the word Python. Um, so that in some cases is true, um, but it is much more accurate to use a more advanced model like, uh, Bert or one of the other, um, more contextually rich models. So great clarification. Alright, so Bert, I'm gonna say the acronym again, it's always helpful to keep reiterating the acronym, bidirectional Encoder Representations from Transformers. Um, architecture is fundamentally based on the encoder component of the original transformer. Um, and if you get bit by a python and you be interpreted as the love of the Python language, I'm not sure. I think that really depends on, uh, how you feel about getting bit, uh, I had snakes when I was a kid and I was always a little bit afraid of 'em. Um, okay, so Bert, as an encoder marvel, it really is a state of the art algorithm when it was initially created. Um, contextualized representations. Bert uses, uh, the transformer encoder process input data, which is a lot of jargon, transformer encoder to process input data. And it uses that process to, uh, forge high dimensional context rich. Um, so while we do have some context in our previous methodologies, this really improves the context. Um, the handling these embeddings do not merely mirror the input words or phrases, but they also encapsulate their meanings in light of the surrounding context, offering a robust, uh, foundation for various downstream natural language processing tasks. Um, so pre-training is going to be the main benefit here. Masked language modeling, which we're gonna be talking a little bit about in the next coming slides. Um, Bert went through huge pre-training regimen involving tasks, um, like that mass language modeling, where random words in a sentence are masked and the model is trained to predict them based on surrounding context. Um, and we're also going to be doing, um, next sentence prediction, um, which is another way that the Burt model was trained. Um, Bert will, in next language or next sentence prediction, uh, Bert attempts to predict whether the two sentences are likely to appear contextually or, uh, consecutively in a text. Um, and that training helps it cultivate an understanding of relationships and coherence between sentences. These pre-training steps serve as a crucial playground, allowing Burt to learn dynamic languages, syntactical properties, and semantic nuances before it embarks on specific tasks. Um, another section that we're gonna be discussing is word positional information. Um, unlike some earlier models that might overlook, uh, the sequential nature of language, Bert will meticulously incorporate word positions information into its embeddings. Um, and when forming those embeddings, Bert encodes positional information to make sure that the sequential and, uh, structural aspects of the language which are super pivotal in understanding and meaning and context are preserved and utilized in the training phase. Um, Bert's ability to create deeply contextualize word representation has really pioneered the advancements in lots of natural language processing algorithms. Um, providing a powerful pre-trained starting point for lots of tasks, uh, like tech summarization and question answering and a ton of other stuff. There would not be Chad GBT without Bert. Um, and as we embrace the elegance of Bert, we observe how it's architecture, uh, founded on the transformer encoder and enriched through some pre-training, um, how it offers robust context aware base for tons of different NLP uh, applications. And moving forward, uh, let's look at how Bert is adapted and fine tuned, uh, for specific use cases and Bert input representation. Uh, we can see how Bert manages all of these different inputs and how that's going to be different from, um, all of the improvements on Bert that we're gonna be talking about later. Um, and we are now at the halfway point of the lecture time, um, and this is the point where I know I can get monotone and sometimes like, oh my gosh, Ben is gonna be talking for another two hours. But, uh, I really appreciate everybody's, um, understanding and, um, attention that you're paying to the class. This is really important to, these are really important topics and I'm sure that it will be helpful for to your career in the future. Having this knowledge in natural, natural language processing is going to be, uh, really beneficial. Even if you're not a natural language processing engineer, it's always important to remember all of these, these things. Um, so a four hour lecture is a really long time, so I really appreciate everybody's time and willingness to have their wrapped attention, uh, for these whole four-ish hours. So looking at Howbert manages these embeddings and understanding their, uh, words and their order in the sentences. So if we have the input sentence, my dog is cute, um, he likes playing, um, and see I as a human immediately combined those. Um, but Bert, um, can, might not necessarily do that. Um, and we'll talk about how Bert is different, uh, when combining these or these words versus the improvements on it in the future or in the future slides. So for token embedding, so remember, each of these are tokens that are embedded. Uh, this is the embedded token of each one of these. We have an entire embedding for, or we have an entire embedding for just the, um, ING at ending. Um, so this is a map from words or word pieces to vectors of numbers. Imagine like each word here is getting a unique fingerprint that's effectively just a bunch of numbers of numbers. These are looked up from a table. And yes, these vectors are learned during training. Bert will tweak them bit by bit as it learns to reduce prediction errors in, uh, the training tasks. And that's token embeddings in segment embeddings. Once we finish token embedding, um, birth could be reading a sentence or two, and if it's two, it needs to know which word belongs to which sentence. Um, so words from the first sentence here, my dog is cute. And the words from the second sentence he likes playing, um, have separators in between. They're segment embedded or segment embedded, and they are added to the token embeddings of all of the words in each of their respective sentences. Um, they're also learned and refined during the training process. So, um, vasan answering your first question, what's up with the split? Uh, let me just discuss that live real quick. Um, so if you consider the English language, um, there's lots of tenses. Uh, so we have like, play played, playing, et cetera, right? Um, so here is just play here is play ed, and here's play, ING. So for the concept of play, we have at least three different interpretations, uh, replicated by the different, uh, the different text, the different, uh, sorry, I'm blanking on the, the sub or the, um, following words, the tense. So Ed or uh, um, ING. So we have to store each one of those as opposed to if we just have ING stored, then we can use that embedding, uh, to apply to just the base word. So like, if you think of play, um, and we have playing, um, and we can think of something like eat, uh, we can apply this same contextual embedding of, uh, hash ING to all verbs, um, playing, eating, um, looking things like that. So that really saves space because we can have each of these sections stored separately. Alright, so covering token, embedding segment, embedding, position embedding. Um, so for position embedding right here, um, Bert will need to know the position of a word within a sequence. So I think that gets a little bit into how's question. Um, but ta please feel free to answer that as well. Uh, your answers are very complete, which is awesome. Um, so to achieve that, each position up to the max sequence length gets its own. Embedding another bunch of numbers, um, is the embedding words in the first position, get the first positional, embedding words within the second position, get the second positional embedding, and so on and so forth. These positional embeddings, um, are also learned during that training phase and bringing everything together. Each word is represented by a sum of three vectors has its own token embedding has its own segment embedding, um, and has its own positional embedding. Um, all of these numbers are crunched together to give us a pretty rich cocktail of information for each word to Bert. Um, and how are they learned? How does Bert, uh, educate itself? How is it machine learning? Um, Bert tries to get better at all of these tasks. Um, and if it makes a mistake, it slightly adjusts all of these embeddings to reduce the chance of making the same mistake. Next time. Bert gradually fine tunes this understanding of words as it learns the meanings of those words, the order of the, those words and the sentence, uh, membership to get a better understanding at text took me quite a few years to learn English, uh, and how to speak. Um, Bert is only a couple years old, so is doing a great job so far. And like we discussed, remember we've got positional embedding, segment embedding, token embedding, and from all of those three embeddings, um, we can have the sentence, um, Paris is a beautiful city, and sentence BI love Paris. So those tokenizing those different sentences, we can get, um, lots of different tokens, uh, distinct for each sentence. Um, so we have, Paris is a beautiful city, and then a separator. I love Paris. So two different sentences, uh, and we'll be talking about that in the next slide. So here we have those sentences. Paris is a beautiful city. I love Paris. I personally have never been there, but the lecture slides have, um, position embedding is used to incorporate information about the position of words or tokens within a sequence. So how position embedding works, um, in transformer models, like what we're talking about today. Position embedding is added to the embedding vectors of the input tokens. Um, and that provides information about how their positions are in this sequence that allows the Burt model to distinguish between, um, different positions in a sequence without relying solely on the token order. Um, our encoding function, um, the positional encoding here is usually created with a mathematical function. Um, and that generates a unique vector in, um, each position for each sequence. Um, most common approach here is to use like trigonometry functions, like we were talking about earlier in the slide, like sign and code is signed to create those embeddings, um, that capture the positional information there. Um, and Bert is essentially the Transformers encoder in general. Um, and we need to give Bert information about the position of those words in our sentence before feeding them directly into Burt. Um, so kind of similar to what we had in the previous slides, just at a, a more advanced level, um, positional embedding is used to get the position, uh, the, or provide that position, uh, vector to Burke or to Burt. Um, and Bert specifically used trained positional embeddings. So like what we're looking at in this example, each one of these, um, embeddings is translated here. So 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where separators are also, and, uh, start statement at close statement are also considered in those embeddings and explaining it further in the coming slides For segment embeddings, um, we use segment embeddings to distinguish two sentences. So we have this one segment here. Paris is a beautiful city. And then we have the next segment here. I love Paris. See how the segment embedding. This is segment one or segment A. Um, and then this is segment B. Um, so apart from that separator token, we kind of sort out, um, the indicator of our model to distinguish between those two sentences. Um, so we feed in both input tokens, each segment, um, and then we output the segment embeddings to, uh, enrich that context. And then for the final phase, uh, token embedding in this diagram, we have all the embeddings for all the tokens. And, um, CLS indicates the embedding of that token. Um, and e Paris, um, indicates the embedding of the Paris token and so on and so forth. Uh, so the token embedding is a representation of each of those words, or sub words or strings or sentences in that language converted into a continuous vector space. Um, these embeddings capture semantic relationships between the words. Um, so that allows us to really have rich context and for our models to understand the, uh, process of national language. So like I said before, all of the separators, um, that's how we separate sentence to sentence. And CLS is for classification. Um, separators used to. So, uh, that's kind of the input token is we're classifying. Um, and before feeding all those tokens in divert, we convert those tokens into embeddings and using the embeddings, um, we have that final layer of token embedding. Um, and all of these token embeddings will be learned during that training process. So getting even further, we've got the Bert tokenization mechanism. And, um, Bert uses a very special kind of tokenize, which is called the word piece, tokenize, not to be confused with one piece tokenize. Um, let's start by pre-training the model here in this example. Um, so we have that sentence. Let us start pre-training. Uh, the model, remember, uh, pre is a prefix. That's what I was looking before, prefix and suffix. Um, so if we take the prefix and suffix of all base words, um, then that saves us a ton of space. So pre-train, um, And pre-training becomes train with the suffix, pre or prefix pre, and then the suffix, um, in, Um, and that is the tokenized vector here. So one thing that you can notice is if, if that token is not presi, uh, present in the vocabulary, it'll split it into sub tokens, like I was saying, with the prefix and the suffix. Um, and we do that until we can find all of the tokens in the vocabulary. So we have this unique, uh, corpus bank of tokens, um, that we end in a unigram. Um, and in the above example, we can see that, uh, pre-training is split into those three segments. Um, every sub token, um, which is a part of a word, is, um, given that hash mark to differentiate it from other tokens. Um, and this strategy better handles the unseen token during the inference phase. And like we were saying before, really helps us with reducing the size complexity of our model. All right, so now we're gonna get a little bit deeper into how Burt was trained. So the Burt methodology, um, uses two general sections, uh, for training. Um, and that is, we kind of discussed it a little bit before, mast language model, MLM, not to be confused with LLM. Um, and then next sentence prediction. Uh, questions have been relatively quiet for the last little bit. Um, we are going to take a break in a couple of minutes, um, at 1130, let's take a break. Um, usually I'll watch questions come in, and if we like have a low volume, um, that means that everybody's really tired and I believe in you, we can do it together. We've got just over 90 minutes remaining. And, um, this has been a lot of lecture slides. I think we've got some code demos, which are always fun to watch in, um, three or four slides. So we'll take a break once we get to the code demo. We've got 1, 2, 3 slides including this one. Um, and then we'll get to the code demo, and then we'll do the code demo, and then we'll start fresh and be excited and everybody is gonna learn today and it's gonna be awesome. Okay? Uh, mass language modeling is used, um, for the pre-training phase of models like Bert. Um, who remembers what Bert stands for? Drop it into the chat. All right, five seconds until I give you the first word as a hint. Bi-directional is the first word, second word we talked a lot about earlier. Oh, look at that. Great work. Great work. Benjamin. Great work. Have great work. Fassan crushed it. Bi-directional encoder representations from transformers. All right, good work team. Um, okay, so Bert, the goal of MLM is to train a model to predict missing or mass words with a given context, promoting a deep language, or sorry, promoting a deep understanding of language, um, semantics and context. Um, here's how it works. So our input, how are you doing today? We put it into this black box, Burt mass language model and the output. How are you doing today, um, with this mask? See, I naturally filled that in as a human. I read, how are you today doing today? And I just implicitly put that mask in as you, we know that that value is likely you as humans. Um, but we're not letting Bert know that we're testing Bert on that. So we have that masked. Um, and then the output is a probability vector where we have you, uh, they or your, all of those are potential options for the output, whether or not they're grammatically correct, they're still options. And then, uh, the Bert output process, uh, will assign probabilities. So probability, that's statistics, terminology or statistics, um, diction. Um, so they may have probability of you probably like, I don't know, maybe 80%. Uh, how are they doing today? Maybe that's like 10%. How are your doing today? Uh, maybe that's like 10% or, um, like 5%. And then, um, it will provide enough to, uh, give options to, uh, some to one typically. Um, but the model would choose the mask here as how are you doing today? Just like you and I would. Um, so said in different words, the input sequence, which is usually some type of sentence or text. In this case, it's how are you doing today? Or how are masks doing today? Um, the impex has a certain percentage of the words or tokens randomly selected and replaced with a mask. Token does not always have to be just the one, it could be many. Um, this masking is done to create a situation where the model must predict mask words based on the surrounding context. Um, that objective of this process is to model, um, and predict the original identities of that mask. Token is the original identity you, is it they, is it your, that's what Bert's job is. Um, this task inherently is bidirectional because, uh, the model must consider both the left and the right context to accurately predict the, um, uh, the output, the missing words. So that's why Bert is bidirectional, uh, for the training. The model is presented with sequences containing mask tokens, and it learns to generate the correct tokens by optimizing a loss function. Y'all remember loss functions, we talked about it eons ago and, you know, 50 or so slides ago. Um, so using that loss function, it will optimize, uh, the training process and, uh, identify the discrepancy of the predicted probabilities, uh, for the mask tokens and the actual tokens. Um, and then for fine tuning downstream tasks, after that pre-training process on, um, the mass language model task, a model can be fine tuned on a specific downstream task like text classification, named entity recognition, uh, things like that to leverage knowledge gained during the mass language, um, modeling phase. Um, so we start with this kind of general Bert model, and then we leverage, um, the specifics. Uh, when we're tasked with, are we interested in text classification? Are we interested in named entity recognition, uh, et cetera, et cetera, to improve and fine tune the model. Alright, now what's the other side of Bert? Next sentence prediction. NSP. So in this example, Assan likes cookies. Do you like them? And Bert d uh, determines classification. Yes. Sentence B follows sentence A. So, next sentence prediction. Um, the goal in general is to train a model to predict whether a given pair of consecutive sentences in a corpus follows each other logically or if they're randomly paired. Um, so for a sentence, pair construction pairs in a consecutive sentence are randomly sampled from a large corpus. For each pair, there's at least 50% chance that the next, uh, that the second sentence in that pair is actually the next sentence, um, that follows the first one. Um, and there's a 50% chance that it doesn't, um, for labeling. Once that process is the sentence for construction is done, um, the model is then trained to predict whether or not the next sentence is indeed the next sentence, or that it logically follows the first one. If it is the next sentence, then it's labeled as is next, or binary one. Um, or if it's a random sentence, it's not next or binary zero. Um, and that objective is training the model. Um, it's optimized to correctly classify, uh, yes or no to next sentence. And that loss function that is calculated by the correct yes no prediction, uh, measures the discrepancy between the predicted probabilities and the true false labels. Um, learning contextual representations by training on the next sentence, prediction task. The model learns to understand the relationships between the sentences, um, between the sentences and the, um, rest of the model. And for fine tuning as we get further down, um, after pre-training on tasks like next sentence, pre prediction, um, we can fine tune on specific downstream te tasks, uh, like in MLM, tax test, classification, named entity recognition, and lots of other stuff. Alright, let's get through this last slide and then we'll take the break. This slide should be pretty quick actually. So, um, this just kind of demonstrates how huge Bert is in general. Uh, so Bert oftentimes is fit on. Um, I think our download in the code, um, is like one over one gigabyte. Uh, so for a model that's largely string based, um, and just general parameters and vectors, um, that's huge. A gigabyte that's massive. Think about an Excel sheet that's a gigabyte. Um, lots and lots and lots of rows and columns. Um, and this is much more compressed than a Excel sheet. Um, so there's Burt base that's about 10, uh, or 110 million parameters. You can think of parameters as like columns. Um, Burt Large has three times that. Um, and there's Burt, tiny, Burt, mini, Burt, small, Burt, medium, lots of different Burts. Um, and this is the size complexity of each one of those. So l here, oh, lemme get the pen out. Um, l here is, uh, the number of encoder layers, and h is the number of embedding dimensions. Um, so as we go from tiny to base, uh, we increase in size. Um, so the, the main goal here is, um, use the right tool for the job. Don't use a huge one when you have a small problem or don't use a tiny one when you have a huge problem. Um, and you can apply many different ways to, um, optimize your model. And I recommend doing that for each, uh, model. Uh, sorry, I'm just reading the question. Uh, correct in drawing a correlation between the continuous back of words and the way we use, uh, bird utilize maps. I, I think there's certainly a correlation. Um, looks like Rosh is, uh, also answering that question. Um, so that may be more helpful, uh, while we get towards our break. Uh, so good work everybody. Uh, we made it to the next break. Um, it is 1133 Pacific. Um, so that will put us back at, uh, 1143 Pacific. Um, so I will see you all in 10 minutes. All right, everybody, uh, let's get started again. Um, so we are back together. We just finished talking about Burt in theory. Now we are going to talk about Burt in practice. Um, and here I am now standing. Um, okay, here's the code book. Everybody should be able to see. Um, actually it looks like I need to reshare screen four. All right, can everybody see the, uh, Bert examples, pre-trained model? All right, thank you, Benjamin. Uh, the thank you, uh, that, like, if I made it all the way through this and then nobody could see the code. Okay, so this is a really great code book to demonstrate. Uh, Bert Bert does take, uh, quite a bit of time to run. So I've, um, done the run beforehand. Um, and if we have some time, we can, um, look at each individually, um, and rerun. But, um, Bert is a huge model, so it does actually have some, um, computational time, uh, that would not be great during the demo. Um, all right, let's zoom in a little bit. Thanks for, uh, asking. Is that better? All right, cool. Thank you for prompting. Um, this is actually displayed on a huge screen, uh, for me, so, um, it might show up small on the screen. Share. I, okay, cool. So just going through our favorite acronym, Bi-Directional Encoder Representations from Transformers. It's based on Transformers deep learning model, which we've learned about, um, and deep learning because it does have that, uh, feedback loop, uh, pre-training, um, to, um, the next application model. Uh, each output element is connected into the input element, and the weightings between them are dynamically calculated based on their connection. Um, it's designed to have that pre-trained deep bi-directional representation from unlabeled text jointly, uh, conditioning the left and the right context. Uh, all right, let's zoom in a little bit more. Um, so implementing the pre-trained Bert model for the following tasks, quest or text summarization is what we're gonna talk about first, and then it's gonna follow by question and answering system. Um, definitely prompt me in the chat. I'm looking at it in another screen if you can't see. Um, let me f 11 this, so it's a little bit better. Uh, can I minimize this? Yes. All right. Um, so remember this is pre-run to avoid delays. Um, so here we are installing the, uh, required libraries. And if you haven't used CoLab before, it's fairly new, super great. Um, but it uses a lot of, uh, terminal commands. So here we just have PIP install, um, Bert, uh, extractive summarizer. So this is that Burt Library. Um, and you can see, um, it checks a lot of Burt different requirements for previous packages. Um, and then we import the Bert Summarizer. Um, and that actually takes quite a bit of time, um, has nice progress bars that you can see. Um, but we use that. Um, again, Bert conveys core information of original text through the shortest text possible. Um, here we're gonna do some sentence summary, and they're generated by the model, not just extracted. So this is generative. Um, so now let's summarize text with Bert. So first, load the model, um, kind of similar to like linear regression and all of the other psychic learn packages. We just declare a summarizer, uh, to burnt model, uh, variable. Um, and second step, uh, to generate and print out the summary. Uh, we have the original text. Um, for parameters, we have the model, um, which is that Bert summary model, uh, text. We have the original text, which is down here, Um, min length notes, minimum length of the summary. Great variable name, number of sentences is number of sentences that'll be available in the summary. Uh, so we have text, we have min length, um, number sentences. Um, and then that goes into the Bert model function, which is, uh, the summarizer, um, that we then print, uh, Bert summary. Um, and that's run. We have, uh, so that's just the model. Um, notice, so this is just a, a python thing. Uh, if you're not super aware of Python. So here we're, uh, just declaring the model. Um, we are not calling the model. So, um, once we declare the model here, um, then we declare the text, natural language processing, not gonna read that whole thing. Um, but we have this, uh, whole block of text. And then, um, we pass that through this previously declared Jen text summary function. Um, so we have model which we declared text, which we declared minimum length is, uh, 60 number of sentences is three, so we call that. Um, and then it's just avoid function, uh, which means it doesn't return anything, it just prints, uh, or does the task. Um, and you can see that it prints the summary. Um, so it has this whole text, um, here that I didn't wanna read at all, and this whole summary, uh, with, um, a significantly shorter, um, text that is a little bit more approachable for reading. Uh, so text summarization is, uh, a great use of the burn model as well. Um, so here we can summarize it into two sentences, so it's even shorter. And I actually might read this one. Here we go. Natural language processing is an area of computer science and artificial intelligence concerned with interaction between human, uh, between computers and humans in natural language. Natural language processing has, uh, existed for more than 50 years and has roots in the field of, uh, linguistics. Um, so that is a great summary. Uh, from this, you can see it's pulled out some sentences, like natural language processing has, uh, existed for more than 50 years in brute field of linguistics. Um, but like all of this kind of stuff in the middle here, not super critical as defined by bird. Um, and that's actually something really important, um, in the work that I'm doing now, um, with just summarizing models. Um, so if we have like an org leader, for example, that um, is talking to thousands and thousands of their direct reports, their employees, and they don't want to audit a million different emails, um, about different opinions. So we can take this overall text of a million emails and distill it into this much more readable by somebody who's time limited, um, interpretation and summary. And that org leader with a thousand direct reports can get general sentiment of their overall employee experience. And that's something that is, um, really valuable to a lot of different org leaders and, uh, connected to a lot of the work I'm doing currently at Microsoft. Um, so that's summarization for Bert. Question answering tasks, um, that's a whole separate topic. Um, here, input to the Bert model is typically in question and paragraph form. Um, Bert has to extract the answer of the given question from that paragraph. The input embeddings of that are, uh, the sum of the token embeddings and, um, involve some of the segment embeddings as well. Um, token embeddings, the context token is added to the input words at, uh, the token at the beginning of the question. And that separator, um, so that, um, period between sentences token is inserted at the end of, uh, both the question and the paragraph. For segment embeddings, A marker indicating sequence A or sequence B is added to each token. Um, this allows the model to distinguish between sentences. Um, and we can look at the diagram below. So all those tokens are marked as a, uh, for the first sentence and b for the second sentence. So for the input question of how many parameter does Bert large have, um, we have, um, the question here and then the reference with separator here. So we have the separator preloaded if, uh, there's a feature question or anything like that. Um, and then the reference text, which has been pre-trained on the model is bur large is really big. It has 24 layers and embedding size of over a thousand for a total of, uh, 340 million parameters. And that's over a gigabyte for just that model. Um, so it's gonna take a couple minutes to download to your collab instance, which is why we've done it already. Um, so first of all, we, uh, to do the rest of this demo, we're installing all the required libraries for, uh, question answering tasks. Um, and then we, um, upgrade those in informers and, uh, all of the earth transformers. So we have all these packages installed, and I believe you're provided this, uh, demo afterwards so you can, uh, do it on your own instance. Um, importing the required libraries, we have Bert for question answer. This is a nice feature of CoLab. We get, um, the, um, documentation right in the tool tip there when you hover over that text. And then we have Burt Tokenize, um, and then Torch, which is a great package. Uh, and then the model in general, um, Burt, large Uncased word masking fine tuned squad, uh, that whole large, um, descriptor, uh, it's a pre-trained on the English language using masked, uh, language modeling. So everybody remember mask language modeling. That was an exciting, uh, uh, slide. Um, so this model is not cased and it does not make difference between, uh, English and English. Um, and surprisingly that has caused me a lot of headache. Um, so it's good that that's already done for you. Um, you know, you haven't been in code unless you've, uh, wasted an entire day because of, uh, bad case, uh, management. So it's good that it's already done it for you. Um, so differently to other Burt models. This was trained using a new technique, which is whole word masking. Um, so in that case, all the tokens corresponding to a word are master all at once. Um, the overall masking rate remains the same, but in this case, each mask word piece token is predicted independently. Um, and then after pre-training, um, it has been fine tuned. Um, this specific, uh, bur large case on hold bird masking, uh, it's been fine tuned by the squad dataset on the squad dataset. Uh, so we have this specific squad dataset that we have used to improve and fine tune our model. Um, and after that fine tuning, we've got 24 layers over a thousand hidden dimensions, 16 attention heads, um, and just over 300 million parameters, um, which is slightly different than what we had before in the base model, which is, uh, uh, 340 million. So slightly simplified here. Um, so loading the model Bert, uh, model here, uh, from pre-trained the uncased forward masking fine tuned squad. See how long this takes to run. All right, already run, look at that. So we can see the output here. Um, this is mostly just initializing it, loading the tokenize, which we've already done. Um, and then declaring the function. So what this function will do is it'll be helpful for finding the answer to a given question from a given paragraph. Um, so remember that example in code or in, uh, the lecture slides where we're talking about where does a question start, where does a question end? Um, so we use the function in code plus to encode that sequence. That function returns a dictionary that contains input IDs, token type IDs, uh, and the attention mask. Um, but we only need input IDs and token type IDs for, uh, the quality assurance task. So here we're declaring the encoding variable with this function in code plus. Um, and then we have the question, and then, um, the text pair, uh, question or text pair text, um, and then we have our inputs in the token embedding. And then we have our segment embeddings, like we were talking about before. And then we have our input tokens. Um, so we have, from there, we declare start scores and end scores using the, uh, model input. Um, and we use that to pass parameters, uh, like the token embedding, segment embedding that are declared up here. Um, and then we use the arg max function to get the start index of the answer and then the end index of the answer. So remember, um, if you have, let's find an example here, um, in this big block of text, the question is, what is machine learning? And then this is the return, uh, text. Um, so it's important for us to be able to identify what section of a user prompt is the actual question. So that's what we mean when we have the start index and the end index of the question and the answer you ever talking about the answer. Um, and then we have finding the answer from a given paragraph using the start and end index. Um, and then here we have recover any potential words that were broken down into subs, um, finally storing the correct answer, um, and then printing it out. Um, this is just a, a bucketing thing. Um, and then we print out the final, uh, correct answer at the end. Um, so here at the end of the day, we just have this simple output, right? So the simple output, once we, uh, call the function up here with the input of question and the output of paragraph or the, uh, reference text paragraph, it's just, um, user asks, what is machine learning question is question here. And paragraph is, um, that's the reference text. Um, so the final corrected answer is going to be the scientific study of algorithms and statistical models. Um, so let's see if we can find this text or this answer in that text. So there we go. Machine learning is the scientific study of algorithms and statistical models. So considering the org leader example, um, where we have this super important person with a million direct reports, um, and they are a very busy person and they don't wanna read this entire block of text because they look at it and they're like, Ugh, that seems like a lot of work. So you can deliver them something that is this, um, model that simplifies. So all they have to do is ask a question and they get the answer from this overall corpus of text. So this, uh, when scaled becomes something like chat two PT where you can ask it pretty much anything because it's paragraph or corpus that it references is the entire internet. So that is that code book. Um, one thing that you can do to kind of like test case, this is, uh, replace question and paragraph with different prompts and different response, uh, books. So you can see how it changes over time and how it changes with different inputs. Um, but let me switch back over to the lecture slides quickly. Any questions on the code book before we, uh, do the final switch back, or not final switch until the next code book? All right, thank you, Benjamin. Oh, look at that. I didn't even have to resume. Um, all right, let's keep rolling. Let me just get the notes here. Um, and here we're gonna look at in addition to Bert, which has been, um, really popular for a long time, there have been improvements. So, so on top of Bert, we now have Roberta developed by Facebook, uh, robustly optimized Bert pre-training approach, um, which is simply, uh, put in statistic terms, it's a bootstrapping, uh, improvement of, um, Bert. And then we have Albert, which is a simplified version of Bert. Um, it takes effectively all of the good with minimal of the bad, um, and makes it a much more lightweight algorithm. And then we have electro, which is efficiently learning and encoder that classifies tokens, uh, replacements accurately. So said simply it, um, determines, uh, correctness, um, and whether or not an item was replaced or original. Um, but in a practical sense, I would say we're 90% of the time going to be using in our day-to-day work. Uh, Roberta and Albert, um, Electra could be used occasionally, but it's a much more, uh, advanced nuance algorithm. Um, in my personal work, uh, I would choose probably Albert for most of the use cases, and if I'm not getting good fitting, I'd probably hit Albert or Roberta. Um, and then maybe Bert after that, uh, depending on a couple of cases that we're, we'll talk about a in a second here. All right. Uh, so for Roberta, um, Roberta is an important variation of Bert, um, in a short for robust optimized Bert pre-training approach. So this variant of Bert, while maintaining the foundational architecture of Bert, um, introduces notable modifications to the pre-training procedure and enhances the model's capabilities. Um, Bert does have some disadvantages with under training, and that's been identified through lots of different researchers, um, and data scientists looking at the Burt model, uh, in depth. Um, and they discerned a potential suboptimal training issue, um, and hypothesized that Burt could be Undertrained. Uh, Roberta was, uh, formulated to further exploit births potential, um, by altering its pre-training regimen, usually by, um, or mostly by implementing bootstrapping, which is statistical sampling with repetition, um, to improve the size of the dataset. Um, and another way they did that is, uh, dynamic masking. So remember the masking concept where we take a specific, um, word and redact it. Roberta introduces, uh, lots of improvement using the masked language model task using that dynamic masking. So as opposed to static masking where the same word is, uh, muted or masked each time, um, dynamic masking implies that the words selected to be masked are different across different aspects of training, different times we, uh, iterate through, uh, training. Um, so therefore, Roberta training is, um, only on the MLM task and does not rely on predicting the sequential relation between the two sentences. So Bert Algorithm uses both, um, MLM mask language model, um, methodology to train and next sentence prediction. So uses of those. Whereas Roberta just relies on masked language modeling. Um, that adjustment that Roberta does, uh, prevents the model from memorization of mask words, uh, due to the continual change of the masked instances. Um, so it can't just get good, um, at memorizing things. It actually has to learn where, uh, the context could be used. Um, so why would we drop NSP? Why would we drop next sentence prediction? Um, so it was initially present in Bert's training, um, but we found that it was not as valuable as we initially considered, and, um, we removed it to allow for, um, a more optimized model. Um, so because of that, we're able to use larger batch sizes during training, and that has a more substantially, um, increased number of examples, uh, for training. Um, and it can happen concurrently during iteration. Um, and then also Roberta utilizes bite level BPE tokenize, uh, which is a lot of words, BPE bite level pair encoding, so BPE bite pair encoding. Um, so it uses, Roberta uses BPE as a tokenize, uh, providing it with a lot of advantages and advantageous capability to handle text and multiple languages and effectively process larger text corpuses. Um, so through all those changes in pre-training, Roberta tries to navigate the challenges identified in Burt achieving refined performance through various natural language processing tasks by fully leveraging the potential of the transformer architecture. Oh, my pen's. There we go. It was off. All right, here we go. So, Roberta, using dynamic masking instead of static masking, remember that's what we talked about the last slide. Um, here we are going to, um, use an example. So, um, static masking again means we're hiding the same words every time. Um, we show a sentence to the model during training, so it's easier to predict sometimes, uh, that makes it, um, under fit. So dynamic masking, we, uh, hide different words every time during training. Um, so imagine a sentence, we make 10 copies of it. Now we hide different words in each copy. Like here, it's the same sentence, right? We arrived at the airport in time. Um, so we have sentence one, sentence two, all the way to sentence 10. So in sentence one, we're masking arrived and we're masking time. Sentence two, we're masking at, uh, we're masking airport sentence 10, we're masking the, and we're masking in. Um, and we repeat those, uh, different processes, randomization over time. Um, and if you're interested, that concept of expanding and randomizing, um, is just statistical bootstrapping. Um, so let me write that down if you're interested in researching it further. So It's bootstrapping applied in a different way. So there's actually a lot of literature on it. Um, and it's a very robust algorithm to improve. Um, actually recently used it work, uh, like just the other day to improve one of my analysis. Um, and one thing that it does, it is it, uh, creates a statistically kind of like a, a sample, um, uh, a sample mean distribution, uh, or sample normal distribution. Um, and that really helps, um, improve algorithms and allows you to play nice with, um, analyses. Um, because instead of just having one record, you have many records and it, uh, leverages the central limit theorem to allow for potential normal distributions and sample means statistically. Um, that's not exactly how we're using it in this context, but it's in that same vein. So it does, um, allow for improvements. Alright, Um, so training Roberta using dynamic masking. So here we are training this model for 40 epics, right? So there are a ton of different replicates, um, and we are, uh, replicating it many times. So we are randomizing, randomizing, randomizing and bootstrapping, et cetera, et cetera. So for each epic we feed the sentence with, uh, many different tokens or with different tokens that are masked. So same sentence, different tokens, mask each time, um, the model will see that sentence, um, repeated over and over again with different masks, and that will help improve the model. Um, so that is generally, um, at a very high level how Roberta HA is trained and how, um, it is different from Bert. Um, but the top level that you have to remember of how it's different Roberta and Bert is Roberta does not use NSP. Um, and it relies heavily on, uh, MLM, um, and how it changes MLM to improve it is dynamic masking. So why would we use each of 'em? Um, Roberta model more extension of training dataset than Bert, uh, consisting over 160 million sentences, which is a lot of sentences. Um, deeper transformer model than Bert. Um, lots of transformer layers, double, uh, max language modeling objective. If that's what you're intending to research, then that's obviously the best case. 'cause Roberta has a more robust, uh, MLM, um, it has a much larger vocabulary than Bert over a million words. Um, and Roberta training has sub word tokenization in general, uh, dynamic masking, like I was saying before, and can take longer sequences up to about 500 tokens, a little over 500 tokens, um, compared to Bert who is much less. Um, and just general pros and cons, uh, Roberta removes next sentence prediction. Uh, significantly increases. Oh, looks like there's a poll on screen, uh, for you to take. Now. Uh, we've got about 45 minutes left in the class, so we always appreciate your feedback. Um, and I promise right after that, we're gonna get back into the code. Um, I think probably in about 10, 15 minutes after the next code session, uh, we're gonna take just a five minute break because that will put us at 30 minutes to go, which is the hardest 30 minutes, but we know, um, we're almost there. Really appreciate everybody's feedback. All right, closing the poll in 10 seconds. Um, and then after this, also feel free to connect with me on LinkedIn, um, and ask any questions. Um, I think I'm the only tankas on there. I think it's like me and like my dad. So I'm the one that's not a photographer. Um, always happy to, uh, I think we've also got a career coaching session on Wednesday. I'm not exactly sure how to sign up for that. Um, but I think you've been communicated, uh, how to sign up for the career coaching session, but always happy to talk about that stuff and stoked to talk about data. Um, speaking of which, let's talk about data. Thank you for the feedback. Let's keep rolling. Um, okay, so we talked about the main thing, Roberto versus Bert pre-training. Objective sentence piece. I don't think we've talked. Yeah. Okay. Sentence piece tokenization. Roberta uses a sentence piece tokenize, which allows for dynamic vocabulary and that adapts to training data. Um, that's different than birch, uh, which usually uses word piece, tokenize sentence piece tokenize. So at the sentence level, tokenizing instead of word level, um, that is typically, um, advantageous for languages and rich morphology. Um, so like when more context is required, you usually have to cast a wider net. So look at sentence tokenization as opposed to word tokenization. Um, so Roberta handles that a little bit better. Uh, learning rate schedules, Roberta employees a more aggressive training setup with larger batch size and a linear learning rate. Um, and this may result in faster convergence, so getting to an answer a little quicker. Um, and improved performance in some cases. Uh, for model architecture, both Bert and Roberta are based on transformer architecture. What are transformers? That was what we talked about earlier in the lecture, um, when we're talking about encoders and decoders. So both Bert and Roberta are, um, uh, based on those models, um, they normalization both used. Um, so depending on the specific task and desired model behavior, uh, bur and Roberta, uh, differences may be relevant on whether or not, um, NSP is used. Next sentence prediction. Um, and fine tuning, um, choice between bur and Roberta when fine tuning is pretty dependent on performance of the model. On the downstream tasks. After fine tuning, it's usually advisable to experiment and evaluate both models on target tasks to determine which one performs better. Um, so when I am, uh, doing analyses, I don't just run one thing because it's 100% the right case. Um, sometimes Roberta performs better, sometimes Bert performs better and they're pretty computationally, um, trivial to run, um, at the same time. So when I'm trying to answer a research question, I usually do just do all algorithms and, um, if they are, uh, performing similarly, I'll probably lean towards the one that does it faster. Does that same, um, analysis faster, um, or more robust? So fastest one, um, that we're gonna talk about in a little bit is, Albert most robust is Roberta, and then Birch is the one that most people know, which honestly is something valid as well. Um, so all of those things are important to consider. Alright, code demo, we're gonna do the code demo, and then we're gonna take a five minute break and we're gonna be happy and it's gonna be awesome. Um, so here we have the code demo. Um, let me just start. All right, yeah, so this should be already run. So again, unfortunately I can't run it live just because these, uh, code books are ra rather large. Um, so it would take all of the time we have remaining, uh, just to run a couple of these, uh, slides, like five minutes. But we don't wanna watch, uh, the corgis run across the screen for five minutes. All right, uh, introduction. Oh, actually, uh, let's zoom in. Um, all right, that should match what we had before, maybe a little bit more zoomed in. Um, let me know in the chat if this is, if we need to zoom in a little bit more. Um, so Roberta, like we were talking about in the slides, implementation of Bert with some key changes. Um, and those changes include hyper parameterization, um, lots of embedding tweaks, uses bite level, uh, BPE as a tokenize, and that is a different pre-training scheme. Um, Roberta is trained for longer sequences, so bigger, um, capacity and number of iterations is increased, uh, from a hundred K to 300 K and sometimes even 500 K as well. Um, uses larger bite level vocab, um, and has 50 K sub-board units instead of character level. Uh, BPE, um, the size of 30 K and Bert. Um, so all of its stock points into MLM, uh, none of its stock points into, uh, NSP. Um, Roberta doesn't use, uh, token type IDs and we don't need to define tokens, which is pretty cool. Um, only separate segments, uh, with the separation or separation. Um, in the token, um, larger mini batches, oh, geez, sorry, uh, markdown, uh, larger mini batches and learning rates, uh, are used in Thea training. Um, NSPs removed, like I was saying before. But, um, let's get into the code. Um, so this is connecting it to Google Drive. If you have not used, um, the CoLab, that's really the only negative I have with, uh, CoLab. Um, it's a little bit difficult to, um, manage your route directory. So I usually just save, um, the code book locally, um, in my Google Drive folder, and then have, um, an abso or a, um, reference mapping, uh, for the actual path. Um, and you can in, uh, Google find the location, um, of each, uh, file by going to properties. And that helped me out a lot. Uh, so we install the Transformers torch, um, install the accelerate package, um, and then we clone this, uh, repo, uh, dataset from the, um, get page here. So tweet dataset. Um, so here we are getting into the analysis, defining the Roberta base model from Transformers. We import the pipeline, um, and we choose the model Roberta base. And our token is also Roberta base. Um, so this is the load here. And after that we declare a function to, uh, predict the masked token in the sentence. Um, so our function here, we're gonna predict the mass token. We have the input model, and then we have, uh, the input sentence. Um, and then this is just going to be for everything that we predict. Um, we print the sequence with, uh, some formatting, um, and then we print the, uh, confidence score. Um, so here you see we have this mask input send the, uh, mask back, uh, calling that function with the Roberta model. So we, we have the Roberta model input into the predict mask token. And then our goal is to predict what this is. And see, we haven't really trained it on anything. Um, we are just calling the pre-trained model and we manually masking this. Um, it looks like there's another poll. Uh, I think we already answered this. Looks like there's some slightly different questions. Uh, we're gonna take about 15 seconds to answer this, 'cause I think we're running pretty short on time. Oh, okay. Uh, thank you Benjamin. Uh, so this one is for me. Um, the last one was for the ta. So really appreciate everybody's feedback. All right, let's give it 10 more seconds. All right, thanks everybody for the responses. We've got about, uh, 30 minutes left in the class, so we'll finish this code book and then take five minutes, not 10 minutes. Um, and then we will finish the last, I think, 15, 20 slides. Um, okay, here we go. So, predict the mask token, um, send the mask back. Um, so Roberta is trying to figure out what we're trying to say. Send the pictures back, send the photos back, send the emails back so we're all kind of in the same region. Um, and you can see here it has confidence score with each one of these. So Roberta is pretty sure that it'd send the pictures back, but it also might be send the photos back. So, um, all these three here are pretty tied. Um, and what's interesting is those are all things that humans would say, right? Send the emails back, send the pictures back, send the photos back. It's pretty uncommon, uh, for somebody to say, send the image back or send the letters back, especially nowadays. And there is a fairly significant confidence drop in those. So that is really interesting that it captures natural language sentiment or natural language performance. Um, so one thing that is important is while, uh, Roberta models are updated, um, and the pre-train models are continually trained, it may not, um, have the most UpToDate information, especially when talking about like, uh, culture and, um, societal, um, leanings and things like that. Uh, so when we're putting in e or then when the input is, Elon Musk is the founder of x, uh, when X is mask, um, Elon Musk is the founder of Tesla, uh, SpaceX. So we're pretty confident in that. Um, and I'm, one thing that we should consider is Elon Musk is the founder of Twitter. Um, while he's not the founder, I would expect this, uh, confidence score to be a little bit higher because he's the current CEO. So this may not be the, um, like we can tune this model to improve that prediction a little bit better. Um, especially because there is no Twitter anymore. It's X, right? Um, okay, so configuring, tokenizing training and saving the model. Uh, so prep, or prep, preparing the data set is what we're gonna do first. So we build the, uh, token here, um, the BPE tokenize, uh, and we have token on Roberta based token or the model on Roberta base as well. Um, and then here we are preparing the transformers, um, based on this training text. Um, and then from there we'll load some libraries in a little bit. But let's just get into some context quickly. Uh, so what is the data collator and what does it do? Data correlators are objects that will form a branch using a list of data sets and elements as input. Um, those elements are the same type as the elements of the training dataset or ev eval dataset. And to be able to build branches. Data collators, um, may apply some processing or padding, um, from, uh, things like this data collator, uh, for language modeling. And they also apply some random data augmentation like masking, um, random masking specifically on that formed branch. Um, and here we're gonna use that function for randomly masking 15% of the tokens. For MLM tests, uh, parameters are, uh, the token, and the token is typically used for, uh, encoding the data. Um, the MLM process, um, or the MLM um, input is whether or not to use mask language modeling. If we set that to false, um, the labels and the inputs, um, are ignored. Um, otherwise the labels are, um, set to true or not ignored, I should say. Um, and for MLM probability, um, like I was saying before, if we want to include that in our model, then we set to true. All right, so finally, um, importing that final model data co, uh, data collator for large language model, big functioning. Um, so we declare it, we declare the tokenize that we're using that we declared up above. Um, we are using MLM, um, and we have, uh, 15% masking randomly 15% of the tokens. Um, so in a sentence of 100 words, 15 of those will be masked. Oh, um, shout out from, uh, Reja and, uh, Benjamin and a couple other people. Um, really appreciate the feedback. Um, always good to hear, um, and stoked to contribute to the knowledge of new, um, data science people and data engineers and us other engineers or whoever you may be. Um, okay, so difference between Roberta model base and a Roberta model retrained. Um, Roberta model base is a pretty generic model. Um, and we have that pre-training step versus that fine tuning to get Roberta a retrained model. Um, so here we are training the model. We have transformer or from transformers, we're importing the trainer and the training arguments, um, we're initializing here. Training arguments are this function. Um, I know when it gets to code, it's pretty difficult to interpret all the time, but just like go line by line and we will read it together. Um, the output directory is where our model is going to go, um, and whether or not we're overriding our current, um, model, which is kind of that learning process. Um, and how many epics we're gonna train on. Uh, one is just so that we kind of reduce the computational load. You can go higher and higher and higher. Um, this is the batch side or batch size. Also consider that for, um, computational load. Um, and then this is seed. So, uh, randomness, um, there's lots. So seed in randomness is, it's actually really difficult to get a truly random process, uh, by generating numbers randomly. You have to have, um, numbers that are truly random. And for that you have to set a seed. There's an approved list of randomness, um, which is hilarious to say, but, um, that approved randomness will allow us to make sure that our model is the most effective. So seed selects one of those approved random lists. Um, so seed one here. Um, and then passing the model training arguments to the data correlator, uh, the data set, uh, to the trainer function and training the model. Um, so here we take the training function, we declare the model that we've discussed. Uh, we have the training arguments, and then we have the data collator and the data set. Um, and then we train, uh, see, this took about three minutes. Um, so this is going to be a little bit, uh, computationally expensive. And this is a simplified model. So, um, if you're training a more advanced model, just, uh, remember to plan in advance. Um, and then we're loading that trained model, that retrained model. Um, and now we're using again. So here we have a lot of calls and a lot of outputs. Um, what we're going to be looking at is this right here, Roberta retrained. Roberta retrained, Roberta base. Um, so look at these two differences. Uh, first one is retrained, second one is base. Um, I hate watching mask sports. And the next one is the same. I hate watching mask sports. This one is Roberta retrained. This one is Roberta Bass. So let's look at the differences. So Roberta retrained, um, I hate watching fantasy sport. I hate watching stupid sport college sport, live sport, pro sport. I hate watching fantasy sport. So that's the same college sport, live sport, professional sport, pro sport. So the difference is, we can see here, uh, stupid sport is not available in the base model. Um, and college is a little bit higher in the base model. Um, professional sport is also, um, duplicated here. So pro sport, professional sport. So this, um, may be a little bit more applicable in our setting. Um, so I hate watching fantasy sport. Um, stupid sport. College sport may be more applicable for us because we have tuned it that way. Um, so we can see the confidence is lower 10% versus 24%. Um, but in fine tuning, you will have to take a little bit more ownership over the model and how it predicts your things. Um, so just make sure that, um, when you are fitting algorithms, um, just try different things when you're tuning it, you're retraining it, um, to fit your purposes. Um, and then here we can just see, uh, in the demo slides, I'm gonna kind of gloss over these next few 'cause it's about the same. Um, I'm a male model fashion female, Russian young. Um, this is the retrained. And the next, I'm a male female, professional fashion Russian. So very similar. Um, and then here we have a little bit lower, uh, confidence for male. Um, and then, uh, female is significantly lower on the list, but about the same confidence level. Um, so it is up to you to determine, um, your cases. Um, which one you would recommend retrain or base model here. Um, I think for this circumstance, I'd probably recommend base model because there is a higher confidence, um, on male model. Um, and if that is what we're looking at, um, then that is the model I'd recommend. Usually when, um, we are in this state, uh, where we have such a low difference, uh, only four percentage points, um, I would probably not consider those two different from each other. Other, um, I do like in the base model, how there's this big gap between the leading one and the, uh, a second level because that allows me to make a confident decision that, uh, we are predicting, hello, I'm a male model to be the number one, um, return output value. All right, let's get back to the lecture slides. Oh, shoot, uh, five minute break. So we are back at, uh, 1245, uh, PST. Um, and I do want to take this break instead of just power through because I really want to pay attention to the last couple slides. And at the end of four hours, we're all toast. So just five minutes and then we will power through the next few slides. All right, let's get back, uh, rolling on Albert. So Albert is a like version of Bert. So Bert is that base model. Um, and then we have reduced it in size, but maintained the robustness of the analysis. Um, so similar to Bert, Albert uses self supervised learning with mass language modeling and MLM, um, and again, uh, NSP is not used in Albert. Uh, Albert uses sentence order prediction instead. Uh, SOP umbert consists of just over a million parameters, and Albert is significantly smaller. We'll be, uh, looking at a really nice visual presenting the difference between, uh, bird and Albert in the next couple of slides. So, um, it does a few different, or it, um, simplifies a few different ways. Cross layer parameter sharing, factored embedding, um, and then inner sentence, uh, coherence prediction. Um, so Albert first we're getting into cross layer parameter sharing. Um, so in cross layer parameter sharing, um, it's an interesting method for reducing the number of parameters. In the Burt model, Burt consists of n number of encoder layers, uh, usually a ton, um, and sometimes it could be 12, sometimes more. Uh, remember those different types of encodings we talked about at the beginning of the lecture, um, all shared where, uh, we share all the parameters and the sublayers of the first encoder with all the sublayers of the other encoders shared Feedforward network. Um, and in this, we only share the parameters of the feedforward network of the first layer encoder with the Feedforward net network of the other encoder layers. So we're simplifying that. Um, and also another option would be shared attention. And in this layer, um, we only share the parameters of multi-head attention, um, of the first encoder layer with multi-head attention of the other encoder layers. And then this is kind of just a, a visual description. Um, so we can kind of see how Albert, uh, potentially shortcuts some of those encoder steps to, um, reduce model complexity. Uh, we have multiple layers, and if we share the weights of those multiple layers, um, as opposed to, uh, have each individual weights replicated for each of those layers, then the model size will decrease. So we may have factor weights in multi-head attrition that are duplicates of add and normalized and, uh, fee forward, et cetera, et cetera. So we remove those duplicates, and that is how Albert is faster by the removing of duplicate weights. Um, so here's another option in Albert or another way that it reduces complexity. Uh, factorized embedding parameterization. Um, in the Burt model, its improvements like Excel net and Roberta. Um, the input layer embedding and the hidden layer embedding have the same size roughly. Um, so remember that multi-layer visual that we talked about earlier in the, uh, lecture, um, both term, both in terms of modeling and an application, um, that's pretty suboptimal. Um, while it is effective, in some cases it could be improved, and that is improved in Albert. Um, in this model, the two embedding matrices are separated at the input level, and they only need to refine, uh, the context independent learning. Um, and the hidden level H requires some more context dependent learning. Um, and that leads to a pretty huge reduction in size, um, up to about 80% with only a minor, uh, per performance drop when compared to Bert. So again, this is why I usually select Albert, because, um, it is significantly less computationally expensive, uh, compared to Bert or Roberta. So start with Albert, um, unless you're explaining something, and then start with the base. Bert. Um, so inter sentence co coherence prediction, uh, similar to Bert Albert used the MLM mass language model in training. Um, instead of using next sentence prediction, we're using SOP. Um, so sentence order prediction, which we'll discuss here in a little bit. Um, so it's a binary classification loss is what it uses. Uh, remember loss, uh, function calculation. Uh, it's kind of like the residuals of, um, transformer um, models. Um, so it uses that to predict whether two segments occur, uh, consecutively, um, in original text. Um, and this checks the loss for coherence as well as, um, next sentence. But SOP only looks for sentence coherence. So SOP is only looking for sentence coherence, not, uh, what the next sentence may be. Um, so this is interesting here. Burt Base has nine times as many parameters as Albert base. Remember, there's different models sizes. Um, so Burt Base has nine times as many parameters as Albert. Um, Burt large has 18 times as many parameters as Albert large, so it's way bigger. Um, so that's kind of the top line you use Albert because it's much faster, faster. Um, here if Burt large was used as a baseline, Albert large is 1.7 times as likely, or 1.7 times faster, uh, in iterating through the data. Um, and while you may not care about the speed, what the speed allows is, um, potential preventing overflow errors that could cause your analysis to just be crushed, um, because the code will not, uh, compile. All right? And Ekra is not used as frequently, but we're gonna touch on it briefly. Um, efficient learning and, um, sorry. Electra is efficient learning and a coder that classifies token replacements accurately. Um, so it's known for language understanding and some efficiency, um, and it's best described. Uh, in the next slide here you can see, um, we change and identify what is the original and what is replaced. So what is masked? Um, so we can see the generator, which is typically a small MLM, um, the input value, the mask, uh, the is masked, and then chief is the original. And then, uh, co, uh, the original here is cooked, uh, which is masked. Um, and then we can see here the Electra model identifies what has been replaced, um, that is much more computationally difficult because not only do you have to identify what is, um, the original word, you also have to identify, uh, has that been replaced. So, um, it, it is typically recommended over using Electra, um, to fine tune Burt, uh, for a specific task, uh, like we have been doing in a couple of the, uh, code demos. Um, so during that fine tuning, we can adjust the weights of the model, um, by using the pre-train model along with additional classification layers. Um, we can update only the weights of the classification layer and not the pre-trained Burt model. Um, and when we do this, it becomes very similar using the pre-trained model as the, the feature extractor. And then here's some more examples about fine tuning Bert for text classification. Um, sentiment analysis is really important. Um, I do sentiment analysis a ton in my work. Um, so looking at the sentence I love Paris, is that positive? Is that negative sentiment? Um, so once we tokenize it, um, we can identify, uh, the key kernel, um, token of love, making that a positive indication. Um, so that, um, text classification is something that we can do with, um, Burt really well. Um, and then here, fine tuning, this is just another way to fine tune. Um, we have that same sentence. Uh, I love Paris. We pass it to our naive vanilla pre-trained Bert. Um, and then we have the Bert output, um, that we now pass to, um, more customized, uh, or more fine tuning functions like the soft max and the Feedforward network. And from that, we can get, we're pretty sure 90% sure that this is positive sentiment and there's a chance 10% chance that it's negative. Um, and we would be correct. We as humans know, love is usually positive usually. And then fine tuning for question answering. Um, so previously we had classification, now we have question answering. So, uh, Bert can do both depending on what function you use. Um, and for question answering, um, just reiterating what we previously displayed in a couple of different code demos. Um, we have the pre-training, Burt over on the left hand side here. Um, and this provides some decent output, um, which we can then take and potentially improve in the fine tuning step. So if our goal is to, um, adapt the pre-trained birched for a specific question answering tasks, um, and our dataset might be like a, a labeled dataset, uh, consisting of question answer pairs. Um, each pair usually includes something like a, a context passage or a question about the passage and then the corresponding answer. So that's our data set. Um, so what our tokenization would be is the question and the answer and the context passage. Those are all tokenized using the same sub word tokenization. Um, and from that input, we then, um, fine tune, um, given packages, uh, or passages like Bert is a powerful natural language processing model. We can dynamically mask those things, um, and using all of the other tools in our now filled tool belts to help fine tune, um, our answers, um, and improve, uh, our output. Um, so fine tuning, this is exactly what we saw in the code demo where we have our question, what is an immune system? And we have our output paragraph, um, the immune system is, and then you can see the answer right here. So we can pass many, many, many, many of these question paragraph pairs like we have in our example dataset, um, to fine tune this model. 'cause, um, having more trainings, data sets, especially like, um, if you are training or if you are fine tuning a model for the context of, uh, the healthcare analytics, you may pass this question paragraph pair as an additional context for the model here. Um, so that would help fine tune. Um, and then like we were saying before, this is another similar example with a different question. We have what system, uh, the immune system tissue, so that question and that answer that we saw on the previous slide. And then we have the pre-trained Bert. And then we can apply things like, um, multiple, uh, training question paragraph sets or, uh, running it through a dot product, uh, and soft, uh, max transformations. Um, and then we have the probabilities of the start and end word. And then, um, that's pretty much what we've got today. Um, oh wow, 45 seconds left. Let's get through the brain teaser section. Um, so we can have, um, four Bert, some general how to fine tune the model, um, and think about after this class, how can we fine tune the model to generate the answer word by word. Um, yes or no. Um, and just consider that in your day to day going forward. Um, I think you'll get these slides afterwards so we can go through the summary. Um, and there's a couple of links to demos, um, and also some, uh, references and literature after that. Um, so again, feel free to sign up to the, um, uh, career coaching. Feel free to add me on LinkedIn, um, and stay connected. Um, and yeah, I'm always happy to teach and discuss, um, natural language processing or statistical concepts or anything like that. Um, so thank you everybody for your patience. Um, thank you to the TA for answering a ton of great questions. Um, and I'll stick around to help answer in the chat as well if there's any last minute questions.